{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import sys\n",
    "from scipy.sparse import linalg\n",
    "from pathlib import Path\n",
    "import itertools\n",
    "if torch.cuda.is_available():  \n",
    "    device = \"cuda\" \n",
    "else:  \n",
    "    device = \"cpu\"    \n",
    "\n",
    "\n",
    "torch.set_default_dtype(torch.float64)\n",
    "pi = torch.tensor(np.pi,dtype=torch.float64)\n",
    "zero = torch.tensor([0.]).to(device)\n",
    "torch.set_printoptions(precision=6)\n",
    "\n",
    "class model(nn.Module):\n",
    "    \"\"\" ReLU k shallow neural network\n",
    "    Parameters: \n",
    "    input size: input dimension\n",
    "    hidden_size1 : number of hidden layers \n",
    "    num_classes: output classes \n",
    "    k: degree of relu functions\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, hidden_size1, num_classes,k = 1):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size1)\n",
    "        self.fc2 = nn.Linear(hidden_size1, num_classes,bias = False)\n",
    "        self.k = k \n",
    "    def forward(self, x):\n",
    "        u1 = self.fc2(F.relu(self.fc1(x))**self.k)\n",
    "        return u1\n",
    "    def evaluate_derivative(self, x, i):\n",
    "        if self.k == 1:\n",
    "            u1 = self.fc2(torch.heaviside(self.fc1(x),zero) * self.fc1.weight.t()[i-1:i,:] )\n",
    "        else:\n",
    "            u1 = self.fc2(self.k*F.relu(self.fc1(x))**(self.k-1) *self.fc1.weight.t()[i-1:i,:] )  \n",
    "        return u1\n",
    "\n",
    "def adjust_neuron_position(my_model, dims = 3):\n",
    "\n",
    "    def create_mesh_grid(dims, pts):\n",
    "        mesh = torch.tensor(list(itertools.product(pts,repeat=dims)))\n",
    "        vertices = mesh.reshape(len(pts) ** dims, -1) \n",
    "        return vertices\n",
    "    counter = 0 \n",
    "    # positions = torch.tensor([[0.,0.],[0.,1.],[1.,1.],[1.,0.]])\n",
    "    pts = torch.tensor([0.,1.])\n",
    "    positions = create_mesh_grid(dims,pts) \n",
    "    neuron_num = my_model.fc1.bias.size(0)\n",
    "    for i in range(neuron_num): \n",
    "        w = my_model.fc1.weight.data[i:i+1,:]\n",
    "        b = my_model.fc1.bias.data[i]\n",
    "    #     print(w,b)\n",
    "        values = torch.matmul(positions,w.T) # + b\n",
    "        left_end = - torch.max(values)\n",
    "        right_end = - torch.min(values)\n",
    "        offset = (right_end - left_end)/50\n",
    "        if b <= left_end + offset/2 : \n",
    "            b = torch.rand(1)*(right_end - left_end - offset) + left_end + offset/2 \n",
    "            my_model.fc1.bias.data[i] = b \n",
    "        if b >= right_end - offset/2 :\n",
    "            if counter < (dims+1):\n",
    "#                 print(\"here\")\n",
    "                counter += 1\n",
    "            else: # (d + 1) or more \n",
    "                b = torch.rand(1)*(right_end - left_end - offset) + left_end + offset/2 \n",
    "                my_model.fc1.bias.data[i] = b \n",
    "    return my_model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MonteCarlo_Sobol_dDim_weights_points(M ,d = 4):\n",
    "    Sob_integral = torch.quasirandom.SobolEngine(dimension =d, scramble= False, seed=None) \n",
    "    integration_points = Sob_integral.draw(M).double() \n",
    "    integration_points = integration_points.to(device)\n",
    "    weights = torch.ones(M,1).to(device)/M \n",
    "    return weights, integration_points \n",
    "\n",
    "def generate_relu_dict4D(N_list):\n",
    "    \n",
    "    N = np.prod(N_list) \n",
    "\n",
    "    grid_indices = [np.linspace(0,1,N_item,endpoint=False) for N_item in N_list]\n",
    "    grid = np.meshgrid(*grid_indices,indexing='ij')\n",
    "    grid_coordinates = np.column_stack([axis.ravel() for axis in grid]) \n",
    "    samples = torch.tensor(grid_coordinates) \n",
    "\n",
    "    T =torch.tensor([[pi,0,0,0],[0,pi,0,0],[0,0,2*pi,0],[0,0,0,2*2]]) # 2 * sqrt(d)\n",
    "    shift = torch.tensor([0,0,0,-2])\n",
    "    samples = samples@T + shift \n",
    "\n",
    "    f1 = torch.zeros(N,1) \n",
    "    f2 = torch.zeros(N,1)\n",
    "    f3 = torch.zeros(N,1)\n",
    "    f4 = torch.zeros(N,1)\n",
    "    f5 = torch.zeros(N,1)\n",
    "\n",
    "    f1[:,0] = torch.cos(samples[:,0]) \n",
    "    f2[:,0] = torch.sin(samples[:,0]) * torch.cos(samples[:,1])\n",
    "    f3[:,0] = torch.sin(samples[:,0]) * torch.sin(samples[:,1]) * torch.cos(samples[:,2])\n",
    "    f4[:,0] = torch.sin(samples[:,0]) * torch.sin(samples[:,1]) * torch.sin(samples[:,2])  \n",
    "    f5[:,0] = samples[:,3]\n",
    "\n",
    "    Wb_tensor = torch.cat([f1,f2,f3,f4,f5],1) # N x 4 \n",
    "    return Wb_tensor\n",
    "\n",
    "def generate_relu_dict4plusD_QMC(dim, s,N0):\n",
    "    # s*N0 is the total number of samples \n",
    "    samples = torch.rand(s*N0,dim)  \n",
    "\n",
    "    # for i in range(s-1):\n",
    "        # samples = torch.cat([samples,Sob.draw(N0).double()],0)\n",
    "    # Form the transformation matrix and shift vector \n",
    "    diagonal = torch.ones(dim)*pi  \n",
    "    diagonal[-1] =  2*dim**0.5\n",
    "    diagonal[-2] = 2*pi \n",
    "    T = torch.diag(diagonal)\n",
    "\n",
    "    shift = torch.zeros(dim)\n",
    "    shift[-1] = -dim**0.5 \n",
    "    samples = samples@T + shift \n",
    "\n",
    "    Wb_tensor = torch.ones(s*N0,dim+1) # each neuron parameter stored in rows  \n",
    "    for i in range(dim): # 0, 1, ... dim-1 \n",
    "        for j in range(i+1): # 0, 1, ... i \n",
    "            if i == 0: \n",
    "                Wb_tensor[:,i] = Wb_tensor[:,i]*torch.cos(samples[:,j])\n",
    "            if i == (dim - 1):\n",
    "                if j != i:\n",
    "                    Wb_tensor[:,i] = Wb_tensor[:,i] * torch.sin(samples[:,j]) \n",
    "            if i != 0 and i != (dim - 1): \n",
    "                if j != i: \n",
    "                    Wb_tensor[:,i] = Wb_tensor[:,i] * torch.sin(samples[:,j]) \n",
    "                else: \n",
    "                    Wb_tensor[:,i] = Wb_tensor[:,i] * torch.cos(samples[:,j]) \n",
    "            \n",
    "    Wb_tensor[:,dim] = samples[:,-1] \n",
    "\n",
    "    return Wb_tensor.to(device)\n",
    "\n",
    "def minimize_linear_layer_H1_explicit_assemble_efficient(model,alpha, target, g_N, weights, integration_points, w_bd, pts_bd, activation = 'relu',solver=\"direct\",memory = 2**29 ):\n",
    "    \"\"\" -div alpha grad u(x) + u = f \n",
    "    Parameters\n",
    "    ----------\n",
    "    model: \n",
    "        nn model\n",
    "    alpha:\n",
    "        alpha function\n",
    "    target:\n",
    "        rhs function f \n",
    "    pts_bd:\n",
    "        integration points on the boundary, embdedded in the domain \n",
    "    \"\"\" \n",
    "    zero = torch.tensor([0.]).to(device)\n",
    "    start_time = time.time() \n",
    "    w = model.fc1.weight.data \n",
    "    b = model.fc1.bias.data \n",
    "    neuron_num = b.size(0) \n",
    "    dim = integration_points.size(1) \n",
    "    M = integration_points.size(0)\n",
    "    coef_alpha = alpha(integration_points) # alpha  \n",
    "    \n",
    "    total_size = neuron_num * M # memory, number of floating numbers \n",
    "    print('total size: {} {} = {}'.format(neuron_num,M,total_size))\n",
    "    num_batch = total_size//memory + 1 # divide according to memory\n",
    "    print(\"num batches: \",num_batch)\n",
    "    batch_size = M//num_batch\n",
    "    start_ind = 0\n",
    "    end_ind = 0 \n",
    "    jac = torch.zeros(b.size(0),b.size(0)).to(device)\n",
    "    rhs = torch.zeros(b.size(0),1).to(device)\n",
    "\n",
    "    for j in range(0,M,batch_size): # batch operation in data points \n",
    "        end_ind = j + batch_size\n",
    "        basis_value_col = F.relu(integration_points[j:end_ind] @ w.t()+ b)**(model.k) \n",
    "        weighted_basis_value_col = basis_value_col * weights[j:end_ind] \n",
    "        jac += weighted_basis_value_col.t() @ basis_value_col \n",
    "        rhs += weighted_basis_value_col.t() @ (target(integration_points[j:end_ind,:])) \n",
    "\n",
    "    # Assemble the boundary condition term <g,v>_{\\Gamma_N} \n",
    "    size_pts_bd = int(pts_bd.size(0)/(2*dim))\n",
    "    # M_bc = size_pts_bd \n",
    "    # total_size = M_bc * neuron_num \n",
    "    # num_batch = total_size//memory + 1 \n",
    "    # batch_size = M_bc//num_batch\n",
    "    if g_N != None:\n",
    "        bcs_N = g_N(dim)\n",
    "        for ii, g_ii in bcs_N:\n",
    "            weighted_g_N = -g_ii(pts_bd[2*ii*size_pts_bd:(2*ii+1)*size_pts_bd,:])* w_bd[2*ii*size_pts_bd:(2*ii+1)*size_pts_bd,:]\n",
    "            basis_value_bd_col = F.relu(pts_bd[2*ii*size_pts_bd:(2*ii+1)*size_pts_bd,:] @ w.t()+ b)**(model.k)\n",
    "            rhs += basis_value_bd_col.t() @ weighted_g_N\n",
    "\n",
    "            weighted_g_N = g_ii(pts_bd[(2*ii+1)*size_pts_bd:(2*ii+2)*size_pts_bd,:])* w_bd[(2*ii+1)*size_pts_bd:(2*ii+2)*size_pts_bd,:]\n",
    "            basis_value_bd_col = F.relu(pts_bd[(2*ii+1)*size_pts_bd:(2*ii+2)*size_pts_bd,:] @ w.t()+ b)**(model.k)\n",
    "            rhs += basis_value_bd_col.t() @ weighted_g_N\n",
    "            \n",
    "    # Stiffness matrix term in the jacobian \n",
    "    for d in range(dim):\n",
    "        end_ind = 0 \n",
    "        if model.k == 1:  \n",
    "            for j in range(0,M,batch_size): \n",
    "                end_ind = j + batch_size \n",
    "                basis_value_dxi_col = torch.heaviside(integration_points[j:end_ind] @ w.t()+ b, zero) * w.t()[d:d+1,:]\n",
    "                weighted_basis_value_dx_col = basis_value_dxi_col * weights[j:end_ind] * coef_alpha[j:end_ind] \n",
    "                jac += weighted_basis_value_dx_col.t() @ basis_value_dxi_col \n",
    "#             basis_value_dxi_col = torch.heaviside(integration_points @ w.t()+ b, zero) * w.t()[d:d+1,:]\n",
    "#             weighted_basis_value_dx_col = basis_value_dxi_col * weights * coef_alpha \n",
    "#             jac += weighted_basis_value_dx_col.t() @ basis_value_dxi_col \n",
    "\n",
    "        else: \n",
    "            for j in range(0,M,batch_size):  \n",
    "                end_ind = j + batch_size \n",
    "                basis_value_dxi_col = model.k * F.relu(integration_points[j:end_ind] @ w.t()+ b)**(model.k-1) * w.t()[d:d+1,:]\n",
    "                weighted_basis_value_dx_col = basis_value_dxi_col * weights[j:end_ind] * coef_alpha[j:end_ind] \n",
    "                jac += weighted_basis_value_dx_col.t() @ basis_value_dxi_col \n",
    "#             basis_value_dxi_col = model.k * F.relu(integration_points @ w.t()+ b)**(model.k-1) * w.t()[d:d+1,:]\n",
    "#             weighted_basis_value_dx_col = basis_value_dxi_col * weights * coef_alpha  \n",
    "#             jac += weighted_basis_value_dx_col.t() @ basis_value_dxi_col \n",
    "\n",
    "    print(\"assembling the mass matrix time taken: \", time.time()-start_time) \n",
    "\n",
    "    start_time = time.time()    \n",
    "    if solver == \"cg\": \n",
    "        sol, exit_code = linalg.cg(np.array(jac.detach().cpu()),np.array(rhs.detach().cpu()),tol=1e-12)\n",
    "        sol = torch.tensor(sol).view(1,-1)\n",
    "    elif solver == \"direct\": \n",
    "#         sol = np.linalg.inv( np.array(jac.detach().cpu()) )@np.array(rhs.detach().cpu())\n",
    "        sol = (torch.linalg.solve( jac.detach(), rhs.detach())).view(1,-1)\n",
    "    elif solver == \"ls\":\n",
    "        sol = (torch.linalg.lstsq(jac.detach().cpu(),rhs.detach().cpu(),driver='gelsd').solution).view(1,-1)\n",
    "        # sol = (torch.linalg.lstsq(jac.detach(),rhs.detach()).solution).view(1,-1) # gpu/cpu, driver = 'gels', cannot solve singular\n",
    "    print(\"solving Ax = b time taken: \", time.time()-start_time)\n",
    "    return sol \n",
    "\n",
    "\n",
    "def OGANeumannReLU10D(my_model,alpha, target,g_N, u_exact, u_exact_grad, N_list,num_epochs,plot_freq, M, k =1, rand_deter = 'deter', linear_solver = \"direct\",memory = 2**29): \n",
    "    \"\"\" Orthogonal greedy algorithm using 1D ReLU dictionary over [-pi,pi]\n",
    "    Parameters\n",
    "    ----------\n",
    "    my_model: \n",
    "        nn model \n",
    "    target: \n",
    "        target function\n",
    "    num_epochs: int \n",
    "        number of training epochs \n",
    "    integration_intervals: int \n",
    "        number of subintervals for piecewise numerical quadrature \n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    err: tensor \n",
    "        rank 1 torch tensor to record the L2 error history  \n",
    "    model: \n",
    "        trained nn model \n",
    "    \"\"\"\n",
    "    #Todo Done\n",
    "    dim = 10 \n",
    "    gw_expand, integration_points = MonteCarlo_Sobol_dDim_weights_points(M, d=dim)\n",
    "    gw_expand = gw_expand.to(device)\n",
    "    integration_points = integration_points.to(device)\n",
    "\n",
    "    # define integration on the boundary \n",
    "    gw_expand_bd, integration_points_bd = MonteCarlo_Sobol_dDim_weights_points(M//10, d=dim-1)\n",
    "    size_pts_bd = integration_points_bd.size(0) \n",
    "    gw_expand_bd_faces = torch.tile(gw_expand_bd,(2*dim,1))\n",
    "    \n",
    "    integration_points_bd_faces = torch.zeros(2*dim*integration_points_bd.size(0),dim).to(device)\n",
    "    for ind in range(dim): \n",
    "        integration_points_bd_faces[2 *ind * size_pts_bd :(2 *ind +1) * size_pts_bd,ind:ind+1] = 0 \n",
    "        integration_points_bd_faces[(2 *ind)*size_pts_bd :(2 * ind +1) * size_pts_bd,:ind] = integration_points_bd[:,:ind]\n",
    "        integration_points_bd_faces[(2 *ind)*size_pts_bd :(2 * ind +1) * size_pts_bd,ind+1:] = integration_points_bd[:,ind:]\n",
    "\n",
    "        integration_points_bd_faces[(2 *ind +1) * size_pts_bd:(2 *ind +2)*size_pts_bd,ind:ind+1] = 1\n",
    "        integration_points_bd_faces[(2 *ind +1) * size_pts_bd:(2 *ind +2)*size_pts_bd,:ind] = integration_points_bd[:,:ind]        \n",
    "        integration_points_bd_faces[(2 *ind +1) * size_pts_bd:(2 *ind +2)*size_pts_bd,ind+1:] = integration_points_bd[:,ind:]\n",
    "\n",
    "\n",
    "    err = torch.zeros(num_epochs+1)\n",
    "    err_h10 = torch.zeros(num_epochs+1).to(device) \n",
    "    if my_model == None: \n",
    "        func_values = - target(integration_points)\n",
    "        num_neuron = 0\n",
    "\n",
    "        list_b = []\n",
    "        list_w = []\n",
    "    else: \n",
    "        func_values = - (target(integration_points) + my_model(integration_points).detach())\n",
    "        bias = my_model.fc1.bias.detach().data\n",
    "        weights = my_model.fc1.weight.detach().data\n",
    "        num_neuron = int(bias.size(0))\n",
    "\n",
    "        list_b = list(bias)\n",
    "        list_w = list(weights)\n",
    "    \n",
    "    ## l2 error \n",
    "    func_values_sqrd = func_values*func_values\n",
    "    err[0]= torch.sum(func_values_sqrd*gw_expand)**0.5\n",
    "    ## h1 seminorm \n",
    "    if u_exact_grad != None:\n",
    "        u_grad = u_exact_grad() \n",
    "        for grad_i in u_grad: \n",
    "            err_h10[0] += torch.sum((grad_i(integration_points))**2 * gw_expand)**0.5\n",
    "    \n",
    "    start_time = time.time()\n",
    "    solver = linear_solver\n",
    "\n",
    "    N0 = np.prod(N_list)\n",
    "    if rand_deter == 'deter':\n",
    "#         relu_dict_parameters = generate_relu_dict4D(N_list).to(device)\n",
    "        assert rand_deter == \"rand\", \"no deterministic dictionary, dimension\"+str(dim)+\"too large\"\n",
    "    \n",
    "    print(\"using linear solver: \",solver)\n",
    "    M2 = integration_points.size(0) # add \n",
    "    for i in range(num_epochs): \n",
    "        start_time = time.time()\n",
    "        print(\"epoch: \",i+1, end = '\\t')\n",
    "        if rand_deter == 'rand':\n",
    "            relu_dict_parameters = generate_relu_dict4plusD_QMC(dim, 1,N0).to(device) \n",
    "        if num_neuron == 0: \n",
    "            func_values = - target(integration_points)\n",
    "        else: \n",
    "            func_values = - target(integration_points) + my_model(integration_points).detach()\n",
    "\n",
    "        weight_func_values = func_values*gw_expand \n",
    "\n",
    "        ### ======================= \n",
    "        total_size = M2 * N0 \n",
    "        num_batch = total_size//memory + 1 \n",
    "        batch_size = N0//num_batch\n",
    "        output = torch.zeros(N0,1).to(device)\n",
    "        print(\"argmax batch num, \", num_batch) \n",
    "        \n",
    "        for j in range(0,N0,batch_size):  \n",
    "            end_index = j + batch_size  \n",
    "            basis_values_batch = (F.relu( torch.matmul(integration_points,relu_dict_parameters[j:end_index,0:dim].T ) - relu_dict_parameters[j:end_index,dim])**k).T # uses broadcasting    \n",
    "            output[j:end_index,:]  = torch.matmul(basis_values_batch,weight_func_values)[:,:] \n",
    "        \n",
    "        # grad u part\n",
    "        alpha_coef = alpha(integration_points) # alpha \n",
    "        if my_model!= None:\n",
    "            if k == 1:  \n",
    "                for j in range(0,N0,batch_size):  \n",
    "                    end_index = j + batch_size \n",
    "                    derivative_part = torch.heaviside(integration_points @ (relu_dict_parameters[j:end_index,0:dim].T) - relu_dict_parameters[j:end_index,dim], zero) # dimension 4 \n",
    "                    derivative_part *= alpha_coef # alpha \n",
    "                    for dx_i in range(dim): \n",
    "\n",
    "                        weight_dbasis_values_dxi =  (derivative_part * relu_dict_parameters.t()[dx_i:dx_i+1,j:end_index]) *gw_expand   \n",
    "                        dmy_model_dxi = my_model.evaluate_derivative(integration_points,dx_i+1).detach()\n",
    "                        output[j:end_index,:] += torch.matmul(weight_dbasis_values_dxi.t(), dmy_model_dxi) \n",
    "\n",
    "\n",
    "            else:  \n",
    "                for j in range(0,N0,batch_size):  \n",
    "                    end_index = j + batch_size \n",
    "                    derivative_part = k * F.relu(integration_points @ (relu_dict_parameters[j:end_index,0:dim].T) - relu_dict_parameters[j:end_index,dim])**(k-1) # dimension 4 \n",
    "                    derivative_part *= alpha_coef # alpha\n",
    "                    for dx_i in range(dim): \n",
    "\n",
    "                        weight_dbasis_values_dxi =  (derivative_part * relu_dict_parameters.t()[dx_i:dx_i+1,j:end_index]) * gw_expand    \n",
    "                        dmy_model_dxi = my_model.evaluate_derivative(integration_points,dx_i+1).detach()\n",
    "                        output[j:end_index,:] += torch.matmul(weight_dbasis_values_dxi.t(), dmy_model_dxi) \n",
    "\n",
    "        \n",
    "        #Boundary condition term -<g,v>_{\\Gamma_N}  \n",
    "        M_bc = size_pts_bd \n",
    "        total_size = M_bc * N0 \n",
    "        num_batch = total_size//memory + 1 \n",
    "        batch_size = N0//num_batch\n",
    "        if g_N != None:\n",
    "            bcs_N = g_N(dim) \n",
    "            for ii, g_ii in bcs_N: \n",
    "                \n",
    "                weighted_g_N = -g_ii(integration_points_bd_faces[2*ii*size_pts_bd:(2*ii+1)*size_pts_bd,:])* gw_expand_bd_faces[2*ii*size_pts_bd:(2*ii+1)*size_pts_bd,:]\n",
    "                ## todo \n",
    "                for j in range(0,N0,batch_size):  \n",
    "                    end_index = j + batch_size \n",
    "                    basis_values_bd_faces = (F.relu( torch.matmul(integration_points_bd_faces[2*ii*size_pts_bd:(2*ii+1)*size_pts_bd,:],relu_dict_parameters[j:end_index,0:dim].T ) - relu_dict_parameters[j:end_index,dim])**k).T\n",
    "                    output[j:end_index,:] -= torch.matmul(basis_values_bd_faces,weighted_g_N)\n",
    "                # basis_values_bd_faces = (F.relu( torch.matmul(integration_points_bd_faces[2*ii*size_pts_bd:(2*ii+1)*size_pts_bd,:],relu_dict_parameters[:,0:dim].T ) - relu_dict_parameters[:,dim])**k).T\n",
    "                # output -= torch.matmul(basis_values_bd_faces,weighted_g_N)\n",
    "                \n",
    "                weighted_g_N = g_ii(integration_points_bd_faces[(2*ii+1)*size_pts_bd:(2*ii+2)*size_pts_bd,:])* gw_expand_bd_faces[(2*ii+1)*size_pts_bd:(2*ii+2)*size_pts_bd,:]\n",
    "                ## todo  \n",
    "                for j in range(0,N0,batch_size): \n",
    "                    end_index = j + batch_size \n",
    "                    basis_values_bd_faces = (F.relu( torch.matmul(integration_points_bd_faces[(2*ii+1)*size_pts_bd:(2*ii+2)*size_pts_bd,:],relu_dict_parameters[j:end_index,0:dim].T ) - relu_dict_parameters[j:end_index,dim])**k).T\n",
    "                    output[j:end_index,:] -= torch.matmul(basis_values_bd_faces,weighted_g_N)\n",
    "\n",
    "                # basis_values_bd_faces = (F.relu( torch.matmul(integration_points_bd_faces[(2*ii+1)*size_pts_bd:(2*ii+2)*size_pts_bd,:],relu_dict_parameters[:,0:dim].T ) - relu_dict_parameters[:,dim])**k).T\n",
    "                # output -= torch.matmul(basis_values_bd_faces,weighted_g_N)\n",
    "        \n",
    "        # output = torch.abs(torch.matmul(basis_values,weight_func_values)) # \n",
    "        output = torch.abs(output)\n",
    "        neuron_index = torch.argmax(output.flatten())\n",
    "        print(\"argmax time taken, \", time.time() - start_time)\n",
    "        # print(neuron_index)\n",
    "        list_w.append(relu_dict_parameters[neuron_index,0:dim]) # dimension 4 \n",
    "        list_b.append(-relu_dict_parameters[neuron_index,dim])\n",
    "        num_neuron += 1\n",
    "        my_model = model(dim,num_neuron,1,k).to(device)\n",
    "        w_tensor = torch.stack(list_w, 0 ) \n",
    "        b_tensor = torch.tensor(list_b)\n",
    "        my_model.fc1.weight.data[:,:] = w_tensor[:,:]\n",
    "        my_model.fc1.bias.data[:] = b_tensor[:]\n",
    "\n",
    "        #Todo Done \n",
    "#         alpha = None \n",
    "        sol = minimize_linear_layer_H1_explicit_assemble_efficient(my_model,alpha, target, g_N, gw_expand, integration_points,gw_expand_bd_faces, integration_points_bd_faces,activation = 'relu',solver = solver)\n",
    "\n",
    "        my_model.fc2.weight.data[0,:] = sol[:]\n",
    "        with torch.no_grad():\n",
    "            model_values = my_model(integration_points)\n",
    "        # L2 error ||u - u_n||\n",
    "        diff_values_sqrd = (u_exact(integration_points) - model_values)**2 \n",
    "        err[i+1]= torch.sum(diff_values_sqrd*gw_expand)**0.5\n",
    "\n",
    "        # H10 error || grad(u) - grad(u_n) ||\n",
    "        if u_exact_grad != None:\n",
    "            for ind, grad_i in enumerate(u_grad):  \n",
    "                with torch.no_grad():\n",
    "                    my_model_dxi = my_model.evaluate_derivative(integration_points,ind+1).detach() \n",
    "                err_h10[i+1] += torch.sum((grad_i(integration_points) - my_model_dxi)**2 * gw_expand)**0.5\n",
    "        print(\"l2 error {:.6f}, h1 error {:.6f}\".format(err[i+1],err_h10[i+1]))\n",
    "        print()\n",
    "    print(\"time taken: \",time.time() - start_time)\n",
    "    return err, err_h10.cpu(), my_model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following tests, we compare using deterministic dictionaries with using random dictionary for the following three target functions. \n",
    "\n",
    "- $\\sin(\\pi x_1) \\sin(\\pi x_2)$ \n",
    "- $\\sin(4\\pi x_1) \\sin(8\\pi x_2)$ \n",
    "- Gabor function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using linear solver:  direct\n",
      "epoch:  1\targmax batch num,  33\n",
      "argmax time taken,  1.0013515949249268\n",
      "total size: 1 524288 = 524288\n",
      "num batches:  1\n",
      "assembling the mass matrix time taken:  0.006823539733886719\n",
      "solving Ax = b time taken:  0.07841777801513672\n",
      "l2 error 0.178288, h1 error 2.032888\n",
      "\n",
      "epoch:  2\targmax batch num,  33\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 74\u001b[0m\n\u001b[1;32m     72\u001b[0m N \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mprod(N_list)\n\u001b[1;32m     73\u001b[0m k \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m \n\u001b[0;32m---> 74\u001b[0m err_QMC2, err_h10, my_model \u001b[38;5;241m=\u001b[39m \u001b[43mOGANeumannReLU10D\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmy_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43mg_N\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mu_exact\u001b[49m\u001b[43m,\u001b[49m\u001b[43mu_exact_grad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mN_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43mplot_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mM\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrand_deter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrand\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlinear_solver\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdirect\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m save: \n\u001b[1;32m     77\u001b[0m     folder \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata-neumann/\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "Cell \u001b[0;32mIn[2], line 286\u001b[0m, in \u001b[0;36mOGANeumannReLU10D\u001b[0;34m(my_model, alpha, target, g_N, u_exact, u_exact_grad, N_list, num_epochs, plot_freq, M, k, rand_deter, linear_solver, memory)\u001b[0m\n\u001b[1;32m    283\u001b[0m                     \u001b[38;5;28;01mfor\u001b[39;00m dx_i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(dim): \n\u001b[1;32m    285\u001b[0m                         weight_dbasis_values_dxi \u001b[38;5;241m=\u001b[39m  (derivative_part \u001b[38;5;241m*\u001b[39m relu_dict_parameters\u001b[38;5;241m.\u001b[39mt()[dx_i:dx_i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m,j:end_index]) \u001b[38;5;241m*\u001b[39m gw_expand    \n\u001b[0;32m--> 286\u001b[0m                         dmy_model_dxi \u001b[38;5;241m=\u001b[39m \u001b[43mmy_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate_derivative\u001b[49m\u001b[43m(\u001b[49m\u001b[43mintegration_points\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdx_i\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mdetach()\n\u001b[1;32m    287\u001b[0m                         output[j:end_index,:] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmatmul(weight_dbasis_values_dxi\u001b[38;5;241m.\u001b[39mt(), dmy_model_dxi) \n\u001b[1;32m    289\u001b[0m \u001b[38;5;66;03m#                 derivative_part = k * F.relu(integration_points @ (relu_dict_parameters[:,0:dim].T) - relu_dict_parameters[:,dim])**(k-1) # dimension 4 \u001b[39;00m\n\u001b[1;32m    290\u001b[0m \u001b[38;5;66;03m#                 derivative_part *= alpha_coef # alpha\u001b[39;00m\n\u001b[1;32m    291\u001b[0m \u001b[38;5;66;03m#                 for dx_i in range(dim): \u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    295\u001b[0m         \n\u001b[1;32m    296\u001b[0m         \u001b[38;5;66;03m#Boundary condition term -<g,v>_{\\Gamma_N}  \u001b[39;00m\n",
      "Cell \u001b[0;32mIn[1], line 43\u001b[0m, in \u001b[0;36mmodel.evaluate_derivative\u001b[0;34m(self, x, i)\u001b[0m\n\u001b[1;32m     41\u001b[0m     u1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2(torch\u001b[38;5;241m.\u001b[39mheaviside(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc1(x),zero) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc1\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mt()[i\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:i,:] )\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 43\u001b[0m     u1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk\u001b[38;5;241m*\u001b[39mF\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc1\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mt()[i\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:i,:] )  \n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m u1\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def u_exact(x):\n",
    "    d = 10  \n",
    "    cn =   7.03/d \n",
    "    return torch.exp(-torch.sum( cn**2 * (x - 0.5)**2,dim = 1, keepdim = True))  \n",
    "\n",
    "def u_exact_grad():\n",
    "\n",
    "    def make_grad_i(i):\n",
    "        def grad_i(x):\n",
    "            d = 10  \n",
    "            cn = 7.03/d\n",
    "            return torch.exp(-torch.sum(cn**2 * (x - 0.5)**2, dim=1, keepdim=True)) * (-2 * cn**2 * (x[:, i:i+1] - 0.5))\n",
    "        return grad_i \n",
    "    \n",
    "    u_grad=[] \n",
    "    for i in range(10):\n",
    "        u_grad.append(make_grad_i(i))\n",
    "    return u_grad\n",
    "                                                                \n",
    "                                                                    \n",
    "def alpha(x): \n",
    "    return torch.ones(x.size(0),1).to(device)\n",
    "    # return 0.5 * torch.sin(6 * pi*x[:,0:1]) + 1. \n",
    "\n",
    "# def target(x):\n",
    "# #     z = (  4 * (pi)**2 + 1)*torch.cos( pi*x[:,0:1])*torch.cos( pi*x[:,1:2] ) * torch.cos(pi*x[:,2:3]) * torch.cos( pi*x[:,3:4]) \n",
    "# #     return z \n",
    "#     z_c = torch.cos( pi*x[:,0:1])*torch.cos( pi*x[:,1:2] ) * torch.cos(pi*x[:,2:3]) * torch.cos( pi*x[:,3:4]) \n",
    "#     z1 = 3 * pi**2 * torch.sin(pi * x[:,0:1]) * torch.cos( 6*pi*x[:,1:2] ) * torch.cos(pi*x[:,2:3]) * torch.cos( pi*x[:,3:4]) \n",
    "#     z2 = 0.5 * pi**2 * torch.sin(6*pi * x[:,0:1])* z_c \n",
    "#     z = z1 + z2 + 3/2*pi**2 * torch.sin(6 * pi * x[:,0:1]) * z_c \n",
    "#     z += ( 4 * (pi)**2 + 1)*z_c \n",
    "#     return z \n",
    "\n",
    "def target(x):\n",
    "    d = 10 \n",
    "    cn =   7.03/d \n",
    "    z = torch.exp(-torch.sum( cn**2 * (x - 0.5)**2,dim = 1, keepdim = True)) \n",
    "    return z* ( -torch.sum(  (2 *cn**2 * (x - 0.5))**2 - 2*cn**2 ,dim = 1, keepdim = True) +1)\n",
    "\n",
    "def g_N(dim):\n",
    "    def make_g(i):\n",
    "        def g_i(x):\n",
    "            d = 10  \n",
    "            cn = 7.03 / d\n",
    "            return torch.exp(-torch.sum(cn**2 * (x - 0.5)**2, dim=1, keepdim=True)) * (-2 * cn**2 * (x[:, i:i+1] - 0.5))\n",
    "        return g_i\n",
    "\n",
    "    bcs_N = []\n",
    "    for i in range(dim):\n",
    "        bcs_N.append((i, make_g(i)))\n",
    "    \n",
    "    return bcs_N\n",
    "\n",
    "\n",
    "function_name = \"gaussian\" \n",
    "filename_write = \"data-neumann/10DOGA-{}-order.txt\".format(function_name)\n",
    "f_write = open(filename_write, \"a\")\n",
    "f_write.write(\"\\n\")\n",
    "f_write.close() \n",
    "save = False \n",
    "for N_list in [[2**15]]: # ,[2**6,2**6],[2**7,2**7] \n",
    "    # save = True \n",
    "    f_write = open(filename_write, \"a\")\n",
    "    my_model = None \n",
    "    # Nx = 50   \n",
    "    # order = 3   \n",
    "    M = int(2**19)\n",
    "    exponent = 9 \n",
    "    num_epochs = 2**exponent  \n",
    "    plot_freq = num_epochs \n",
    "    N = np.prod(N_list)\n",
    "    k = 2 \n",
    "    err_QMC2, err_h10, my_model = OGANeumannReLU10D(my_model,alpha, target,g_N, u_exact,u_exact_grad, N_list,num_epochs,plot_freq, M, k = k, rand_deter= 'rand', linear_solver = \"direct\")\n",
    "    \n",
    "    if save: \n",
    "        folder = 'data-neumann/'\n",
    "        filename = folder + 'err_OGA_10D_{}_neuron_{}_N_{}_deterministic.pt'.format(function_name,num_epochs,N)\n",
    "        torch.save(err_QMC2,filename) \n",
    "        folder = 'data-neumann/'\n",
    "        filename = folder + 'model_OGA_10D_{}_neuron_{}_N_{}_deterministic.pt'.format(function_name,num_epochs,N)\n",
    "        torch.save(my_model,filename)\n",
    "\n",
    "    neuron_nums = [2**j for j in range(2,exponent+1)]\n",
    "    err_list = [err_QMC2[i] for i in neuron_nums ]\n",
    "    err_list2 = [err_h10[i] for i in neuron_nums ] \n",
    "    f_write.write('M:{}, relu {} \\n'.format(M,k))\n",
    "    f_write.write('randomized dictionary size: {}\\n'.format(N))\n",
    "    f_write.write(\"neuron num \\t\\t error \\t\\t order \\t\\t h10 error \\\\ order \\n\")\n",
    "    print(\"neuron num \\t\\t error \\t\\t order\")\n",
    "    for i, item in enumerate(err_list):\n",
    "        if i == 0: \n",
    "            # print(neuron_nums[i], end = \"\\t\\t\")\n",
    "            # print(item, end = \"\\t\\t\")\n",
    "            \n",
    "            # print(\"*\")\n",
    "            print(\"{} \\t\\t {:.6f} \\t\\t * \\t\\t {:.6f} \\t\\t * \\n\".format(neuron_nums[i],item, err_list2[i] ) )   \n",
    "            f_write.write(\"{} \\t\\t {} \\t\\t * \\t\\t {} \\t\\t * \\n\".format(neuron_nums[i],item, err_list2[i] ))\n",
    "        else: \n",
    "            # print(neuron_nums[i], end = \"\\t\\t\")\n",
    "            # print(item, end = \"\\t\\t\") \n",
    "            # print(np.log(err_list[i-1]/err_list[i])/np.log(2))\n",
    "            print(\"{} \\t\\t {:.6f} \\t\\t {:.6f} \\t\\t {:.6f} \\t\\t {:.6f} \\n\".format(neuron_nums[i],item,np.log(err_list[i-1]/err_list[i])/np.log(2),err_list2[i] , np.log(err_list2[i-1]/err_list2[i])/np.log(2) ) )\n",
    "            f_write.write(\"{} \\t\\t {} \\t\\t {} \\t\\t {} \\t\\t {} \\n\".format(neuron_nums[i],item,np.log(err_list[i-1]/err_list[i])/np.log(2),err_list2[i] , np.log(err_list2[i-1]/err_list2[i])/np.log(2) ))\n",
    "    f_write.write(\"\\n\")\n",
    "    f_write.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total size: 100 2097152 = 209715200\n",
      "num batches:  1\n",
      "assembling the mass matrix time taken:  0.007988691329956055\n"
     ]
    },
    {
     "ename": "_LinAlgError",
     "evalue": "torch.linalg.solve: The solver failed because the input matrix is singular.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31m_LinAlgError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 75\u001b[0m\n\u001b[1;32m     72\u001b[0m     integration_points_bd_faces[(\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39mind \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m size_pts_bd:(\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39mind \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m*\u001b[39msize_pts_bd,ind\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m:] \u001b[38;5;241m=\u001b[39m integration_points_bd[:,ind:]\n\u001b[1;32m     74\u001b[0m my_model \u001b[38;5;241m=\u001b[39m adjust_neuron_position(my_model\u001b[38;5;241m.\u001b[39mcpu(),dims\u001b[38;5;241m=\u001b[39mdim)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 75\u001b[0m sol \u001b[38;5;241m=\u001b[39m \u001b[43mminimize_linear_layer_H1_explicit_assemble_efficient\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmy_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[43m                \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mg_N\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mintegration_points\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[43m            \u001b[49m\u001b[43mw_bd\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mgw_expand_bd_faces\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpts_bd\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mintegration_points_bd_faces\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactivation\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrelu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43msolver\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdirect\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mmemory\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m29\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m \n\u001b[1;32m     79\u001b[0m my_model\u001b[38;5;241m.\u001b[39mfc2\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mdata[\u001b[38;5;241m0\u001b[39m,:] \u001b[38;5;241m=\u001b[39m sol[:]\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "Cell \u001b[0;32mIn[18], line 153\u001b[0m, in \u001b[0;36mminimize_linear_layer_H1_explicit_assemble_efficient\u001b[0;34m(model, alpha, target, g_N, weights, integration_points, w_bd, pts_bd, activation, solver, memory)\u001b[0m\n\u001b[1;32m    150\u001b[0m         sol \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(sol)\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    151\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m solver \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdirect\u001b[39m\u001b[38;5;124m\"\u001b[39m: \n\u001b[1;32m    152\u001b[0m \u001b[38;5;66;03m#         sol = np.linalg.inv( np.array(jac.detach().cpu()) )@np.array(rhs.detach().cpu())\u001b[39;00m\n\u001b[0;32m--> 153\u001b[0m         sol \u001b[38;5;241m=\u001b[39m (\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinalg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msolve\u001b[49m\u001b[43m(\u001b[49m\u001b[43m \u001b[49m\u001b[43mjac\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrhs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m solver \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mls\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    155\u001b[0m         sol \u001b[38;5;241m=\u001b[39m (torch\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mlstsq(jac\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu(),rhs\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu(),driver\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgelsd\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39msolution)\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31m_LinAlgError\u001b[0m: torch.linalg.solve: The solver failed because the input matrix is singular."
     ]
    }
   ],
   "source": [
    "## example of solving a linear pde using standard galerkin \n",
    "\n",
    "def u_exact(x):\n",
    "    d = 10  \n",
    "    cn =   7.03/d \n",
    "    return torch.exp(-torch.sum( cn**2 * (x - 0.5)**2,dim = 1, keepdim = True))  \n",
    "\n",
    "def u_exact_grad():\n",
    "\n",
    "    def make_grad_i(i):\n",
    "        def grad_i(x):\n",
    "            d = 10  \n",
    "            cn = 7.03/d\n",
    "            return torch.exp(-torch.sum(cn**2 * (x - 0.5)**2, dim=1, keepdim=True)) * (-2 * cn**2 * (x[:, i:i+1] - 0.5))\n",
    "        return grad_i \n",
    "    \n",
    "    u_grad=[] \n",
    "    for i in range(10):\n",
    "        u_grad.append(make_grad_i(i))\n",
    "    return u_grad\n",
    "                                                                                                      \n",
    "def alpha(x): \n",
    "    return torch.ones(x.size(0),1).to(device)\n",
    "\n",
    "# def target(x):\n",
    "# #     z = (  4 * (pi)**2 + 1)*torch.cos( pi*x[:,0:1])*torch.cos( pi*x[:,1:2] ) * torch.cos(pi*x[:,2:3]) * torch.cos( pi*x[:,3:4]) \n",
    "# #     return z \n",
    "#     z_c = torch.cos( pi*x[:,0:1])*torch.cos( pi*x[:,1:2] ) * torch.cos(pi*x[:,2:3]) * torch.cos( pi*x[:,3:4]) \n",
    "#     z1 = 3 * pi**2 * torch.sin(pi * x[:,0:1]) * torch.cos( 6*pi*x[:,1:2] ) * torch.cos(pi*x[:,2:3]) * torch.cos( pi*x[:,3:4]) \n",
    "#     z2 = 0.5 * pi**2 * torch.sin(6*pi * x[:,0:1])* z_c \n",
    "#     z = z1 + z2 + 3/2*pi**2 * torch.sin(6 * pi * x[:,0:1]) * z_c \n",
    "#     z += ( 4 * (pi)**2 + 1)*z_c \n",
    "#     return z \n",
    "\n",
    "def target(x):\n",
    "    d = 10 \n",
    "    cn =   7.03/d \n",
    "    z = torch.exp(-torch.sum( cn**2 * (x - 0.5)**2,dim = 1, keepdim = True)) \n",
    "    return z* ( -torch.sum(  (2 *cn**2 * (x - 0.5))**2 - 2*cn**2 ,dim = 1, keepdim = True) +1)\n",
    "\n",
    "def g_N(dim):\n",
    "    def make_g(i):\n",
    "        def g_i(x):\n",
    "            d = 10  \n",
    "            cn = 7.03 / d\n",
    "            return torch.exp(-torch.sum(cn**2 * (x - 0.5)**2, dim=1, keepdim=True)) * (-2 * cn**2 * (x[:, i:i+1] - 0.5))\n",
    "        return g_i\n",
    "\n",
    "    bcs_N = []\n",
    "    for i in range(dim):\n",
    "        bcs_N.append((i, make_g(i)))\n",
    "    \n",
    "    return bcs_N\n",
    "\n",
    "dim = 10 \n",
    "M = int(2**21) # 0.5M points \n",
    "my_model = model(dim,100,2).to(device)\n",
    "weights, integration_points = MonteCarlo_Sobol_dDim_weights_points(M ,d = dim)\n",
    "# define integration on the boundary\n",
    "gw_expand_bd, integration_points_bd = MonteCarlo_Sobol_dDim_weights_points(M//2**3, d=dim-1)\n",
    "size_pts_bd = integration_points_bd.size(0) \n",
    "gw_expand_bd_faces = torch.tile(gw_expand_bd,(2*dim,1))\n",
    "\n",
    "integration_points_bd_faces = torch.zeros(2*dim*integration_points_bd.size(0),dim).to(device)\n",
    "for ind in range(dim): \n",
    "    integration_points_bd_faces[2 *ind * size_pts_bd :(2 *ind +1) * size_pts_bd,ind:ind+1] = 0 \n",
    "    integration_points_bd_faces[(2 *ind)*size_pts_bd :(2 * ind +1) * size_pts_bd,:ind] = integration_points_bd[:,:ind]\n",
    "    integration_points_bd_faces[(2 *ind)*size_pts_bd :(2 * ind +1) * size_pts_bd,ind+1:] = integration_points_bd[:,ind:]\n",
    "\n",
    "    integration_points_bd_faces[(2 *ind +1) * size_pts_bd:(2 *ind +2)*size_pts_bd,ind:ind+1] = 1\n",
    "    integration_points_bd_faces[(2 *ind +1) * size_pts_bd:(2 *ind +2)*size_pts_bd,:ind] = integration_points_bd[:,:ind]        \n",
    "    integration_points_bd_faces[(2 *ind +1) * size_pts_bd:(2 *ind +2)*size_pts_bd,ind+1:] = integration_points_bd[:,ind:]\n",
    "\n",
    "my_model = adjust_neuron_position(my_model.cpu(),dims=dim).to(device)\n",
    "sol = minimize_linear_layer_H1_explicit_assemble_efficient(my_model,alpha, \\\n",
    "                target, g_N, weights, integration_points, \\\n",
    "            w_bd = gw_expand_bd_faces, pts_bd = integration_points_bd_faces, activation = 'relu',solver=\"direct\",memory = 2**29 ) \n",
    "\n",
    "my_model.fc2.weight.data[0,:] = sol[:]\n",
    "with torch.no_grad():\n",
    "    model_values = my_model(integration_points)\n",
    "# L2 error ||u - u_n||\n",
    "diff_values_sqrd = (u_exact(integration_points) - model_values)**2 \n",
    "err_l2= torch.sum(diff_values_sqrd*weights)**0.5\n",
    "\n",
    "print(err_l2)\n",
    "\n",
    "err_h10 = 0 \n",
    "if u_exact_grad != None:\n",
    "    u_grad = u_exact_grad()  \n",
    "    for ind, grad_i in enumerate(u_grad):  \n",
    "        with torch.no_grad():\n",
    "            my_model_dxi = my_model.evaluate_derivative(integration_points,ind+1).detach() \n",
    "        err_h10 += torch.sum((grad_i(integration_points) - my_model_dxi)**2 * weights)**0.5\n",
    "print(err_h10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\cos(\\pi x_1) \\cos( \\pi x_2) \\cos( \\pi x_3) \\cos( \\pi x_4)$  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def u_exact(x):\n",
    "    return torch.cos(pi*x[:,0:1])*torch.cos( pi*x[:,1:2]) * torch.cos(pi*x[:,2:3]) * torch.cos(pi*x[:,3:4]) \n",
    "def alpha(x): \n",
    "#     return torch.ones(x.size(0),1).to(device)\n",
    "    return 0.5 * torch.sin(6 * pi*x[:,0:1]) + 1. \n",
    "\n",
    "def target(x):\n",
    "#     z = (  4 * (pi)**2 + 1)*torch.cos( pi*x[:,0:1])*torch.cos( pi*x[:,1:2] ) * torch.cos(pi*x[:,2:3]) * torch.cos( pi*x[:,3:4]) \n",
    "#     return z \n",
    "    z_c = torch.cos( pi*x[:,0:1])*torch.cos( pi*x[:,1:2] ) * torch.cos(pi*x[:,2:3]) * torch.cos( pi*x[:,3:4]) \n",
    "    z1 = 3 * pi**2 * torch.sin(pi * x[:,0:1]) * torch.cos( 6*pi*x[:,1:2] ) * torch.cos(pi*x[:,2:3]) * torch.cos( pi*x[:,3:4]) \n",
    "    z2 = 0.5 * pi**2 * torch.sin(6*pi * x[:,0:1])* z_c \n",
    "    z = z1 + z2 + 3/2*pi**2 * torch.sin(6 * pi * x[:,0:1]) * z_c \n",
    "    z += ( 4 * (pi)**2 + 1)*z_c \n",
    "    return z \n",
    "\n",
    "function_name = \"cospix\" \n",
    "filename_write = \"data-neumann/4DOGA-{}-order.txt\".format(function_name)\n",
    "f_write = open(filename_write, \"a\")\n",
    "f_write.write(\"\\n\")\n",
    "f_write.close() \n",
    "save = False \n",
    "for N_list in [[2**2,2**2,2**2,2**2]]: # ,[2**6,2**6],[2**7,2**7] \n",
    "    # save = True \n",
    "    f_write = open(filename_write, \"a\")\n",
    "    my_model = None \n",
    "    # Nx = 50   \n",
    "    # order = 3   \n",
    "    M = 500000  \n",
    "    exponent = 8 \n",
    "    num_epochs = 2**exponent  \n",
    "    plot_freq = num_epochs \n",
    "    N = np.prod(N_list)\n",
    "    err_QMC2, my_model = OGANeumannReLU4D(my_model,alpha, target,u_exact, N_list,num_epochs,plot_freq, M, k = 1, rand_deter= 'rand', linear_solver = \"direct\")\n",
    "    \n",
    "    if save: \n",
    "        folder = 'data-neumann/'\n",
    "        filename = folder + 'err_OGA_4D_{}_neuron_{}_N_{}_deterministic.pt'.format(function_name,num_epochs,N)\n",
    "        torch.save(err_QMC2,filename) \n",
    "        folder = 'data-neumann/'\n",
    "        filename = folder + 'model_OGA_4D_{}_neuron_{}_N_{}_deterministic.pt'.format(function_name,num_epochs,N)\n",
    "        torch.save(my_model,filename)\n",
    "\n",
    "    neuron_nums = [2**j for j in range(2,exponent+1)]\n",
    "    err_list = [err_QMC2[i] for i in neuron_nums ]\n",
    "    f_write.write('deterministic dictionary size: {}\\n'.format(N))\n",
    "    f_write.write(\"neuron num \\t\\t error \\t\\t order\\n\")\n",
    "    print(\"neuron num \\t\\t error \\t\\t order\")\n",
    "    for i, item in enumerate(err_list):\n",
    "        if i == 0: \n",
    "            print(neuron_nums[i], end = \"\\t\\t\")\n",
    "            print(item, end = \"\\t\\t\")\n",
    "            print(\"*\")\n",
    "            f_write.write(\"{} \\t\\t {} \\t\\t * \\n\".format(neuron_nums[i],item))\n",
    "        else: \n",
    "            print(neuron_nums[i], end = \"\\t\\t\")\n",
    "            print(item, end = \"\\t\\t\") \n",
    "            print(np.log(err_list[i-1]/err_list[i])/np.log(2))\n",
    "            f_write.write(\"{} \\t\\t {} \\t\\t {} \\n\".format(neuron_nums[i],item,np.log(err_list[i-1]/err_list[i])/np.log(2)))\n",
    "    f_write.write(\"\\n\")\n",
    "    f_write.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
