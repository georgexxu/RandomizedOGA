{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import sys\n",
    "import sympy as sp \n",
    "from scipy.sparse import linalg\n",
    "from pathlib import Path\n",
    "import os \n",
    "if torch.cuda.is_available():  \n",
    "    device = \"cuda\" \n",
    "else:  \n",
    "    device = \"cpu\" \n",
    "\n",
    "pi = torch.tensor(np.pi,dtype=torch.float64)\n",
    "torch.set_default_tensor_type(torch.DoubleTensor)\n",
    "ZERO = torch.tensor([0.]).to(device)\n",
    "\n",
    "class model(nn.Module):\n",
    "    \"\"\" ReLU k shallow neural network\n",
    "    Parameters: \n",
    "    input size: input dimension\n",
    "    hidden_size1 : number of hidden layers \n",
    "    num_classes: output classes \n",
    "    k: degree of relu functions\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, hidden_size1, num_classes,k = 1):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size1)\n",
    "        self.fc2 = nn.Linear(hidden_size1, num_classes,bias = False)\n",
    "        self.k = k \n",
    "    def forward(self, x):\n",
    "        u1 = self.fc2(F.relu(self.fc1(x))**self.k)\n",
    "        return u1\n",
    "    def evaluate_derivative(self, x, i):\n",
    "        if self.k == 1:\n",
    "            u1 = self.fc2(torch.heaviside(self.fc1(x),ZERO) * self.fc1.weight.t()[i-1:i,:] )\n",
    "        else:\n",
    "            u1 = self.fc2(self.k*F.relu(self.fc1(x))**(self.k-1) *self.fc1.weight.t()[i-1:i,:] )  \n",
    "        return u1\n",
    "    def evaluate_2ndderivative(self,x,i,j): \n",
    "        if self.k == 2:\n",
    "            u1 = self.fc2( 2 * torch.heaviside(self.fc1(x),ZERO) * (self.fc1.weight.t()[i-1:i,:])*self.fc1.weight.t()[j-1:j,:]) \n",
    "        else:\n",
    "            u1 = self.fc2( self.k*(self.k-1)*F.relu(self.fc1(x))**(self.k-2) * (self.fc1.weight.t()[i-1:i,:])* (self.fc1.weight.t()[j-1:j,:]))  \n",
    "        return u1\n",
    "\n",
    "def plot_2D(f): \n",
    "    \n",
    "    Nx = 400\n",
    "    Ny = 400 \n",
    "    xs = np.linspace(0, 1, Nx)\n",
    "    ys = np.linspace(0, 1, Ny)\n",
    "    x, y = np.meshgrid(xs, ys, indexing='xy')\n",
    "    xy_comb = np.stack((x.flatten(),y.flatten())).T\n",
    "    xy_comb = torch.tensor(xy_comb)\n",
    "    z = f(xy_comb).reshape(Nx,Ny)\n",
    "    z = z.detach().numpy()\n",
    "    plt.figure(dpi=200)\n",
    "    ax = plt.axes(projection='3d')\n",
    "    ax.plot_surface(x , y , z )\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def plot_subdomains(my_model):\n",
    "    x_coord =torch.linspace(0,1,200)\n",
    "    wi = my_model.fc1.weight.data\n",
    "    bi = my_model.fc1.bias.data \n",
    "    for i, bias in enumerate(bi):  \n",
    "        if wi[i,1] !=0: \n",
    "            plt.plot(x_coord, - wi[i,0]/wi[i,1]*x_coord - bias/wi[i,1])\n",
    "        else: \n",
    "            plt.plot(x_coord,  - bias/wi[i,0]*torch.ones(x_coord.size()))\n",
    "\n",
    "    plt.xlim([-1,1])\n",
    "    plt.ylim([-1,1])\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    return 0   \n",
    "\n",
    "## Initialization\n",
    "def adjust_neuron_position(my_model,target=None):\n",
    "    my_model = my_model.cpu()\n",
    "    counter = 0 \n",
    "#     positions = torch.tensor([[-1.,-1.],[-1.,1.],[1.,1.],[1.,-1.]])\n",
    "    positions = torch.tensor([[0.,0.],[0.,1.],[1.,1.],[1.,0.]])\n",
    "    neuron_num = my_model.fc1.bias.size(0)\n",
    "    for i in range(neuron_num): \n",
    "        w = my_model.fc1.weight.data[i:i+1,:]\n",
    "        b = my_model.fc1.bias.data[i]\n",
    "        values = torch.matmul(positions,w.T) # + b\n",
    "        left_end = - torch.max(values)\n",
    "        right_end = - torch.min(values) \n",
    "        off_set = (right_end - left_end)/1000 \n",
    "        if b <= left_end + off_set: # nearly vanishing\n",
    "            b = torch.rand(1)*(right_end - left_end - off_set*2) + left_end + off_set \n",
    "            my_model.fc1.bias.data[i] = b \n",
    "        if b >= right_end - off_set: # nearly nonvanishing everywhere\n",
    "            if counter < 3:\n",
    "                counter += 1\n",
    "            else: # 3 or more \n",
    "                b = torch.rand(1)*(right_end - left_end - off_set*2) + left_end + off_set\n",
    "                my_model.fc1.bias.data[i] = b \n",
    "    return my_model\n",
    "\n",
    "def PiecewiseGQ2D_weights_points(Nx, order): \n",
    "    \"\"\" A slight modification of PiecewiseGQ2D function that only needs the weights and integration points.\n",
    "    Parameters\n",
    "    ----------\n",
    "\n",
    "    Nx: int \n",
    "        number of intervals along the dimension. No Ny, assume Nx = Ny\n",
    "    order: int \n",
    "        order of the Gauss Quadrature\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    long_weights: torch.tensor\n",
    "    integration_points: torch.tensor\n",
    "    \"\"\"\n",
    "\n",
    "#     print(\"order: \",order )\n",
    "    x, w = np.polynomial.legendre.leggauss(order)\n",
    "    gauss_pts = np.array(np.meshgrid(x,x,indexing='ij')).reshape(2,-1).T\n",
    "    weights =  (w*w[:,None]).ravel()\n",
    "\n",
    "    gauss_pts =torch.tensor(gauss_pts)\n",
    "    weights = torch.tensor(weights)\n",
    "\n",
    "    h = 1/Nx # 100 intervals \n",
    "    long_weights =  torch.tile(weights,(Nx**2,1))\n",
    "    long_weights = long_weights.reshape(-1,1)\n",
    "    long_weights = long_weights * h**2 /4 \n",
    "\n",
    "    integration_points = torch.tile(gauss_pts,(Nx**2,1))\n",
    "    scale_factor = h/2 \n",
    "    integration_points = scale_factor * integration_points\n",
    "\n",
    "    index = np.arange(1,Nx+1)-0.5\n",
    "    ordered_pairs = np.array(np.meshgrid(index,index,indexing='ij'))\n",
    "    ordered_pairs = ordered_pairs.reshape(2,-1).T\n",
    "\n",
    "    # print(ordered_pairs)\n",
    "    # print()\n",
    "    ordered_pairs = torch.tensor(ordered_pairs)\n",
    "    # print(ordered_pairs.size())\n",
    "    ordered_pairs = torch.tile(ordered_pairs, (1,order**2)) # number of GQ points\n",
    "    # print(ordered_pairs)\n",
    "\n",
    "    ordered_pairs =  ordered_pairs.reshape(-1,2)\n",
    "    # print(ordered_pairs)\n",
    "    translation = ordered_pairs*h \n",
    "    # print(translation)\n",
    "\n",
    "    integration_points = integration_points + translation \n",
    "#     print(integration_points.size())\n",
    "    # func_values = integrand2_torch(integration_points)\n",
    "    return long_weights.to(device), integration_points.to(device)\n",
    "\n",
    "def PiecewiseGQ3D_weights_points(Nx, order): \n",
    "    \"\"\" A slight modification of PiecewiseGQ2D function that only needs the weights and integration points.\n",
    "    Parameters\n",
    "    ----------\n",
    "\n",
    "    Nx: int \n",
    "        number of intervals along the dimension. No Ny, assume Nx = Ny\n",
    "    order: int \n",
    "        order of the Gauss Quadrature\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    long_weights: torch.tensor\n",
    "    integration_points: torch.tensor\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    target : \n",
    "        Target function \n",
    "    Nx: int \n",
    "        number of intervals along the dimension. No Ny, assume Nx = Ny\n",
    "    order: int \n",
    "        order of the Gauss Quadrature\n",
    "    \"\"\"\n",
    "\n",
    "    # print(\"order: \",order )\n",
    "    x, w = np.polynomial.legendre.leggauss(order)\n",
    "    gauss_pts = np.array(np.meshgrid(x,x,x,indexing='ij')).reshape(3,-1).T\n",
    "    weight_list = np.array(np.meshgrid(w,w,w,indexing='ij'))\n",
    "    weights =   (weight_list[0]*weight_list[1]*weight_list[2]).ravel() \n",
    "\n",
    "    gauss_pts =torch.tensor(gauss_pts)\n",
    "    weights = torch.tensor(weights)\n",
    "\n",
    "    h = 1/Nx # 100 intervals \n",
    "    long_weights =  torch.tile(weights,(Nx**3,1))\n",
    "    long_weights = long_weights.reshape(-1,1)\n",
    "    long_weights = long_weights * h**3 /8 \n",
    "\n",
    "    integration_points = torch.tile(gauss_pts,(Nx**3,1))\n",
    "    # print(\"shape of integration_points\", integration_points.size())\n",
    "    scale_factor = h/2 \n",
    "    integration_points = scale_factor * integration_points\n",
    "\n",
    "    index = np.arange(1,Nx+1)-0.5\n",
    "    ordered_pairs = np.array(np.meshgrid(index,index,index,indexing='ij'))\n",
    "    ordered_pairs = ordered_pairs.reshape(3,-1).T\n",
    "\n",
    "    # print(ordered_pairs)\n",
    "    # print()\n",
    "    ordered_pairs = torch.tensor(ordered_pairs)\n",
    "    # print(ordered_pairs.size())\n",
    "    ordered_pairs = torch.tile(ordered_pairs, (1,order**3)) # number of GQ points\n",
    "    # print(ordered_pairs)\n",
    "\n",
    "    ordered_pairs =  ordered_pairs.reshape(-1,3)\n",
    "    # print(ordered_pairs)\n",
    "    translation = ordered_pairs*h \n",
    "    # print(translation)\n",
    "\n",
    "    integration_points = integration_points + translation \n",
    "\n",
    "    return long_weights.to(device), integration_points.to(device)\n",
    "\n",
    "def minimize_linear_layer_H2_explicit_assemble_efficient_general_dim(model,target,weights, integration_points,activation = 'relu',solver=\"direct\",memory = 2**28 ):\n",
    "    \"\"\"Biharmonic equation solver\n",
    "    \\Delta^2 u + u = f, in \\Omega\n",
    "    \\Delta u = 0, \\partial_n (\\Delta u) = 0  on \\partial \\Omega\n",
    "    \"\"\"\n",
    "    # weights, integration_points = PiecewiseGQ2D_weights_points(Nx, order) \n",
    "    # integration_points.requires_grad_(True) \n",
    "    start_time = time.time() \n",
    "    w = model.fc1.weight.data \n",
    "    b = model.fc1.bias.data \n",
    "    neuron_num = b.size(0) \n",
    "    dim = integration_points.size(1) \n",
    "    M = integration_points.size(0)\n",
    "\n",
    "    total_size = neuron_num * M # memory, number of floating numbers \n",
    "    print('total size: {} {} = {}'.format(neuron_num,M,total_size))\n",
    "    num_batch = total_size//memory + 1 # divide according to memory\n",
    "    print(\"num batches: \",num_batch)\n",
    "    batch_size = M//num_batch\n",
    "    start_ind = 0\n",
    "    end_ind = 0 \n",
    "    jac = torch.zeros(b.size(0),b.size(0)).to(device)\n",
    "    rhs = torch.zeros(b.size(0),1).to(device)\n",
    "    \n",
    "    start_time = time.time() \n",
    "    for j in range(0,M,batch_size): # batch operation in data points \n",
    "        end_ind = j + batch_size\n",
    "        basis_value_col = F.relu(integration_points[j:end_ind] @ w.t()+ b)**(model.k) \n",
    "        weighted_basis_value_col = basis_value_col * weights[j:end_ind] \n",
    "        jac += weighted_basis_value_col.t() @ basis_value_col \n",
    "        rhs += weighted_basis_value_col.t() @ (target(integration_points[j:end_ind,:])) \n",
    "\n",
    "#     if activation == 'relu':\n",
    "#         assert model.k != 1, \"k must not be 1\"  \n",
    "#         basis_value_col = F.relu(integration_points @ w.t()+ b)**(model.k) \n",
    "    \n",
    "    \n",
    "#     weighted_basis_value_col = basis_value_col * weights \n",
    "#     ## assemble the mass matrix term\n",
    "#     jac = weighted_basis_value_col.t() @ basis_value_col  \n",
    "#     rhs = weighted_basis_value_col.t() @ (target(integration_points)) \n",
    "    print(\"assembling the mass matrix time taken: \", time.time()-start_time) \n",
    "    \n",
    "    \n",
    "    ## assemble the biharmonic term \n",
    "    for i in range(1,dim + 1): \n",
    "        for j in range(i,dim + 1): \n",
    "            if i == j: \n",
    "                for jj in range(0,M,batch_size):## batch operation \n",
    "                    end_ind = jj + batch_size\n",
    "                    if model.k == 2:  \n",
    "                        dxx_basis_value_col = 2 * torch.heaviside(integration_points[jj:end_ind]  @ w.t()+ b, ZERO) * (w.t()[i-1:i,:])**2 \n",
    "                    else: \n",
    "                        dxx_basis_value_col = model.k * (model.k -1) * F.relu(integration_points[jj:end_ind] @ w.t()+ b)**(model.k-2) * (w.t()[i-1:i,:])**2 \n",
    "                    weighted_dxx_basis_value_col = dxx_basis_value_col * weights[jj:end_ind] \n",
    "                    jac += weighted_dxx_basis_value_col.t() @ dxx_basis_value_col \n",
    "                \n",
    "            else: \n",
    "                for jj in range(0,M,batch_size):## batch operation \n",
    "                    end_ind = jj + batch_size\n",
    "                    if model.k == 2:  \n",
    "                        dxy_basis_value_col = 2 * torch.heaviside(integration_points[jj:end_ind] @ w.t()+ b, ZERO) * (w.t()[i-1:i,:])* (w.t()[j-1:j,:]) \n",
    "                    else: \n",
    "                        dxy_basis_value_col = model.k * (model.k -1) * F.relu(integration_points[jj:end_ind] @ w.t()+ b)**(model.k-2) * (w.t()[i-1:i,:])* (w.t()[j-1:j,:])\n",
    "                    weighted_dxy_basis_value_col = dxy_basis_value_col * weights[jj:end_ind] \n",
    "                    jac += 2 * (weighted_dxy_basis_value_col.t() @ dxy_basis_value_col) \n",
    "\n",
    "    print(\"assembling the matrix time taken: \", time.time()-start_time)   \n",
    "    \n",
    "    start_time = time.time()    \n",
    "    if solver == \"cg\": \n",
    "        sol, exit_code = linalg.cg(np.array(jac.detach().cpu()),np.array(rhs.detach().cpu()),tol=1e-12)\n",
    "        sol = torch.tensor(sol).view(1,-1)\n",
    "    elif solver == \"direct\": \n",
    "#         sol = np.linalg.inv( np.array(jac.detach().cpu()) )@np.array(rhs.detach().cpu())\n",
    "        sol = (torch.linalg.solve( jac.detach(), rhs.detach())).view(1,-1)\n",
    "    elif solver == \"ls\":\n",
    "        sol = (torch.linalg.lstsq(jac.detach().cpu(),rhs.detach().cpu(),driver='gelsd').solution).view(1,-1)\n",
    "        # sol = (torch.linalg.lstsq(jac.detach(),rhs.detach()).solution).view(1,-1) # gpu/cpu, driver = 'gels', cannot solve singular\n",
    "    print(\"solving Ax = b time taken: \", time.time()-start_time)\n",
    "    return sol \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sympy as sp\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Define 3D symbolic variables\n",
    "x_sym, y_sym, z_sym = sp.symbols('x y z')\n",
    "vars3d = [x_sym, y_sym, z_sym]\n",
    "\n",
    "# Define your 3D function, e.g., a separable extension of your 2D function\n",
    "u_expr_3d = (4 * x_sym * (x_sym - 1))**4 * (4 * y_sym * (y_sym - 1))**4 * (4 * z_sym * (z_sym - 1))**4\n",
    "\n",
    "# Define a helper function for multi-index derivatives\n",
    "def multi_derivative(expr, vars, orders):\n",
    "    for var, order in zip(vars, orders):\n",
    "        expr = sp.diff(expr, var, order)\n",
    "    return expr\n",
    "\n",
    "# For the biharmonic operator in 3D, list the terms:\n",
    "# Pure fourth derivatives\n",
    "order0_terms = [(0, 0, 0)]\n",
    "order1_terms = [(1,0,0),(0,1,0),(0,0,1)] \n",
    "order2_terms_pure = [(2, 0, 0), (0,2,0), (0, 0, 2)]\n",
    "order2_terms_mixed = [(1, 1, 0), (1, 0, 1), (0, 1, 1)]  \n",
    "pure_terms = [(4, 0, 0), (0, 4, 0), (0, 0, 4)]\n",
    "# Mixed terms (each counted twice)\n",
    "mixed_terms = [(2, 2, 0), (2, 0, 2), (0, 2, 2)]\n",
    "\n",
    "all_order_terms = order0_terms + order1_terms + order2_terms_pure + order2_terms_mixed + pure_terms + mixed_terms \n",
    "# lambdify the exact solution \n",
    "u_exact_sym_func = sp.lambdify((x_sym, y_sym,z_sym), u_expr_3d, modules='numpy')\n",
    "# Define wrapper functions that accept PyTorch tensors as input and return torch tensors.\n",
    "def u_exact(x_tensor):\n",
    "    # Assume x_tensor is a tensor of shape (N, 2) where each row is (x, y)\n",
    "    x_np = x_tensor.detach().cpu().numpy()\n",
    "    # Evaluate the symbolic function using the first and second columns\n",
    "    result_np = u_exact_sym_func(x_np[:, 0], x_np[:, 1],x_np[:, 2]).reshape(-1,1)\n",
    "    # Convert result to a torch tensor, preserving the device and dtype of the input\n",
    "    return torch.from_numpy(np.array(result_np)).to(x_tensor.device).type(x_tensor.dtype)\n",
    "\n",
    "# Compute and lambdify each 4th derivative term\n",
    "lambdified_terms = {}\n",
    "for orders in all_order_terms:\n",
    "    deriv_expr = multi_derivative(u_expr_3d, vars3d, orders)\n",
    "    func = sp.lambdify((x_sym, y_sym, z_sym), deriv_expr, modules='numpy')\n",
    "    lambdified_terms[orders] = func\n",
    "\n",
    "# Define wrapper functions that convert torch tensors and evaluate these lambdified functions\n",
    "def evaluate_term(x_tensor, orders):\n",
    "    # x_tensor assumed to be shape (N, 3)\n",
    "    x_np = x_tensor.detach().cpu().numpy()\n",
    "    result_np = lambdified_terms[orders](x_np[:, 0], x_np[:, 1], x_np[:, 2]).reshape(-1, 1)\n",
    "    return torch.from_numpy(result_np).to(x_tensor.device).type(x_tensor.dtype)\n",
    "\n",
    "def rhs_3d(x_tensor):\n",
    "    # Biharmonic operator in 3D:\n",
    "    result = (evaluate_term(x_tensor, (4, 0, 0)) +\n",
    "              evaluate_term(x_tensor, (0, 4, 0)) +\n",
    "              evaluate_term(x_tensor, (0, 0, 4)) +\n",
    "              2 * evaluate_term(x_tensor, (2, 2, 0)) +\n",
    "              2 * evaluate_term(x_tensor, (2, 0, 2)) +\n",
    "              2 * evaluate_term(x_tensor, (0, 2, 2)) +\n",
    "            evaluate_term(x_tensor, (0, 0, 0)))  # plus any lower-order term if needed\n",
    "            #    u_exact(x_tensor)) \n",
    "    return result\n",
    "\n",
    "def compute_l2_error_biharmonic(my_model,M,batch_size_2,integration_weights,integration_points):\n",
    "    ### depend on some non-local variables \n",
    "    err = 0  \n",
    "    d = integration_points.size(1) \n",
    "    zero_tuple = tuple([0]*d) \n",
    "    if my_model == None: \n",
    "        for jj in range(0,M,batch_size_2): \n",
    "            end_index = jj + batch_size_2 \n",
    "            err += integration_weights[jj:end_index,:].t()@ ( evaluate_term(integration_points[jj:end_index,:], zero_tuple))**2\n",
    "    else: \n",
    "        for jj in range(0,M,batch_size_2): \n",
    "            end_index = jj + batch_size_2 \n",
    "            err += integration_weights[jj:end_index,:].t()@ ( evaluate_term(integration_points[jj:end_index,:], zero_tuple)  - my_model(integration_points[jj:end_index,:]).detach())**2\n",
    "    return err**0.5  \n",
    "\n",
    "def compute_h2_error_biharmonic(my_model,M,batch_size_2,integration_weights,integration_points):\n",
    "    ### depend on some non-local variables \n",
    "    errh2 = torch.zeros(1,1).to(device) \n",
    "    if my_model == None: \n",
    "        for kk in range(0,M,batch_size_2): \n",
    "            end_index = kk + batch_size_2 \n",
    "            for multi_index in order2_terms_pure: \n",
    "                ii = [i+1 for i, value in enumerate(multi_index) if value != 0][0] \n",
    "                errh2 += integration_weights[kk:end_index,:].t()@(evaluate_term(integration_points[kk:end_index,:],orders= multi_index))**2\n",
    "            for multi_index in order2_terms_mixed:\n",
    "                ii,jj = [i+1 for i, value in enumerate(multi_index) if value != 0] \n",
    "                errh2 += 2 * integration_weights[kk:end_index,:].t()@(evaluate_term(integration_points[kk:end_index,:],orders= multi_index))**2\n",
    "    else: \n",
    "        for kk in range(0,M,batch_size_2): \n",
    "            end_index = kk + batch_size_2 \n",
    "            for multi_index in order2_terms_pure: \n",
    "                ii = [i+1 for i, value in enumerate(multi_index) if value != 0][0] \n",
    "                errh2 += integration_weights[kk:end_index,:].t()@(evaluate_term(integration_points[kk:end_index,:],orders= multi_index) - my_model.evaluate_2ndderivative(integration_points[kk:end_index,:],ii,ii).detach())**2\n",
    "            for multi_index in order2_terms_mixed:\n",
    "                ii,jj = [i+1 for i, value in enumerate(multi_index) if value != 0] \n",
    "                errh2 += 2 * integration_weights[kk:end_index,:].t()@(evaluate_term(integration_points[kk:end_index,:],orders= multi_index) - my_model.evaluate_2ndderivative(integration_points[kk:end_index,:],ii,jj).detach())**2\n",
    "\n",
    "    return errh2**0.5  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def OGABiharmonicReLU3D(my_model,target, N_list,num_epochs, Nx, order, k =1, rand_deter = 'rand', linear_solver = \"direct\",memory = 2**28): \n",
    "\n",
    "    integration_weights, integration_points = PiecewiseGQ3D_weights_points(Nx, order)\n",
    "\n",
    "    err = torch.zeros(num_epochs+1)\n",
    "    errh2 = torch.zeros(num_epochs+1)\n",
    "    if my_model == None: \n",
    "        func_values = target(integration_points)\n",
    "        num_neuron = 0\n",
    "\n",
    "        list_b = []\n",
    "        list_w = []\n",
    "    else: \n",
    "        func_values = target(integration_points) - my_model(integration_points).detach()\n",
    "        bias = my_model.fc1.bias.detach().data\n",
    "        weights = my_model.fc1.weight.detach().data\n",
    "        num_neuron = int(bias.size(0))\n",
    "\n",
    "        list_b = list(bias)\n",
    "        list_w = list(weights)\n",
    "    \n",
    "    # initial error Todo Done\n",
    "    func_values_sqrd = func_values*func_values\n",
    "    M = integration_points.size(0) \n",
    "    dim = integration_points.size(1)  \n",
    "\n",
    "    num_neuron = 0 if my_model == None else int(my_model.fc1.bias.detach().data.size(0))\n",
    "    total_size2 = M*(num_neuron+1)\n",
    "    num_batch2 = total_size2//memory + 1 \n",
    "    batch_size_2 = M//num_batch2 # in\n",
    "\n",
    "    err[0]= compute_l2_error_biharmonic(my_model,M,batch_size_2,integration_weights,integration_points)\n",
    "    errh2[0] = compute_h2_error_biharmonic(my_model,M,batch_size_2,integration_weights,integration_points)\n",
    "    start_time = time.time()\n",
    "    solver = linear_solver\n",
    "\n",
    "    N0 = np.prod(N_list)\n",
    "\n",
    "    for i in range(num_epochs): \n",
    "        print(\"epoch: \",i+1, end = '\\t')\n",
    "        if rand_deter == 'rand':\n",
    "            relu_dict_parameters = generate_relu_dict4plusD_sphere(dim,1,N0).to(device)  \n",
    "\n",
    "        neuron_num = my_model.fc2.weight.size(1) if my_model != None else 0\n",
    "        total_size2 = M*(neuron_num+1)\n",
    "        num_batch2 = total_size2//memory + 1 \n",
    "        batch_size_2 = M//num_batch2 # integration points \n",
    "        print(\"num batches argmax: \",num_batch2)\n",
    "        func_values = torch.zeros(M,1).to(device)  \n",
    "\n",
    "        if num_neuron == 0: \n",
    "            for jj in range(0,M,batch_size_2): \n",
    "                end_index = jj + batch_size_2\n",
    "                func_values[jj:end_index] += - target(integration_points[jj:end_index])\n",
    "        else: \n",
    "            for jj in range(0,M,batch_size_2): \n",
    "                end_index = jj + batch_size_2\n",
    "                func_values[jj:end_index] += - target(integration_points[jj:end_index])\n",
    "                func_values[jj:end_index] += my_model(integration_points[jj:end_index]).detach()\n",
    "\n",
    "        weight_func_values = func_values*integration_weights  \n",
    "        total_size = M * N0 \n",
    "        num_batch = total_size//memory + 1 \n",
    "        batch_size_1 = N0//num_batch # dictionary elements\n",
    "        output1 = torch.zeros(N0,1).to(device) \n",
    "        print(\"num batches argmax: \",num_batch)\n",
    "        for jj in range(0,N0,batch_size_1):  \n",
    "            end_index = jj + batch_size_1 \n",
    "            basis_values = (F.relu( torch.matmul(integration_points,relu_dict_parameters[jj:end_index,0:dim].T ) - relu_dict_parameters[jj:end_index,dim])**k) # uses broadcasting\n",
    "            output1[jj:end_index] += basis_values.t()@weight_func_values #\n",
    "\n",
    "        output2 = torch.zeros(output1.size()).to(device) \n",
    "        assert k != 1, \"k must not be 1\"  \n",
    "        for ii in range(1,dim+1):\n",
    "            for jj in range(ii,dim+1):\n",
    "                for kk in range(0,M,batch_size_2): \n",
    "                    end_index = kk + batch_size_2 \n",
    "                    if ii == jj: \n",
    "                        if k == 2:  \n",
    "                            dxx_basis_values = 2 * torch.heaviside(integration_points[kk:end_index,:] @ (relu_dict_parameters[:,0:dim].T) - relu_dict_parameters[:,dim], ZERO) * (relu_dict_parameters.t()[ii-1:ii,:])**2  \n",
    "                            if my_model!= None:\n",
    "                                dxx_my_model = my_model.evaluate_2ndderivative(integration_points[kk:end_index,:],ii,ii).detach()\n",
    "                                output2 += dxx_basis_values.t() @ (dxx_my_model*integration_weights[kk:end_index,:]) \n",
    "                        else:  \n",
    "                            dxx_basis_values = k *(k-1) * F.relu(integration_points[kk:end_index,:] @ (relu_dict_parameters[:,0:dim].T) - relu_dict_parameters[:,dim])**(k-2) * (relu_dict_parameters.t()[ii-1:ii,:])**2 \n",
    "\n",
    "                            if my_model!= None:\n",
    "                                dxx_my_model = my_model.evaluate_2ndderivative(integration_points[kk:end_index,:],ii,ii).detach()\n",
    "\n",
    "                                output2 += dxx_basis_values.t() @ (dxx_my_model*integration_weights[kk:end_index,:]) \n",
    "                    else:    \n",
    "                        if k == 2:  \n",
    "                            dxy_basis_values = 2 * torch.heaviside(integration_points[kk:end_index,:] @ (relu_dict_parameters[:,0:dim].T) - relu_dict_parameters[:,dim], ZERO) * (relu_dict_parameters.t()[ii-1:ii,:]) * (relu_dict_parameters.t()[jj-1:jj,:]) \n",
    "                            if my_model!= None:\n",
    "                                dxy_my_model = my_model.evaluate_2ndderivative(integration_points[kk:end_index,:],ii,jj).detach() \n",
    "                                output2 += 2 * dxy_basis_values.t() @ (dxy_my_model*integration_weights[kk:end_index,:])  \n",
    "                        else:  \n",
    "                            dxy_basis_values = k *(k-1) * F.relu(integration_points[kk:end_index,:] @ (relu_dict_parameters[:,0:dim].T) - relu_dict_parameters[:,dim])**(k-2) * (relu_dict_parameters.t()[ii-1:ii,:]) * (relu_dict_parameters.t()[jj-1:jj,:])\n",
    "                            if my_model!= None:\n",
    "                                dxy_my_model = my_model.evaluate_2ndderivative(integration_points[kk:end_index,:],ii,jj).detach() \n",
    "                                output2 += 2 *  dxy_basis_values.t() @ (dxy_my_model*integration_weights[kk:end_index,:])  \n",
    "\n",
    "        if my_model!= None:\n",
    "            output = torch.abs(output1 + output2) \n",
    "        else: \n",
    "            output = torch.abs(output1) \n",
    "        # output = torch.abs(torch.matmul(basis_values,weight_func_values)) # \n",
    "        neuron_index = torch.argmax(output.flatten())\n",
    "        \n",
    "        # print(neuron_index)\n",
    "        list_w.append(relu_dict_parameters[neuron_index,0:dim]) # \n",
    "        list_b.append(-relu_dict_parameters[neuron_index,dim])\n",
    "        num_neuron += 1\n",
    "        my_model = model(dim,num_neuron,1,k).to(device)\n",
    "        w_tensor = torch.stack(list_w, 0 ) \n",
    "        b_tensor = torch.tensor(list_b)\n",
    "        my_model.fc1.weight.data[:,:] = w_tensor[:,:]\n",
    "        my_model.fc1.bias.data[:] = b_tensor[:]\n",
    "\n",
    "        #Todo Done \n",
    "        # sol = minimize_linear_layer_H2_explicit_assemble_efficient(my_model,target,gw_expand, integration_points,activation = 'relu',solver = solver)\n",
    "        sol = minimize_linear_layer_H2_explicit_assemble_efficient_general_dim(my_model,target,integration_weights, integration_points,activation = 'relu',solver = solver,memory = memory)\n",
    "        my_model.fc2.weight.data[0,:] = sol[:]\n",
    "\n",
    "        # L2 error ||u - u_n|| and H2 error \n",
    "        num_neuron = 0 if my_model == None else int(my_model.fc1.bias.detach().data.size(0))\n",
    "        total_size2 = M*(num_neuron+1)\n",
    "        num_batch2 = total_size2//memory + 1 \n",
    "        batch_size_2 = M//num_batch2 # in\n",
    "        err[i+1]= compute_l2_error_biharmonic(my_model,M,batch_size_2,integration_weights,integration_points)\n",
    "        errh2[i+1] = compute_h2_error_biharmonic(my_model,M,batch_size_2,integration_weights,integration_points)\n",
    "\n",
    "    print(\"time taken: \",time.time() - start_time)\n",
    "    return err, errh2, my_model\n",
    "\n",
    "def generate_relu_dict4plusD_sphere(dim, s,N0): # \n",
    "    samples = torch.randn(s*N0,dim +1) \n",
    "    samples = samples/samples.norm(dim=1,keepdim=True)  \n",
    "    Wb = samples \n",
    "    return Wb "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_convergence_order(err_l2,err_h10,exponent,dict_size, filename,write2file = False):\n",
    "    \n",
    "    if write2file:\n",
    "        file_mode = \"a\" if os.path.exists(filename) else \"w\"\n",
    "        f_write = open(filename, file_mode)\n",
    "    \n",
    "    neuron_nums = [2**j for j in range(2,exponent+1)]\n",
    "    err_list = [err_l2[i] for i in neuron_nums ]\n",
    "    err_list2 = [err_h10[i] for i in neuron_nums ] \n",
    "    # f_write.write('M:{}, relu {} \\n'.format(M,k))\n",
    "    if write2file:\n",
    "        f_write.write('dictionary size: {}\\n'.format(dict_size))\n",
    "        f_write.write(\"neuron num \\t\\t error \\t\\t order \\t\\t h10 error \\\\ order \\n\")\n",
    "    print(\"neuron num \\t\\t error \\t\\t order\")\n",
    "    for i, item in enumerate(err_list):\n",
    "        if i == 0: \n",
    "            # print(neuron_nums[i], end = \"\\t\\t\")\n",
    "            # print(item, end = \"\\t\\t\")\n",
    "            \n",
    "            # print(\"*\")\n",
    "            print(\"{} \\t\\t {:.6f} \\t\\t * \\t\\t {:.6f} \\t\\t * \\n\".format(neuron_nums[i],item, err_list2[i] ) )\n",
    "            if write2file: \n",
    "                f_write.write(\"{} \\t\\t {} \\t\\t * \\t\\t {} \\t\\t * \\n\".format(neuron_nums[i],item, err_list2[i] ))\n",
    "        else: \n",
    "            # print(neuron_nums[i], end = \"\\t\\t\")\n",
    "            # print(item, end = \"\\t\\t\") \n",
    "            # print(np.log(err_list[i-1]/err_list[i])/np.log(2))\n",
    "            print(\"{} \\t\\t {:.6f} \\t\\t {:.6f} \\t\\t {:.6f} \\t\\t {:.6f} \\n\".format(neuron_nums[i],item,np.log(err_list[i-1]/err_list[i])/np.log(2),err_list2[i] , np.log(err_list2[i-1]/err_list2[i])/np.log(2) ) )\n",
    "            if write2file: \n",
    "                f_write.write(\"{} \\t\\t {} \\t\\t {} \\t\\t {} \\t\\t {} \\n\".format(neuron_nums[i],item,np.log(err_list[i-1]/err_list[i])/np.log(2),err_list2[i] , np.log(err_list2[i-1]/err_list2[i])/np.log(2) ))\n",
    "    if write2file:     \n",
    "        f_write.write(\"\\n\")\n",
    "        f_write.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  1\tnum batches argmax:  1\n",
      "num batches argmax:  1\n",
      "total size: 1 125000 = 125000\n",
      "num batches:  1\n",
      "assembling the mass matrix time taken:  0.1059567928314209\n",
      "assembling the matrix time taken:  0.11044979095458984\n",
      "solving Ax = b time taken:  0.00026488304138183594\n",
      "epoch:  2\tnum batches argmax:  1\n",
      "num batches argmax:  1\n",
      "total size: 2 125000 = 250000\n",
      "num batches:  1\n",
      "assembling the mass matrix time taken:  0.11887907981872559\n",
      "assembling the matrix time taken:  0.14170193672180176\n",
      "solving Ax = b time taken:  0.0001468658447265625\n",
      "epoch:  3\tnum batches argmax:  1\n",
      "num batches argmax:  1\n",
      "total size: 3 125000 = 375000\n",
      "num batches:  1\n",
      "assembling the mass matrix time taken:  0.13043880462646484\n",
      "assembling the matrix time taken:  0.15095877647399902\n",
      "solving Ax = b time taken:  0.00013399124145507812\n",
      "epoch:  4\tnum batches argmax:  1\n",
      "num batches argmax:  1\n",
      "total size: 4 125000 = 500000\n",
      "num batches:  1\n",
      "assembling the mass matrix time taken:  0.10462403297424316\n",
      "assembling the matrix time taken:  0.12853503227233887\n",
      "solving Ax = b time taken:  0.0001621246337890625\n",
      "epoch:  5\tnum batches argmax:  1\n",
      "num batches argmax:  1\n",
      "total size: 5 125000 = 625000\n",
      "num batches:  1\n",
      "assembling the mass matrix time taken:  0.10636591911315918\n",
      "assembling the matrix time taken:  0.12614703178405762\n",
      "solving Ax = b time taken:  0.00020003318786621094\n",
      "epoch:  6\tnum batches argmax:  1\n",
      "num batches argmax:  1\n",
      "total size: 6 125000 = 750000\n",
      "num batches:  1\n",
      "assembling the mass matrix time taken:  0.10607790946960449\n",
      "assembling the matrix time taken:  0.1381850242614746\n",
      "solving Ax = b time taken:  0.00016307830810546875\n",
      "epoch:  7\tnum batches argmax:  1\n",
      "num batches argmax:  1\n",
      "total size: 7 125000 = 875000\n",
      "num batches:  1\n",
      "assembling the mass matrix time taken:  0.10950708389282227\n",
      "assembling the matrix time taken:  0.1451730728149414\n",
      "solving Ax = b time taken:  0.0002129077911376953\n",
      "epoch:  8\tnum batches argmax:  1\n",
      "num batches argmax:  1\n",
      "total size: 8 125000 = 1000000\n",
      "num batches:  1\n",
      "assembling the mass matrix time taken:  0.1061100959777832\n",
      "assembling the matrix time taken:  0.14124608039855957\n",
      "solving Ax = b time taken:  0.000186920166015625\n",
      "epoch:  9\tnum batches argmax:  1\n",
      "num batches argmax:  1\n",
      "total size: 9 125000 = 1125000\n",
      "num batches:  1\n",
      "assembling the mass matrix time taken:  0.11414194107055664\n",
      "assembling the matrix time taken:  0.15941524505615234\n",
      "solving Ax = b time taken:  0.00020694732666015625\n",
      "epoch:  10\tnum batches argmax:  1\n",
      "num batches argmax:  1\n",
      "total size: 10 125000 = 1250000\n",
      "num batches:  1\n",
      "assembling the mass matrix time taken:  0.11847591400146484\n",
      "assembling the matrix time taken:  0.16983604431152344\n",
      "solving Ax = b time taken:  0.0001800060272216797\n",
      "epoch:  11\tnum batches argmax:  1\n",
      "num batches argmax:  1\n",
      "total size: 11 125000 = 1375000\n",
      "num batches:  1\n",
      "assembling the mass matrix time taken:  0.1182408332824707\n",
      "assembling the matrix time taken:  0.1831049919128418\n",
      "solving Ax = b time taken:  0.00019884109497070312\n",
      "epoch:  12\tnum batches argmax:  1\n",
      "num batches argmax:  1\n",
      "total size: 12 125000 = 1500000\n",
      "num batches:  1\n",
      "assembling the mass matrix time taken:  0.11707019805908203\n",
      "assembling the matrix time taken:  0.17593908309936523\n",
      "solving Ax = b time taken:  0.00017905235290527344\n",
      "epoch:  13\tnum batches argmax:  1\n",
      "num batches argmax:  1\n",
      "total size: 13 125000 = 1625000\n",
      "num batches:  1\n",
      "assembling the mass matrix time taken:  0.11626005172729492\n",
      "assembling the matrix time taken:  0.17261099815368652\n",
      "solving Ax = b time taken:  0.00017786026000976562\n",
      "epoch:  14\tnum batches argmax:  1\n",
      "num batches argmax:  1\n",
      "total size: 14 125000 = 1750000\n",
      "num batches:  1\n",
      "assembling the mass matrix time taken:  0.11920905113220215\n",
      "assembling the matrix time taken:  0.19609880447387695\n",
      "solving Ax = b time taken:  0.000186920166015625\n",
      "epoch:  15\tnum batches argmax:  1\n",
      "num batches argmax:  1\n",
      "total size: 15 125000 = 1875000\n",
      "num batches:  1\n",
      "assembling the mass matrix time taken:  0.12216806411743164\n",
      "assembling the matrix time taken:  0.20029592514038086\n",
      "solving Ax = b time taken:  0.00019407272338867188\n",
      "epoch:  16\tnum batches argmax:  1\n",
      "num batches argmax:  1\n",
      "total size: 16 125000 = 2000000\n",
      "num batches:  1\n",
      "assembling the mass matrix time taken:  0.12108802795410156\n",
      "assembling the matrix time taken:  0.19295883178710938\n",
      "solving Ax = b time taken:  0.0001857280731201172\n",
      "epoch:  17\tnum batches argmax:  1\n",
      "num batches argmax:  1\n",
      "total size: 17 125000 = 2125000\n",
      "num batches:  1\n",
      "assembling the mass matrix time taken:  0.12267923355102539\n",
      "assembling the matrix time taken:  0.20270013809204102\n",
      "solving Ax = b time taken:  0.0002999305725097656\n",
      "epoch:  18\tnum batches argmax:  1\n",
      "num batches argmax:  1\n",
      "total size: 18 125000 = 2250000\n",
      "num batches:  1\n",
      "assembling the mass matrix time taken:  0.12507915496826172\n",
      "assembling the matrix time taken:  0.22417998313903809\n",
      "solving Ax = b time taken:  0.0003502368927001953\n",
      "epoch:  19\tnum batches argmax:  1\n",
      "num batches argmax:  1\n",
      "total size: 19 125000 = 2375000\n",
      "num batches:  1\n",
      "assembling the mass matrix time taken:  0.13205885887145996\n",
      "assembling the matrix time taken:  0.23377108573913574\n",
      "solving Ax = b time taken:  0.00042891502380371094\n",
      "epoch:  20\tnum batches argmax:  1\n",
      "num batches argmax:  1\n",
      "total size: 20 125000 = 2500000\n",
      "num batches:  1\n",
      "assembling the mass matrix time taken:  0.12595701217651367\n",
      "assembling the matrix time taken:  0.2193918228149414\n",
      "solving Ax = b time taken:  0.0003859996795654297\n",
      "epoch:  21\tnum batches argmax:  1\n",
      "num batches argmax:  1\n",
      "total size: 21 125000 = 2625000\n",
      "num batches:  1\n",
      "assembling the mass matrix time taken:  0.12677001953125\n",
      "assembling the matrix time taken:  0.22121024131774902\n",
      "solving Ax = b time taken:  0.0004909038543701172\n",
      "epoch:  22\tnum batches argmax:  1\n",
      "num batches argmax:  1\n",
      "total size: 22 125000 = 2750000\n",
      "num batches:  1\n",
      "assembling the mass matrix time taken:  0.1272871494293213\n",
      "assembling the matrix time taken:  0.22622919082641602\n",
      "solving Ax = b time taken:  0.0004138946533203125\n",
      "epoch:  23\tnum batches argmax:  1\n",
      "num batches argmax:  1\n",
      "total size: 23 125000 = 2875000\n",
      "num batches:  1\n",
      "assembling the mass matrix time taken:  0.1275157928466797\n",
      "assembling the matrix time taken:  0.22606492042541504\n",
      "solving Ax = b time taken:  0.0004830360412597656\n",
      "epoch:  24\tnum batches argmax:  1\n",
      "num batches argmax:  1\n",
      "total size: 24 125000 = 3000000\n",
      "num batches:  1\n",
      "assembling the mass matrix time taken:  0.12910795211791992\n",
      "assembling the matrix time taken:  0.2368009090423584\n",
      "solving Ax = b time taken:  0.0003960132598876953\n",
      "epoch:  25\tnum batches argmax:  1\n",
      "num batches argmax:  1\n",
      "total size: 25 125000 = 3125000\n",
      "num batches:  1\n",
      "assembling the mass matrix time taken:  0.13063812255859375\n",
      "assembling the matrix time taken:  0.25372314453125\n",
      "solving Ax = b time taken:  0.00046896934509277344\n",
      "epoch:  26\tnum batches argmax:  1\n",
      "num batches argmax:  1\n",
      "total size: 26 125000 = 3250000\n",
      "num batches:  1\n",
      "assembling the mass matrix time taken:  0.13164687156677246\n",
      "assembling the matrix time taken:  0.25229692459106445\n",
      "solving Ax = b time taken:  0.00046181678771972656\n",
      "epoch:  27\tnum batches argmax:  1\n",
      "num batches argmax:  1\n",
      "total size: 27 125000 = 3375000\n",
      "num batches:  1\n",
      "assembling the mass matrix time taken:  0.1374812126159668\n",
      "assembling the matrix time taken:  0.27318501472473145\n",
      "solving Ax = b time taken:  0.0004951953887939453\n",
      "epoch:  28\tnum batches argmax:  1\n",
      "num batches argmax:  1\n",
      "total size: 28 125000 = 3500000\n",
      "num batches:  1\n",
      "assembling the mass matrix time taken:  0.15567517280578613\n",
      "assembling the matrix time taken:  0.286876916885376\n",
      "solving Ax = b time taken:  0.0004971027374267578\n",
      "epoch:  29\tnum batches argmax:  1\n",
      "num batches argmax:  1\n",
      "total size: 29 125000 = 3625000\n",
      "num batches:  1\n",
      "assembling the mass matrix time taken:  0.13425183296203613\n",
      "assembling the matrix time taken:  0.2617199420928955\n",
      "solving Ax = b time taken:  0.000431060791015625\n",
      "epoch:  30\tnum batches argmax:  1\n",
      "num batches argmax:  1\n",
      "total size: 30 125000 = 3750000\n",
      "num batches:  1\n",
      "assembling the mass matrix time taken:  0.13414597511291504\n",
      "assembling the matrix time taken:  0.27084994316101074\n",
      "solving Ax = b time taken:  0.0004589557647705078\n",
      "epoch:  31\tnum batches argmax:  1\n",
      "num batches argmax:  1\n",
      "total size: 31 125000 = 3875000\n",
      "num batches:  1\n",
      "assembling the mass matrix time taken:  0.13572406768798828\n",
      "assembling the matrix time taken:  0.27147507667541504\n",
      "solving Ax = b time taken:  0.00047898292541503906\n",
      "epoch:  32\tnum batches argmax:  1\n",
      "num batches argmax:  1\n",
      "total size: 32 125000 = 4000000\n",
      "num batches:  1\n",
      "assembling the mass matrix time taken:  0.1130681037902832\n",
      "assembling the matrix time taken:  0.24952125549316406\n",
      "solving Ax = b time taken:  0.0002951622009277344\n",
      "time taken:  23.192259073257446\n",
      "neuron num \t\t error \t\t order\n",
      "4 \t\t 0.340501 \t\t * \t\t 11.950635 \t\t * \n",
      "\n",
      "8 \t\t 0.684306 \t\t -1.006982 \t\t 11.821322 \t\t 0.015696 \n",
      "\n",
      "16 \t\t 0.513621 \t\t 0.413937 \t\t 11.473597 \t\t 0.043074 \n",
      "\n",
      "32 \t\t 0.503503 \t\t 0.028705 \t\t 10.077842 \t\t 0.187131 \n",
      "\n",
      "epoch:  1\tnum batches argmax:  1\n",
      "num batches argmax:  1\n",
      "total size: 1 125000 = 125000\n",
      "num batches:  1\n",
      "assembling the mass matrix time taken:  0.10340309143066406\n",
      "assembling the matrix time taken:  0.10607576370239258\n",
      "solving Ax = b time taken:  0.00011420249938964844\n",
      "epoch:  2\tnum batches argmax:  1\n",
      "num batches argmax:  1\n",
      "total size: 2 125000 = 250000\n",
      "num batches:  1\n",
      "assembling the mass matrix time taken:  0.1041111946105957\n",
      "assembling the matrix time taken:  0.12191319465637207\n",
      "solving Ax = b time taken:  0.00013017654418945312\n",
      "epoch:  3\tnum batches argmax:  1\n",
      "num batches argmax:  1\n",
      "total size: 3 125000 = 375000\n",
      "num batches:  1\n",
      "assembling the mass matrix time taken:  0.10344076156616211\n",
      "assembling the matrix time taken:  0.11244988441467285\n",
      "solving Ax = b time taken:  0.00012421607971191406\n",
      "epoch:  4\tnum batches argmax:  1\n",
      "num batches argmax:  1\n",
      "total size: 4 125000 = 500000\n",
      "num batches:  1\n",
      "assembling the mass matrix time taken:  0.10389924049377441\n",
      "assembling the matrix time taken:  0.11643624305725098\n",
      "solving Ax = b time taken:  0.00014519691467285156\n",
      "epoch:  5\tnum batches argmax:  1\n",
      "num batches argmax:  1\n",
      "total size: 5 125000 = 625000\n",
      "num batches:  1\n",
      "assembling the mass matrix time taken:  0.10507798194885254\n",
      "assembling the matrix time taken:  0.1209099292755127\n",
      "solving Ax = b time taken:  0.00014781951904296875\n",
      "epoch:  6\tnum batches argmax:  1\n",
      "num batches argmax:  1\n",
      "total size: 6 125000 = 750000\n",
      "num batches:  1\n",
      "assembling the mass matrix time taken:  0.10787391662597656\n",
      "assembling the matrix time taken:  0.12629103660583496\n",
      "solving Ax = b time taken:  0.00015401840209960938\n",
      "epoch:  7\tnum batches argmax:  1\n",
      "num batches argmax:  1\n",
      "total size: 7 125000 = 875000\n",
      "num batches:  1\n",
      "assembling the mass matrix time taken:  0.10601115226745605\n",
      "assembling the matrix time taken:  0.12806916236877441\n",
      "solving Ax = b time taken:  0.00015783309936523438\n",
      "epoch:  8\tnum batches argmax:  1\n",
      "num batches argmax:  1\n",
      "total size: 8 125000 = 1000000\n",
      "num batches:  1\n",
      "assembling the mass matrix time taken:  0.10598087310791016\n",
      "assembling the matrix time taken:  0.12709903717041016\n",
      "solving Ax = b time taken:  0.0001690387725830078\n",
      "epoch:  9\tnum batches argmax:  1\n",
      "num batches argmax:  1\n",
      "total size: 9 125000 = 1125000\n",
      "num batches:  1\n",
      "assembling the mass matrix time taken:  0.11299324035644531\n",
      "assembling the matrix time taken:  0.15735507011413574\n",
      "solving Ax = b time taken:  0.00020813941955566406\n",
      "epoch:  10\tnum batches argmax:  1\n",
      "num batches argmax:  1\n",
      "total size: 10 125000 = 1250000\n",
      "num batches:  1\n",
      "assembling the mass matrix time taken:  0.11694502830505371\n",
      "assembling the matrix time taken:  0.18386101722717285\n",
      "solving Ax = b time taken:  0.0001747608184814453\n",
      "epoch:  11\tnum batches argmax:  1\n",
      "num batches argmax:  1\n",
      "total size: 11 125000 = 1375000\n",
      "num batches:  1\n",
      "assembling the mass matrix time taken:  0.11792993545532227\n",
      "assembling the matrix time taken:  0.18665480613708496\n",
      "solving Ax = b time taken:  0.00018978118896484375\n",
      "epoch:  12\tnum batches argmax:  1\n",
      "num batches argmax:  1\n",
      "total size: 12 125000 = 1500000\n",
      "num batches:  1\n",
      "assembling the mass matrix time taken:  0.11361408233642578\n",
      "assembling the matrix time taken:  0.16409611701965332\n",
      "solving Ax = b time taken:  0.00017213821411132812\n",
      "epoch:  13\tnum batches argmax:  1\n",
      "num batches argmax:  1\n",
      "total size: 13 125000 = 1625000\n",
      "num batches:  1\n",
      "assembling the mass matrix time taken:  0.11429715156555176\n",
      "assembling the matrix time taken:  0.17368793487548828\n",
      "solving Ax = b time taken:  0.00018596649169921875\n",
      "epoch:  14\tnum batches argmax:  1\n",
      "num batches argmax:  1\n",
      "total size: 14 125000 = 1750000\n",
      "num batches:  1\n",
      "assembling the mass matrix time taken:  0.12236380577087402\n",
      "assembling the matrix time taken:  0.18410587310791016\n",
      "solving Ax = b time taken:  0.00018095970153808594\n",
      "epoch:  15\tnum batches argmax:  1\n",
      "num batches argmax:  1\n",
      "total size: 15 125000 = 1875000\n",
      "num batches:  1\n",
      "assembling the mass matrix time taken:  0.11994504928588867\n",
      "assembling the matrix time taken:  0.21371793746948242\n",
      "solving Ax = b time taken:  0.0004360675811767578\n",
      "epoch:  16\tnum batches argmax:  1\n",
      "num batches argmax:  1\n",
      "total size: 16 125000 = 2000000\n",
      "num batches:  1\n",
      "assembling the mass matrix time taken:  0.11843132972717285\n",
      "assembling the matrix time taken:  0.18508315086364746\n",
      "solving Ax = b time taken:  0.00017595291137695312\n",
      "epoch:  17\tnum batches argmax:  1\n",
      "num batches argmax:  1\n",
      "total size: 17 125000 = 2125000\n",
      "num batches:  1\n",
      "assembling the mass matrix time taken:  0.12247180938720703\n",
      "assembling the matrix time taken:  0.20014595985412598\n",
      "solving Ax = b time taken:  0.00020503997802734375\n",
      "epoch:  18\tnum batches argmax:  1\n",
      "num batches argmax:  1\n",
      "total size: 18 125000 = 2250000\n",
      "num batches:  1\n",
      "assembling the mass matrix time taken:  0.12544989585876465\n",
      "assembling the matrix time taken:  0.2256019115447998\n",
      "solving Ax = b time taken:  0.0005300045013427734\n",
      "epoch:  19\tnum batches argmax:  1\n",
      "num batches argmax:  1\n",
      "total size: 19 125000 = 2375000\n",
      "num batches:  1\n",
      "assembling the mass matrix time taken:  0.12511801719665527\n",
      "assembling the matrix time taken:  0.23032212257385254\n",
      "solving Ax = b time taken:  0.00045418739318847656\n",
      "epoch:  20\tnum batches argmax:  1\n",
      "num batches argmax:  1\n",
      "total size: 20 125000 = 2500000\n",
      "num batches:  1\n",
      "assembling the mass matrix time taken:  0.1226201057434082\n",
      "assembling the matrix time taken:  0.21151399612426758\n",
      "solving Ax = b time taken:  0.0003991127014160156\n",
      "epoch:  21\tnum batches argmax:  1\n",
      "num batches argmax:  1\n",
      "total size: 21 125000 = 2625000\n",
      "num batches:  1\n",
      "assembling the mass matrix time taken:  0.12293815612792969\n",
      "assembling the matrix time taken:  0.21424102783203125\n",
      "solving Ax = b time taken:  0.0004067420959472656\n",
      "epoch:  22\tnum batches argmax:  1\n",
      "num batches argmax:  1\n",
      "total size: 22 125000 = 2750000\n",
      "num batches:  1\n",
      "assembling the mass matrix time taken:  0.12500715255737305\n",
      "assembling the matrix time taken:  0.21973204612731934\n",
      "solving Ax = b time taken:  0.0004839897155761719\n",
      "epoch:  23\tnum batches argmax:  1\n",
      "num batches argmax:  1\n",
      "total size: 23 125000 = 2875000\n",
      "num batches:  1\n",
      "assembling the mass matrix time taken:  0.1371290683746338\n",
      "assembling the matrix time taken:  0.23786115646362305\n",
      "solving Ax = b time taken:  0.00047206878662109375\n",
      "epoch:  24\tnum batches argmax:  1\n",
      "num batches argmax:  1\n",
      "total size: 24 125000 = 3000000\n",
      "num batches:  1\n",
      "assembling the mass matrix time taken:  0.12708091735839844\n",
      "assembling the matrix time taken:  0.2389681339263916\n",
      "solving Ax = b time taken:  0.00047707557678222656\n",
      "epoch:  25\tnum batches argmax:  1\n",
      "num batches argmax:  1\n",
      "total size: 25 125000 = 3125000\n",
      "num batches:  1\n",
      "assembling the mass matrix time taken:  0.12883806228637695\n",
      "assembling the matrix time taken:  0.23513293266296387\n",
      "solving Ax = b time taken:  0.0004298686981201172\n",
      "epoch:  26\tnum batches argmax:  1\n",
      "num batches argmax:  1\n",
      "total size: 26 125000 = 3250000\n",
      "num batches:  1\n",
      "assembling the mass matrix time taken:  0.13307905197143555\n",
      "assembling the matrix time taken:  0.2536489963531494\n",
      "solving Ax = b time taken:  0.0004367828369140625\n",
      "epoch:  27\tnum batches argmax:  1\n",
      "num batches argmax:  1\n",
      "total size: 27 125000 = 3375000\n",
      "num batches:  1\n",
      "assembling the mass matrix time taken:  0.13229584693908691\n",
      "assembling the matrix time taken:  0.26897382736206055\n",
      "solving Ax = b time taken:  0.0005209445953369141\n",
      "epoch:  28\tnum batches argmax:  1\n",
      "num batches argmax:  1\n",
      "total size: 28 125000 = 3500000\n",
      "num batches:  1\n",
      "assembling the mass matrix time taken:  0.13458490371704102\n",
      "assembling the matrix time taken:  0.26100993156433105\n",
      "solving Ax = b time taken:  0.00044417381286621094\n",
      "epoch:  29\tnum batches argmax:  1\n",
      "num batches argmax:  1\n",
      "total size: 29 125000 = 3625000\n",
      "num batches:  1\n",
      "assembling the mass matrix time taken:  0.13184022903442383\n",
      "assembling the matrix time taken:  0.257216215133667\n",
      "solving Ax = b time taken:  0.0004489421844482422\n",
      "epoch:  30\tnum batches argmax:  1\n",
      "num batches argmax:  1\n",
      "total size: 30 125000 = 3750000\n",
      "num batches:  1\n",
      "assembling the mass matrix time taken:  0.13340282440185547\n",
      "assembling the matrix time taken:  0.26430583000183105\n",
      "solving Ax = b time taken:  0.0004961490631103516\n",
      "epoch:  31\tnum batches argmax:  1\n",
      "num batches argmax:  1\n",
      "total size: 31 125000 = 3875000\n",
      "num batches:  1\n",
      "assembling the mass matrix time taken:  0.13629794120788574\n",
      "assembling the matrix time taken:  0.2706010341644287\n",
      "solving Ax = b time taken:  0.0004527568817138672\n",
      "epoch:  32\tnum batches argmax:  1\n",
      "num batches argmax:  1\n",
      "total size: 32 125000 = 4000000\n",
      "num batches:  1\n",
      "assembling the mass matrix time taken:  0.11147904396057129\n",
      "assembling the matrix time taken:  0.2456650733947754\n",
      "solving Ax = b time taken:  0.0004138946533203125\n",
      "time taken:  22.215499877929688\n",
      "neuron num \t\t error \t\t order\n",
      "4 \t\t 0.166010 \t\t * \t\t 12.015091 \t\t * \n",
      "\n",
      "8 \t\t 0.216803 \t\t -0.385115 \t\t 11.933471 \t\t 0.009834 \n",
      "\n",
      "16 \t\t 0.832829 \t\t -1.941634 \t\t 11.474312 \t\t 0.056606 \n",
      "\n",
      "32 \t\t 0.667724 \t\t 0.318768 \t\t 10.454619 \t\t 0.134267 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "function_name = \"biharmonic\" \n",
    "filename_write = \"3DOGA-{}-order.txt\".format(function_name)\n",
    "f_write = open(filename_write, \"a\")\n",
    "f_write.write(\"\\n\")\n",
    "f_write.close() \n",
    "save = True  \n",
    "relu_k= 3 \n",
    "memory = 2**28\n",
    "trial_num = 5\n",
    "for trial in range(trial_num): \n",
    "    for N_list in [[2**3,2**3,2**4]]: #[2**6,2**6],[2**7,2**7] \n",
    "        # save = True \n",
    "        f_write = open(filename_write, \"a\")\n",
    "        my_model = None \n",
    "        Nx = 50\n",
    "        order = 2   \n",
    "        exponent = 10\n",
    "        num_epochs = 2**exponent  \n",
    "        N = np.prod(N_list)\n",
    "        errl2,errh2, my_model = OGABiharmonicReLU3D(my_model,rhs_3d, N_list,num_epochs, Nx, order, k = relu_k, rand_deter= 'rand', linear_solver = \"direct\",memory = memory)\n",
    "        if save: \n",
    "            folder = 'data-biharmonic/'\n",
    "            filename = folder + 'errl2_OGA_3D_{}_neuron_{}_N_{}_rand_trial_{}.pt'.format(function_name,num_epochs,N,trial)\n",
    "            torch.save(errl2,filename)\n",
    "            filename = folder + 'errh2_OGA_3D_{}_neuron_{}_N_{}_rand_trial_{}.pt'.format(function_name,num_epochs,N,trial)\n",
    "            torch.save(errh2,filename)\n",
    "            filename = folder + 'model_OGA_3D_{}_neuron_{}_N_{}_rand_trial_{}.pt'.format(function_name,num_epochs,N,trial)\n",
    "            torch.save(my_model.state_dict(),filename)\n",
    "\n",
    "    show_convergence_order(errl2,errh2,exponent,N, filename_write,write2file = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
