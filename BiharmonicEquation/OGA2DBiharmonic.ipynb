{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import sys\n",
    "import sympy as sp \n",
    "from scipy.sparse import linalg\n",
    "from pathlib import Path\n",
    "import os \n",
    "if torch.cuda.is_available():  \n",
    "    device = \"cuda\" \n",
    "else:  \n",
    "    device = \"cpu\" \n",
    "\n",
    "pi = torch.tensor(np.pi,dtype=torch.float64)\n",
    "torch.set_default_tensor_type(torch.DoubleTensor)\n",
    "ZERO = torch.tensor([0.]).to(device)\n",
    "\n",
    "class model(nn.Module):\n",
    "    \"\"\" ReLU k shallow neural network\n",
    "    Parameters: \n",
    "    input size: input dimension\n",
    "    hidden_size1 : number of hidden layers \n",
    "    num_classes: output classes \n",
    "    k: degree of relu functions\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, hidden_size1, num_classes,k = 1):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size1)\n",
    "        self.fc2 = nn.Linear(hidden_size1, num_classes,bias = False)\n",
    "        self.k = k \n",
    "    def forward(self, x):\n",
    "        u1 = self.fc2(F.relu(self.fc1(x))**self.k)\n",
    "        return u1\n",
    "    def evaluate_derivative(self, x, i):\n",
    "        if self.k == 1:\n",
    "            u1 = self.fc2(torch.heaviside(self.fc1(x),ZERO) * self.fc1.weight.t()[i-1:i,:] )\n",
    "        else:\n",
    "            u1 = self.fc2(self.k*F.relu(self.fc1(x))**(self.k-1) *self.fc1.weight.t()[i-1:i,:] )  \n",
    "        return u1\n",
    "    def evaluate_2ndderivative(self,x,i,j): \n",
    "        if self.k == 2:\n",
    "            u1 = self.fc2( 2 * torch.heaviside(self.fc1(x),ZERO) * (self.fc1.weight.t()[i-1:i,:])*self.fc1.weight.t()[j-1:j,:]) \n",
    "        else:\n",
    "            u1 = self.fc2( self.k*(self.k-1)*F.relu(self.fc1(x))**(self.k-2) * (self.fc1.weight.t()[i-1:i,:])* (self.fc1.weight.t()[j-1:j,:]))  \n",
    "        return u1\n",
    "\n",
    "def plot_2D(f): \n",
    "    \n",
    "    Nx = 400\n",
    "    Ny = 400 \n",
    "    xs = np.linspace(0, 1, Nx)\n",
    "    ys = np.linspace(0, 1, Ny)\n",
    "    x, y = np.meshgrid(xs, ys, indexing='xy')\n",
    "    xy_comb = np.stack((x.flatten(),y.flatten())).T\n",
    "    xy_comb = torch.tensor(xy_comb)\n",
    "    z = f(xy_comb).reshape(Nx,Ny)\n",
    "    z = z.detach().numpy()\n",
    "    plt.figure(dpi=200)\n",
    "    ax = plt.axes(projection='3d')\n",
    "    ax.plot_surface(x , y , z )\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def plot_subdomains(my_model):\n",
    "    x_coord =torch.linspace(0,1,200)\n",
    "    wi = my_model.fc1.weight.data\n",
    "    bi = my_model.fc1.bias.data \n",
    "    for i, bias in enumerate(bi):  \n",
    "        if wi[i,1] !=0: \n",
    "            plt.plot(x_coord, - wi[i,0]/wi[i,1]*x_coord - bias/wi[i,1])\n",
    "        else: \n",
    "            plt.plot(x_coord,  - bias/wi[i,0]*torch.ones(x_coord.size()))\n",
    "\n",
    "    plt.xlim([-1,1])\n",
    "    plt.ylim([-1,1])\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    return 0   \n",
    "\n",
    "## Initialization\n",
    "def adjust_neuron_position(my_model,target=None):\n",
    "    my_model = my_model.cpu()\n",
    "    counter = 0 \n",
    "#     positions = torch.tensor([[-1.,-1.],[-1.,1.],[1.,1.],[1.,-1.]])\n",
    "    positions = torch.tensor([[0.,0.],[0.,1.],[1.,1.],[1.,0.]])\n",
    "    neuron_num = my_model.fc1.bias.size(0)\n",
    "    for i in range(neuron_num): \n",
    "        w = my_model.fc1.weight.data[i:i+1,:]\n",
    "        b = my_model.fc1.bias.data[i]\n",
    "        values = torch.matmul(positions,w.T) # + b\n",
    "        left_end = - torch.max(values)\n",
    "        right_end = - torch.min(values) \n",
    "        off_set = (right_end - left_end)/1000 \n",
    "        if b <= left_end + off_set: # nearly vanishing\n",
    "            b = torch.rand(1)*(right_end - left_end - off_set*2) + left_end + off_set \n",
    "            my_model.fc1.bias.data[i] = b \n",
    "        if b >= right_end - off_set: # nearly nonvanishing everywhere\n",
    "            if counter < 3:\n",
    "                counter += 1\n",
    "            else: # 3 or more \n",
    "                b = torch.rand(1)*(right_end - left_end - off_set*2) + left_end + off_set\n",
    "                my_model.fc1.bias.data[i] = b \n",
    "    return my_model\n",
    "\n",
    "def PiecewiseGQ2D_weights_points(Nx, order): \n",
    "    \"\"\" A slight modification of PiecewiseGQ2D function that only needs the weights and integration points.\n",
    "    Parameters\n",
    "    ----------\n",
    "\n",
    "    Nx: int \n",
    "        number of intervals along the dimension. No Ny, assume Nx = Ny\n",
    "    order: int \n",
    "        order of the Gauss Quadrature\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    long_weights: torch.tensor\n",
    "    integration_points: torch.tensor\n",
    "    \"\"\"\n",
    "\n",
    "#     print(\"order: \",order )\n",
    "    x, w = np.polynomial.legendre.leggauss(order)\n",
    "    gauss_pts = np.array(np.meshgrid(x,x,indexing='ij')).reshape(2,-1).T\n",
    "    weights =  (w*w[:,None]).ravel()\n",
    "\n",
    "    gauss_pts =torch.tensor(gauss_pts)\n",
    "    weights = torch.tensor(weights)\n",
    "\n",
    "    h = 1/Nx # 100 intervals \n",
    "    long_weights =  torch.tile(weights,(Nx**2,1))\n",
    "    long_weights = long_weights.reshape(-1,1)\n",
    "    long_weights = long_weights * h**2 /4 \n",
    "\n",
    "    integration_points = torch.tile(gauss_pts,(Nx**2,1))\n",
    "    scale_factor = h/2 \n",
    "    integration_points = scale_factor * integration_points\n",
    "\n",
    "    index = np.arange(1,Nx+1)-0.5\n",
    "    ordered_pairs = np.array(np.meshgrid(index,index,indexing='ij'))\n",
    "    ordered_pairs = ordered_pairs.reshape(2,-1).T\n",
    "\n",
    "    # print(ordered_pairs)\n",
    "    # print()\n",
    "    ordered_pairs = torch.tensor(ordered_pairs)\n",
    "    # print(ordered_pairs.size())\n",
    "    ordered_pairs = torch.tile(ordered_pairs, (1,order**2)) # number of GQ points\n",
    "    # print(ordered_pairs)\n",
    "\n",
    "    ordered_pairs =  ordered_pairs.reshape(-1,2)\n",
    "    # print(ordered_pairs)\n",
    "    translation = ordered_pairs*h \n",
    "    # print(translation)\n",
    "\n",
    "    integration_points = integration_points + translation \n",
    "#     print(integration_points.size())\n",
    "    # func_values = integrand2_torch(integration_points)\n",
    "    return long_weights.to(device), integration_points.to(device)\n",
    "\n",
    "\n",
    "def minimize_linear_layer_H2_explicit_assemble_efficient(model,target,weights, integration_points,activation = 'relu',solver=\"direct\" ):\n",
    "    \"\"\"Biharmonic equation solver\n",
    "    \\Delta^2 u + u = f, in \\Omega\n",
    "    \\Delta u = 0, \\partial_n (\\Delta u) = 0  on \\partial \\Omega\n",
    "    \"\"\"\n",
    "    # weights, integration_points = PiecewiseGQ2D_weights_points(Nx, order) \n",
    "    # integration_points.requires_grad_(True) \n",
    "    start_time = time.time() \n",
    "    w = model.fc1.weight.data \n",
    "    b = model.fc1.bias.data \n",
    "    neuron_num = b.size(0) \n",
    "\n",
    "    if activation == 'relu':\n",
    "        assert model.k != 1, \"k must not be 1\"  \n",
    "        basis_value_col = F.relu(integration_points @ w.t()+ b)**(model.k) \n",
    "        if model.k == 2:  \n",
    "            dxx_basis_value_col = 2 * torch.heaviside(integration_points @ w.t()+ b, ZERO) * (w.t()[0:1,:])**2 \n",
    "            dxy_basis_value_col = 2 * torch.heaviside(integration_points @ w.t()+ b, ZERO) * (w.t()[0:1,:])* (w.t()[1:2,:]) \n",
    "            dyy_basis_value_col = 2 * torch.heaviside(integration_points @ w.t()+ b,ZERO) * (w.t()[1:2,:])**2 \n",
    "        else: \n",
    "            dxx_basis_value_col = model.k * (model.k -1) * F.relu(integration_points @ w.t()+ b)**(model.k-2) * (w.t()[0:1,:])**2 \n",
    "            dxy_basis_value_col = model.k * (model.k -1) * F.relu(integration_points @ w.t()+ b)**(model.k-2) * (w.t()[0:1,:])* (w.t()[1:2,:]) \n",
    "            dyy_basis_value_col = model.k * (model.k -1)* F.relu(integration_points @ w.t()+ b)**(model.k-2) * (w.t()[1:2,:])**2  \n",
    "    # elif activation == 'tanh': \n",
    "    #     basis_value_col = torch.tanh(integration_points @ w.t()+ b) \n",
    "    #     basis_value_dx_col = tanh_activation_dx(integration_points @ w.t()+ b) * w.t()[0:1,:]\n",
    "    #     basis_value_dy_col = tanh_activation_dx(integration_points @ w.t()+ b) * w.t()[1:2,:]\n",
    "    # elif activation == 'gaussian':\n",
    "    #     basis_value_col = Gaussian_activation(integration_points @ w.t()+ b)\n",
    "    #     basis_value_dx_col = Gaussian_activation_dx(integration_points @ w.t()+ b) * w.t()[0:1,:]\n",
    "    #     basis_value_dy_col = Gaussian_activation_dx(integration_points @ w.t()+ b) * w.t()[1:2,:]\n",
    "    # elif activation == 'cosine':\n",
    "    #     basis_value_col = cosine_activation(integration_points @ w.t()+ b) \n",
    "    #     basis_value_dx_col = cosine_activation_dx(integration_points @ w.t()+ b) * w.t()[0:1,:]\n",
    "    #     basis_value_dy_col = cosine_activation_dx(integration_points @ w.t()+ b) * w.t()[1:2,:] \n",
    "\n",
    "    weighted_basis_value_col = basis_value_col * weights \n",
    "    jac = weighted_basis_value_col.t() @ basis_value_col  # mass matrix \n",
    "    rhs = weighted_basis_value_col.t() @ (target(integration_points)) \n",
    "    print(\"assembling the mass matrix time taken: \", time.time()-start_time) \n",
    "\n",
    "    start_time = time.time() \n",
    "    weighted_dxx_basis_value_col = dxx_basis_value_col * weights\n",
    "    jac += weighted_dxx_basis_value_col.t() @ dxx_basis_value_col \n",
    "    jac += 2 * ( (dxy_basis_value_col * weights).t() @  dxy_basis_value_col )\n",
    "    jac += (dyy_basis_value_col * weights).t() @  dyy_basis_value_col\n",
    "    print(\"assembling the stiffness matrix time taken: \", time.time()-start_time)   \n",
    "#     jac = jac1 + jac2    \n",
    "    \n",
    "    start_time = time.time()    \n",
    "    if solver == \"cg\": \n",
    "        sol, exit_code = linalg.cg(np.array(jac.detach().cpu()),np.array(rhs.detach().cpu()),tol=1e-12)\n",
    "        sol = torch.tensor(sol).view(1,-1)\n",
    "    elif solver == \"direct\": \n",
    "#         sol = np.linalg.inv( np.array(jac.detach().cpu()) )@np.array(rhs.detach().cpu())\n",
    "        sol = (torch.linalg.solve( jac.detach(), rhs.detach())).view(1,-1)\n",
    "    elif solver == \"ls\":\n",
    "        sol = (torch.linalg.lstsq(jac.detach().cpu(),rhs.detach().cpu(),driver='gelsd').solution).view(1,-1)\n",
    "        # sol = (torch.linalg.lstsq(jac.detach(),rhs.detach()).solution).view(1,-1) # gpu/cpu, driver = 'gels', cannot solve singular\n",
    "    print(\"solving Ax = b time taken: \", time.time()-start_time)\n",
    "    return sol \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## \n",
    "import sympy as sp\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Define symbolic variables and the function u_exact\n",
    "x_sym, y_sym = sp.symbols('x y')\n",
    "u_expr = (4 * x_sym * (x_sym - 1))**4 * (4 * y_sym * (y_sym - 1))**4\n",
    "\n",
    "# Compute symbolic derivatives (e.g., first derivative with respect to x)\n",
    "u_x_expr = sp.diff(u_expr, x_sym)\n",
    "u_y_expr = sp.diff(u_expr, y_sym)\n",
    "# Compute higher-order derivatives if needed:\n",
    "u_xx_expr = sp.diff(u_expr, x_sym, 2)\n",
    "u_yy_expr = sp.diff(u_expr, y_sym, 2)\n",
    "u_xy_expr = sp.diff(u_expr, x_sym, y_sym)\n",
    "u_xxxx_expr = sp.diff(u_expr, x_sym, 4)\n",
    "u_yyyy_expr = sp.diff(u_expr, y_sym, 4)\n",
    "u_xxyy_expr = sp.diff(u_expr, x_sym, 2,y_sym,2)\n",
    "\n",
    "\n",
    "\n",
    "# Convert the symbolic expressions to functions using lambdify (returns NumPy arrays)\n",
    "u_exact_sym_func = sp.lambdify((x_sym, y_sym), u_expr, modules='numpy')\n",
    "u_x_sym_func     = sp.lambdify((x_sym, y_sym), u_x_expr, modules='numpy')\n",
    "u_y_sym_func     = sp.lambdify((x_sym, y_sym), u_y_expr, modules='numpy')\n",
    "u_xx_sym_func    = sp.lambdify((x_sym, y_sym), u_xx_expr, modules='numpy')\n",
    "u_yy_sym_func    = sp.lambdify((x_sym, y_sym), u_yy_expr, modules='numpy')\n",
    "u_xy_sym_func    = sp.lambdify((x_sym, y_sym), u_xy_expr, modules='numpy')\n",
    "u_xxxx_sym_func    = sp.lambdify((x_sym, y_sym), u_xxxx_expr, modules='numpy')\n",
    "u_yyyy_sym_func    = sp.lambdify((x_sym, y_sym), u_yyyy_expr, modules='numpy')\n",
    "u_xxyy_sym_func    = sp.lambdify((x_sym, y_sym), u_xxyy_expr, modules='numpy')\n",
    "\n",
    "\n",
    "# Define wrapper functions that accept PyTorch tensors as input and return torch tensors.\n",
    "def u_exact(x_tensor):\n",
    "    # Assume x_tensor is a tensor of shape (N, 2) where each row is (x, y)\n",
    "    x_np = x_tensor.detach().cpu().numpy()\n",
    "    # Evaluate the symbolic function using the first and second columns\n",
    "    result_np = u_exact_sym_func(x_np[:, 0], x_np[:, 1]).reshape(-1,1)\n",
    "    # Convert result to a torch tensor, preserving the device and dtype of the input\n",
    "    return torch.from_numpy(np.array(result_np)).to(x_tensor.device).type(x_tensor.dtype)\n",
    "\n",
    "def u_x(x_tensor):\n",
    "    x_np = x_tensor.detach().cpu().numpy()\n",
    "    result_np = u_x_sym_func(x_np[:, 0], x_np[:, 1]).reshape(-1,1)\n",
    "    return torch.from_numpy(np.array(result_np)).to(x_tensor.device).type(x_tensor.dtype)\n",
    "\n",
    "def u_y(x_tensor):\n",
    "    x_np = x_tensor.detach().cpu().numpy()\n",
    "    result_np = u_y_sym_func(x_np[:, 0], x_np[:, 1]).reshape(-1,1)\n",
    "    return torch.from_numpy(np.array(result_np)).to(x_tensor.device).type(x_tensor.dtype)\n",
    "\n",
    "def u_xx(x_tensor):\n",
    "    x_np = x_tensor.detach().cpu().numpy()\n",
    "    result_np = u_xx_sym_func(x_np[:, 0], x_np[:, 1]).reshape(-1,1)\n",
    "    return torch.from_numpy(np.array(result_np)).to(x_tensor.device).type(x_tensor.dtype)\n",
    "\n",
    "def u_yy(x_tensor):\n",
    "    x_np = x_tensor.detach().cpu().numpy()\n",
    "    result_np = u_yy_sym_func(x_np[:, 0], x_np[:, 1]).reshape(-1,1)\n",
    "    return torch.from_numpy(np.array(result_np)).to(x_tensor.device).type(x_tensor.dtype)\n",
    "\n",
    "def u_xy(x_tensor):\n",
    "    x_np = x_tensor.detach().cpu().numpy()\n",
    "    result_np = u_xy_sym_func(x_np[:, 0], x_np[:, 1]).reshape(-1,1)\n",
    "    return torch.from_numpy(np.array(result_np)).to(x_tensor.device).type(x_tensor.dtype)\n",
    "\n",
    "def u_xxxx(x_tensor):\n",
    "    x_np = x_tensor.detach().cpu().numpy()\n",
    "    result_np = u_xxxx_sym_func(x_np[:, 0], x_np[:, 1]).reshape(-1,1)\n",
    "    return torch.from_numpy(np.array(result_np)).to(x_tensor.device).type(x_tensor.dtype)\n",
    "\n",
    "def u_yyyy(x_tensor):\n",
    "    x_np = x_tensor.detach().cpu().numpy()\n",
    "    result_np = u_yyyy_sym_func(x_np[:, 0], x_np[:, 1]).reshape(-1,1)\n",
    "    return torch.from_numpy(np.array(result_np)).to(x_tensor.device).type(x_tensor.dtype)\n",
    "\n",
    "def u_xxyy(x_tensor):\n",
    "    x_np = x_tensor.detach().cpu().numpy()\n",
    "    result_np = u_xxyy_sym_func(x_np[:, 0], x_np[:, 1]).reshape(-1,1)\n",
    "    return torch.from_numpy(np.array(result_np)).to(x_tensor.device).type(x_tensor.dtype)\n",
    "\n",
    "\n",
    "def rhs(x_tensor):\n",
    "    return u_xxxx(x_tensor) + u_yyyy(x_tensor) + 2 * u_xxyy(x_tensor) + u_exact(x_tensor) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "derivatives = {\n",
    "    \"u_x\": u_x,\n",
    "    \"u_y\": u_y,\n",
    "    \"u_xx\": u_xx,\n",
    "    \"u_yy\": u_yy,\n",
    "    \"u_xy\": u_xy,\n",
    "}\n",
    "\n",
    "def OGABiharmonicReLU2D(my_model,target,u_exact,derivatives, N_list,num_epochs,plot_freq, Nx, order, k =1, rand_deter = 'deter', linear_solver = \"direct\"): \n",
    "    \"\"\" Orthogonal greedy algorithm using 1D ReLU dictionary over [-pi,pi]\n",
    "    Parameters\n",
    "    ----------\n",
    "    my_model: \n",
    "        nn model \n",
    "    target: \n",
    "        target function\n",
    "    num_epochs: int \n",
    "        number of training epochs \n",
    "    integration_intervals: int \n",
    "        number of subintervals for piecewise numerical quadrature \n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    err: tensor \n",
    "        rank 1 torch tensor to record the L2 error history  \n",
    "    model: \n",
    "        trained nn model \n",
    "    \"\"\"\n",
    "    #Todo Done\n",
    "    gw_expand, integration_points = PiecewiseGQ2D_weights_points(Nx, order)\n",
    "    gw_expand = gw_expand.to(device)\n",
    "    integration_points = integration_points.to(device)\n",
    "\n",
    "    err = torch.zeros(num_epochs+1)\n",
    "    errh1 = torch.zeros(num_epochs+1)\n",
    "    errh2 = torch.zeros(num_epochs+1)\n",
    "    if my_model == None: \n",
    "        func_values = target(integration_points)\n",
    "        num_neuron = 0\n",
    "\n",
    "        list_b = []\n",
    "        list_w = []\n",
    "    else: \n",
    "        func_values = target(integration_points) - my_model(integration_points).detach()\n",
    "        bias = my_model.fc1.bias.detach().data\n",
    "        weights = my_model.fc1.weight.detach().data\n",
    "        num_neuron = int(bias.size(0))\n",
    "\n",
    "        list_b = list(bias)\n",
    "        list_w = list(weights)\n",
    "    \n",
    "    # initial error Todo Done\n",
    "    func_values_sqrd = func_values*func_values\n",
    "    # print(func_values_sqrd.size())\n",
    "    # print(gw_expand.size()) \n",
    "    err[0]= torch.sum(func_values_sqrd*gw_expand)**0.5\n",
    "    errh1[0] = (gw_expand.t()@ ( derivatives['u_x'](integration_points)**2 + derivatives['u_y'](integration_points)**2 ))**0.5\n",
    "    errh2[0] = (gw_expand.t()@ ( derivatives['u_xx'](integration_points)**2 \\\n",
    "                                + derivatives['u_yy'](integration_points)**2 \\\n",
    "                                + 2 * derivatives['u_xy'](integration_points)**2 ))**0.5\n",
    "    start_time = time.time()\n",
    "    solver = linear_solver\n",
    "\n",
    "    N0 = np.prod(N_list)\n",
    "    if rand_deter == 'deter':\n",
    "        relu_dict_parameters = generate_relu_dict2D(N_list).to(device)\n",
    "    print(\"using linear solver: \",solver)\n",
    "    for i in range(num_epochs): \n",
    "        print(\"epoch: \",i+1, end = '\\t')\n",
    "        if rand_deter == 'rand':\n",
    "            relu_dict_parameters = generate_relu_dict4plusD_sphere(2,1,N0).to(device)  \n",
    "        if num_neuron == 0: \n",
    "            func_values = - target(integration_points)\n",
    "        else: \n",
    "            func_values = - target(integration_points) + my_model(integration_points).detach()\n",
    "\n",
    "        weight_func_values = func_values*gw_expand  \n",
    "        basis_values = (F.relu( torch.matmul(integration_points,relu_dict_parameters[:,0:2].T ) - relu_dict_parameters[:,2])**k).T # uses broadcasting\n",
    "        \n",
    "        assert k != 1, \"k must not be 1\"  \n",
    "        if k == 2:  \n",
    "            dxx_basis_values = 2 * torch.heaviside(integration_points @ (relu_dict_parameters[:,0:2].T) - relu_dict_parameters[:,2], ZERO) * (relu_dict_parameters.t()[0:1,:])**2  \n",
    "            dyy_basis_values = 2 * torch.heaviside(integration_points @ (relu_dict_parameters[:,0:2].T) - relu_dict_parameters[:,2], ZERO) * (relu_dict_parameters.t()[1:2,:])**2 \n",
    "            dxy_basis_values = 2 * torch.heaviside(integration_points @ (relu_dict_parameters[:,0:2].T) - relu_dict_parameters[:,2], ZERO) * (relu_dict_parameters.t()[0:1,:]) * (relu_dict_parameters.t()[1:2,:]) \n",
    "            if my_model!= None:\n",
    "                dxx_my_model = my_model.evaluate_2ndderivative(integration_points,1,1).detach()\n",
    "                dyy_my_model = my_model.evaluate_2ndderivative(integration_points,2,2).detach() \n",
    "                dxy_my_model = my_model.evaluate_2ndderivative(integration_points,1,2).detach() \n",
    "        else:  \n",
    "            dxx_basis_values = k *(k-1) * F.relu(integration_points @ (relu_dict_parameters[:,0:2].T) - relu_dict_parameters[:,2])**(k-2) * (relu_dict_parameters.t()[0:1,:])**2 \n",
    "            dyy_basis_values = k *(k-1) * F.relu(integration_points @ (relu_dict_parameters[:,0:2].T) - relu_dict_parameters[:,2])**(k-2) * (relu_dict_parameters.t()[1:2,:])**2 \n",
    "            dxy_basis_values = k *(k-1) * F.relu(integration_points @ (relu_dict_parameters[:,0:2].T) - relu_dict_parameters[:,2])**(k-2) * (relu_dict_parameters.t()[0:1,:]) * (relu_dict_parameters.t()[1:2,:])\n",
    "            if my_model!= None:\n",
    "                dxx_my_model = my_model.evaluate_2ndderivative(integration_points,1,1).detach()\n",
    "                dyy_my_model = my_model.evaluate_2ndderivative(integration_points,2,2).detach() \n",
    "                dxy_my_model = my_model.evaluate_2ndderivative(integration_points,1,2).detach() \n",
    "\n",
    "\n",
    "        output1 = torch.matmul(basis_values,weight_func_values) #\n",
    "        if my_model!= None:\n",
    "            output2 = dxx_basis_values.t() @(dxx_my_model*gw_expand) \n",
    "            output2 += 2 * dxy_basis_values.t() @ (dxy_my_model*gw_expand)\n",
    "            output2 += dyy_basis_values .t() @(dyy_my_model*gw_expand)\n",
    "            output = torch.abs(output1 + output2) \n",
    "        else: \n",
    "            output = torch.abs(output1) \n",
    "        # output = torch.abs(torch.matmul(basis_values,weight_func_values)) # \n",
    "        neuron_index = torch.argmax(output.flatten())\n",
    "        \n",
    "        # print(neuron_index)\n",
    "        list_w.append(relu_dict_parameters[neuron_index,0:2]) # \n",
    "        list_b.append(-relu_dict_parameters[neuron_index,2])\n",
    "        num_neuron += 1\n",
    "        my_model = model(2,num_neuron,1,k).to(device)\n",
    "        w_tensor = torch.stack(list_w, 0 ) \n",
    "        b_tensor = torch.tensor(list_b)\n",
    "        my_model.fc1.weight.data[:,:] = w_tensor[:,:]\n",
    "        my_model.fc1.bias.data[:] = b_tensor[:]\n",
    "\n",
    "        #Todo Done \n",
    "        sol = minimize_linear_layer_H2_explicit_assemble_efficient(my_model,target,gw_expand, integration_points,activation = 'relu',solver = solver)\n",
    "\n",
    "        my_model.fc2.weight.data[0,:] = sol[:]\n",
    "        # if (i+1)%plot_freq == 0: \n",
    "        #     plot_2D(my_model.cpu())\n",
    "        #     my_model = my_model.to(device)\n",
    "\n",
    "        model_values = my_model(integration_points).detach()\n",
    "        # func_values = target(integration_points) - model_values\n",
    "        # func_values_sqrd = func_values*func_values\n",
    "\n",
    "        # L2 error ||u - u_n||\n",
    "        diff_values_sqrd = (u_exact(integration_points) - model_values)**2 \n",
    "        err[i+1]= torch.sum(diff_values_sqrd*gw_expand)**0.5\n",
    "        errh1[i+1] += (gw_expand.t() @ (derivatives['u_x'](integration_points) - my_model.evaluate_derivative(integration_points,1).detach())**2).item()\n",
    "        errh1[i+1] += (gw_expand.t() @ (derivatives['u_y'](integration_points) - my_model.evaluate_derivative(integration_points,2).detach())**2).item()\n",
    "        errh1[i+1] = errh1[i+1]**0.5 \n",
    "        errh2[i+1] += (gw_expand.t() @ (derivatives['u_xx'](integration_points) - my_model.evaluate_2ndderivative(integration_points,1,1).detach() )**2).item()\n",
    "        errh2[i+1] += (2 * gw_expand.t() @ (derivatives['u_xy'](integration_points) - my_model.evaluate_2ndderivative(integration_points,1,2).detach() )**2).item()\n",
    "        errh2[i+1] += (gw_expand.t() @ (derivatives['u_yy'](integration_points) - my_model.evaluate_2ndderivative(integration_points,2,2).detach() )**2).item()\n",
    "        errh2[i+1] = errh2[i+1]**0.5\n",
    "    print(\"time taken: \",time.time() - start_time)\n",
    "    return err, errh1,errh2, my_model\n",
    "\n",
    "def generate_relu_dict2D(N_list):\n",
    "    N1 = N_list[0] \n",
    "    N2 = N_list[1]\n",
    "    \n",
    "    theta = np.linspace(0, 2*pi, N1, endpoint= False).reshape(N1,1)\n",
    "    W1 = np.cos(theta)\n",
    "    W2 = np.sin(theta)\n",
    "    W = np.concatenate((W1,W2),1) # N1 x 2\n",
    "    b = np.linspace(-1.42, 1.42, N2,endpoint=False).reshape(N2,1)\n",
    "    \n",
    "    index1 = np.arange(N1)\n",
    "    index2 = np.arange(N2)\n",
    "    ordered_pairs = np.array(np.meshgrid(index1,index2,indexing='ij'))\n",
    "\n",
    "    ordered_pairs = ordered_pairs.reshape(2,-1).T\n",
    "    W = W[ordered_pairs[:,0],:]\n",
    "    b = b[ordered_pairs[:,1],:]\n",
    "    Wb = np.concatenate((W,b),1) # N1 x 3 \n",
    "    Wb_tensor = torch.from_numpy(Wb) \n",
    "    return Wb_tensor\n",
    "\n",
    "# N_list = [10,20]\n",
    "# Wb = generate_relu_dict2D(N_list).to(device)\n",
    "# print(Wb.shape)\n",
    "\n",
    "def generate_relu_dict4plusD_sphere(dim, s,N0): # \n",
    "    samples = torch.randn(s*N0,dim +1) \n",
    "    samples = samples/samples.norm(dim=1,keepdim=True)  \n",
    "    Wb = samples \n",
    "    return Wb "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_convergence_order(err_l2,err_h10,exponent,dict_size, filename,write2file = False):\n",
    "    \n",
    "    if write2file:\n",
    "        file_mode = \"a\" if os.path.exists(filename) else \"w\"\n",
    "        f_write = open(filename, file_mode)\n",
    "    \n",
    "    neuron_nums = [2**j for j in range(2,exponent+1)]\n",
    "    err_list = [err_l2[i] for i in neuron_nums ]\n",
    "    err_list2 = [err_h10[i] for i in neuron_nums ] \n",
    "    # f_write.write('M:{}, relu {} \\n'.format(M,k))\n",
    "    if write2file:\n",
    "        f_write.write('dictionary size: {}\\n'.format(dict_size))\n",
    "        f_write.write(\"neuron num \\t\\t error \\t\\t order \\t\\t h10 error \\\\ order \\n\")\n",
    "    print(\"neuron num \\t\\t error \\t\\t order\")\n",
    "    for i, item in enumerate(err_list):\n",
    "        if i == 0: \n",
    "            # print(neuron_nums[i], end = \"\\t\\t\")\n",
    "            # print(item, end = \"\\t\\t\")\n",
    "            \n",
    "            # print(\"*\")\n",
    "            print(\"{} \\t\\t {:.6f} \\t\\t * \\t\\t {:.6f} \\t\\t * \\n\".format(neuron_nums[i],item, err_list2[i] ) )\n",
    "            if write2file: \n",
    "                f_write.write(\"{} \\t\\t {} \\t\\t * \\t\\t {} \\t\\t * \\n\".format(neuron_nums[i],item, err_list2[i] ))\n",
    "        else: \n",
    "            # print(neuron_nums[i], end = \"\\t\\t\")\n",
    "            # print(item, end = \"\\t\\t\") \n",
    "            # print(np.log(err_list[i-1]/err_list[i])/np.log(2))\n",
    "            print(\"{} \\t\\t {:.6f} \\t\\t {:.6f} \\t\\t {:.6f} \\t\\t {:.6f} \\n\".format(neuron_nums[i],item,np.log(err_list[i-1]/err_list[i])/np.log(2),err_list2[i] , np.log(err_list2[i-1]/err_list2[i])/np.log(2) ) )\n",
    "            if write2file: \n",
    "                f_write.write(\"{} \\t\\t {} \\t\\t {} \\t\\t {} \\t\\t {} \\n\".format(neuron_nums[i],item,np.log(err_list[i-1]/err_list[i])/np.log(2),err_list2[i] , np.log(err_list2[i-1]/err_list2[i])/np.log(2) ))\n",
    "    if write2file:     \n",
    "        f_write.write(\"\\n\")\n",
    "        f_write.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using linear solver:  direct\n",
      "epoch:  1\tassembling the mass matrix time taken:  0.026629209518432617\n",
      "assembling the stiffness matrix time taken:  0.0006809234619140625\n",
      "solving Ax = b time taken:  0.0002460479736328125\n",
      "epoch:  2\tassembling the mass matrix time taken:  0.02208113670349121\n",
      "assembling the stiffness matrix time taken:  0.0008609294891357422\n",
      "solving Ax = b time taken:  0.00014519691467285156\n",
      "epoch:  3\tassembling the mass matrix time taken:  0.02110886573791504\n",
      "assembling the stiffness matrix time taken:  0.0007917881011962891\n",
      "solving Ax = b time taken:  0.0001437664031982422\n",
      "epoch:  4\tassembling the mass matrix time taken:  0.019301176071166992\n",
      "assembling the stiffness matrix time taken:  0.0016169548034667969\n",
      "solving Ax = b time taken:  0.0001552104949951172\n",
      "epoch:  5\tassembling the mass matrix time taken:  0.021066904067993164\n",
      "assembling the stiffness matrix time taken:  0.0010559558868408203\n",
      "solving Ax = b time taken:  0.0001437664031982422\n",
      "epoch:  6\tassembling the mass matrix time taken:  0.021824121475219727\n",
      "assembling the stiffness matrix time taken:  0.001110076904296875\n",
      "solving Ax = b time taken:  0.00013709068298339844\n",
      "epoch:  7\tassembling the mass matrix time taken:  0.026191234588623047\n",
      "assembling the stiffness matrix time taken:  0.0019519329071044922\n",
      "solving Ax = b time taken:  0.0001499652862548828\n",
      "epoch:  8\tassembling the mass matrix time taken:  0.0209810733795166\n",
      "assembling the stiffness matrix time taken:  0.0013208389282226562\n",
      "solving Ax = b time taken:  0.0001380443572998047\n",
      "epoch:  9\tassembling the mass matrix time taken:  0.025481224060058594\n",
      "assembling the stiffness matrix time taken:  0.0024700164794921875\n",
      "solving Ax = b time taken:  0.00016427040100097656\n",
      "epoch:  10\tassembling the mass matrix time taken:  0.02186894416809082\n",
      "assembling the stiffness matrix time taken:  0.002201080322265625\n",
      "solving Ax = b time taken:  0.0001430511474609375\n",
      "epoch:  11\tassembling the mass matrix time taken:  0.022321224212646484\n",
      "assembling the stiffness matrix time taken:  0.004750967025756836\n",
      "solving Ax = b time taken:  0.0001857280731201172\n",
      "epoch:  12\tassembling the mass matrix time taken:  0.021338939666748047\n",
      "assembling the stiffness matrix time taken:  0.0018696784973144531\n",
      "solving Ax = b time taken:  0.00015091896057128906\n",
      "epoch:  13\tassembling the mass matrix time taken:  0.023113012313842773\n",
      "assembling the stiffness matrix time taken:  0.0024690628051757812\n",
      "solving Ax = b time taken:  0.0001583099365234375\n",
      "epoch:  14\tassembling the mass matrix time taken:  0.02262115478515625\n",
      "assembling the stiffness matrix time taken:  0.002647876739501953\n",
      "solving Ax = b time taken:  0.00015807151794433594\n",
      "epoch:  15\tassembling the mass matrix time taken:  0.024591922760009766\n",
      "assembling the stiffness matrix time taken:  0.004865169525146484\n",
      "solving Ax = b time taken:  0.0001747608184814453\n",
      "epoch:  16\tassembling the mass matrix time taken:  0.022298097610473633\n",
      "assembling the stiffness matrix time taken:  0.002724170684814453\n",
      "solving Ax = b time taken:  0.00017309188842773438\n",
      "epoch:  17\tassembling the mass matrix time taken:  0.0230557918548584\n",
      "assembling the stiffness matrix time taken:  0.006885051727294922\n",
      "solving Ax = b time taken:  0.0002899169921875\n",
      "epoch:  18\tassembling the mass matrix time taken:  0.03252887725830078\n",
      "assembling the stiffness matrix time taken:  0.007876873016357422\n",
      "solving Ax = b time taken:  0.00021886825561523438\n",
      "epoch:  19\tassembling the mass matrix time taken:  0.025603771209716797\n",
      "assembling the stiffness matrix time taken:  0.0058329105377197266\n",
      "solving Ax = b time taken:  0.00021028518676757812\n",
      "epoch:  20\tassembling the mass matrix time taken:  0.02309393882751465\n",
      "assembling the stiffness matrix time taken:  0.004309177398681641\n",
      "solving Ax = b time taken:  0.00021314620971679688\n",
      "epoch:  21\tassembling the mass matrix time taken:  0.02330803871154785\n",
      "assembling the stiffness matrix time taken:  0.0036301612854003906\n",
      "solving Ax = b time taken:  0.00021982192993164062\n",
      "epoch:  22\tassembling the mass matrix time taken:  0.023249149322509766\n",
      "assembling the stiffness matrix time taken:  0.00471806526184082\n",
      "solving Ax = b time taken:  0.00019669532775878906\n",
      "epoch:  23\tassembling the mass matrix time taken:  0.028940916061401367\n",
      "assembling the stiffness matrix time taken:  0.004477977752685547\n",
      "solving Ax = b time taken:  0.00020599365234375\n",
      "epoch:  24\tassembling the mass matrix time taken:  0.028013944625854492\n",
      "assembling the stiffness matrix time taken:  0.004446983337402344\n",
      "solving Ax = b time taken:  0.00021505355834960938\n",
      "epoch:  25\tassembling the mass matrix time taken:  0.028022050857543945\n",
      "assembling the stiffness matrix time taken:  0.00710606575012207\n",
      "solving Ax = b time taken:  0.00019979476928710938\n",
      "epoch:  26\tassembling the mass matrix time taken:  0.02507805824279785\n",
      "assembling the stiffness matrix time taken:  0.0040130615234375\n",
      "solving Ax = b time taken:  0.00019407272338867188\n",
      "epoch:  27\tassembling the mass matrix time taken:  0.044332027435302734\n",
      "assembling the stiffness matrix time taken:  0.018697023391723633\n",
      "solving Ax = b time taken:  0.00021386146545410156\n",
      "epoch:  28\tassembling the mass matrix time taken:  0.041771888732910156\n",
      "assembling the stiffness matrix time taken:  0.0188443660736084\n",
      "solving Ax = b time taken:  0.0003218650817871094\n",
      "epoch:  29\tassembling the mass matrix time taken:  0.04927802085876465\n",
      "assembling the stiffness matrix time taken:  0.012865066528320312\n",
      "solving Ax = b time taken:  0.00020194053649902344\n",
      "epoch:  30\tassembling the mass matrix time taken:  0.041200876235961914\n",
      "assembling the stiffness matrix time taken:  0.011859893798828125\n",
      "solving Ax = b time taken:  0.00022029876708984375\n",
      "epoch:  31\tassembling the mass matrix time taken:  0.044777870178222656\n",
      "assembling the stiffness matrix time taken:  0.012269258499145508\n",
      "solving Ax = b time taken:  0.0001914501190185547\n",
      "epoch:  32\tassembling the mass matrix time taken:  0.0441281795501709\n",
      "assembling the stiffness matrix time taken:  0.014008045196533203\n",
      "solving Ax = b time taken:  0.0003361701965332031\n",
      "time taken:  3.7852301597595215\n",
      "neuron num \t\t error \t\t order\n",
      "4 \t\t 3.051433 \t\t * \t\t 15.074060 \t\t * \n",
      "\n",
      "8 \t\t 3.718198 \t\t -0.285116 \t\t 13.976237 \t\t 0.109092 \n",
      "\n",
      "16 \t\t 3.285216 \t\t 0.178615 \t\t 10.554137 \t\t 0.405167 \n",
      "\n",
      "32 \t\t 0.335707 \t\t 3.290715 \t\t 4.366714 \t\t 1.273189 \n",
      "\n",
      "using linear solver:  direct\n",
      "epoch:  1\tassembling the mass matrix time taken:  0.017646074295043945\n",
      "assembling the stiffness matrix time taken:  0.00019097328186035156\n",
      "solving Ax = b time taken:  9.608268737792969e-05\n",
      "epoch:  2\tassembling the mass matrix time taken:  0.01948714256286621\n",
      "assembling the stiffness matrix time taken:  0.0006148815155029297\n",
      "solving Ax = b time taken:  0.00012373924255371094\n",
      "epoch:  3\tassembling the mass matrix time taken:  0.01792597770690918\n",
      "assembling the stiffness matrix time taken:  0.0005681514739990234\n",
      "solving Ax = b time taken:  0.00011706352233886719\n",
      "epoch:  4\tassembling the mass matrix time taken:  0.03195595741271973\n",
      "assembling the stiffness matrix time taken:  0.00116729736328125\n",
      "solving Ax = b time taken:  0.00013709068298339844\n",
      "epoch:  5\tassembling the mass matrix time taken:  0.018312931060791016\n",
      "assembling the stiffness matrix time taken:  0.0009789466857910156\n",
      "solving Ax = b time taken:  0.0001232624053955078\n",
      "epoch:  6\tassembling the mass matrix time taken:  0.019316911697387695\n",
      "assembling the stiffness matrix time taken:  0.0010628700256347656\n",
      "solving Ax = b time taken:  0.0001327991485595703\n",
      "epoch:  7\tassembling the mass matrix time taken:  0.020409107208251953\n",
      "assembling the stiffness matrix time taken:  0.001851797103881836\n",
      "solving Ax = b time taken:  0.00013589859008789062\n",
      "epoch:  8\tassembling the mass matrix time taken:  0.02361297607421875\n",
      "assembling the stiffness matrix time taken:  0.0013649463653564453\n",
      "solving Ax = b time taken:  0.00013399124145507812\n",
      "epoch:  9\tassembling the mass matrix time taken:  0.02314591407775879\n",
      "assembling the stiffness matrix time taken:  0.0024437904357910156\n",
      "solving Ax = b time taken:  0.00015592575073242188\n",
      "epoch:  10\tassembling the mass matrix time taken:  0.027341127395629883\n",
      "assembling the stiffness matrix time taken:  0.0020732879638671875\n",
      "solving Ax = b time taken:  0.00015807151794433594\n",
      "epoch:  11\tassembling the mass matrix time taken:  0.02405714988708496\n",
      "assembling the stiffness matrix time taken:  0.0044667720794677734\n",
      "solving Ax = b time taken:  0.00017690658569335938\n",
      "epoch:  12\tassembling the mass matrix time taken:  0.05095672607421875\n",
      "assembling the stiffness matrix time taken:  0.008015155792236328\n",
      "solving Ax = b time taken:  0.0002009868621826172\n",
      "epoch:  13\tassembling the mass matrix time taken:  0.02457904815673828\n",
      "assembling the stiffness matrix time taken:  0.0018761157989501953\n",
      "solving Ax = b time taken:  0.00015997886657714844\n",
      "epoch:  14\tassembling the mass matrix time taken:  0.019842147827148438\n",
      "assembling the stiffness matrix time taken:  0.002768278121948242\n",
      "solving Ax = b time taken:  0.0001709461212158203\n",
      "epoch:  15\tassembling the mass matrix time taken:  0.02608323097229004\n",
      "assembling the stiffness matrix time taken:  0.004132986068725586\n",
      "solving Ax = b time taken:  0.00015497207641601562\n",
      "epoch:  16\tassembling the mass matrix time taken:  0.022562026977539062\n",
      "assembling the stiffness matrix time taken:  0.004585981369018555\n",
      "solving Ax = b time taken:  0.0004401206970214844\n",
      "epoch:  17\tassembling the mass matrix time taken:  0.020691871643066406\n",
      "assembling the stiffness matrix time taken:  0.0032138824462890625\n",
      "solving Ax = b time taken:  0.0001850128173828125\n",
      "epoch:  18\tassembling the mass matrix time taken:  0.025688886642456055\n",
      "assembling the stiffness matrix time taken:  0.00374603271484375\n",
      "solving Ax = b time taken:  0.00019407272338867188\n",
      "epoch:  19\tassembling the mass matrix time taken:  0.027193069458007812\n",
      "assembling the stiffness matrix time taken:  0.003961801528930664\n",
      "solving Ax = b time taken:  0.00020194053649902344\n",
      "epoch:  20\tassembling the mass matrix time taken:  0.022861957550048828\n",
      "assembling the stiffness matrix time taken:  0.0037670135498046875\n",
      "solving Ax = b time taken:  0.00019097328186035156\n",
      "epoch:  21\tassembling the mass matrix time taken:  0.026353836059570312\n",
      "assembling the stiffness matrix time taken:  0.003537893295288086\n",
      "solving Ax = b time taken:  0.00025010108947753906\n",
      "epoch:  22\tassembling the mass matrix time taken:  0.07126617431640625\n",
      "assembling the stiffness matrix time taken:  0.004850864410400391\n",
      "solving Ax = b time taken:  0.00022912025451660156\n",
      "epoch:  23\tassembling the mass matrix time taken:  0.02327704429626465\n",
      "assembling the stiffness matrix time taken:  0.005133152008056641\n",
      "solving Ax = b time taken:  0.0002148151397705078\n",
      "epoch:  24\tassembling the mass matrix time taken:  0.022404909133911133\n",
      "assembling the stiffness matrix time taken:  0.0037000179290771484\n",
      "solving Ax = b time taken:  0.0002167224884033203\n",
      "epoch:  25\tassembling the mass matrix time taken:  0.028262853622436523\n",
      "assembling the stiffness matrix time taken:  0.004704952239990234\n",
      "solving Ax = b time taken:  0.0002110004425048828\n",
      "epoch:  26\tassembling the mass matrix time taken:  0.023957014083862305\n",
      "assembling the stiffness matrix time taken:  0.010905981063842773\n",
      "solving Ax = b time taken:  0.00034809112548828125\n",
      "epoch:  27\tassembling the mass matrix time taken:  0.02712416648864746\n",
      "assembling the stiffness matrix time taken:  0.01597118377685547\n",
      "solving Ax = b time taken:  0.00041484832763671875\n",
      "epoch:  28\tassembling the mass matrix time taken:  0.02443385124206543\n",
      "assembling the stiffness matrix time taken:  0.0037119388580322266\n",
      "solving Ax = b time taken:  0.0002028942108154297\n",
      "epoch:  29\tassembling the mass matrix time taken:  0.027322769165039062\n",
      "assembling the stiffness matrix time taken:  0.00903773307800293\n",
      "solving Ax = b time taken:  0.00042891502380371094\n",
      "epoch:  30\tassembling the mass matrix time taken:  0.023231983184814453\n",
      "assembling the stiffness matrix time taken:  0.010312318801879883\n",
      "solving Ax = b time taken:  0.0002200603485107422\n",
      "epoch:  31\tassembling the mass matrix time taken:  0.02486276626586914\n",
      "assembling the stiffness matrix time taken:  0.008317947387695312\n",
      "solving Ax = b time taken:  0.00022482872009277344\n",
      "epoch:  32\tassembling the mass matrix time taken:  0.03260016441345215\n",
      "assembling the stiffness matrix time taken:  0.02114105224609375\n",
      "solving Ax = b time taken:  0.00022602081298828125\n",
      "time taken:  3.928718090057373\n",
      "neuron num \t\t error \t\t order\n",
      "4 \t\t 2.089290 \t\t * \t\t 15.560229 \t\t * \n",
      "\n",
      "8 \t\t 5.920349 \t\t -1.502669 \t\t 14.198278 \t\t 0.132147 \n",
      "\n",
      "16 \t\t 4.016813 \t\t 0.559631 \t\t 10.886072 \t\t 0.383232 \n",
      "\n",
      "32 \t\t 0.271243 \t\t 3.888391 \t\t 4.408513 \t\t 1.304119 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "function_name = \"biharmonic\" \n",
    "filename_write = \"2DOGA-{}-order.txt\".format(function_name)\n",
    "f_write = open(filename_write, \"a\")\n",
    "f_write.write(\"\\n\")\n",
    "f_write.close() \n",
    "save = True \n",
    "relu_k= 3 \n",
    "\n",
    "trial_num = 5 \n",
    "for trial in range(trial_num): \n",
    "    for N_list in [[2**5,2**5]]: # ,[2**6,2**6],[2**7,2**7] \n",
    "        # save = True \n",
    "        f_write = open(filename_write, \"a\")\n",
    "        my_model = None \n",
    "        Nx = 400\n",
    "        order = 2   \n",
    "        exponent = 9\n",
    "        num_epochs = 2**exponent  \n",
    "        plot_freq = num_epochs \n",
    "        N = np.prod(N_list)\n",
    "        errl2,errh1,errh2, my_model = OGABiharmonicReLU2D(my_model,rhs,u_exact,derivatives, N_list,num_epochs,plot_freq, Nx, order, k = relu_k, rand_deter= 'rand', linear_solver = \"direct\")\n",
    "        \n",
    "        if save: \n",
    "            folder = 'data-biharmonic-2d-relu3/'\n",
    "            filename = folder + 'err_OGA_2D_{}_neuron_{}_N_{}_rand_trial_{}.pt'.format(function_name,num_epochs,N,trial)\n",
    "            torch.save(errl2,filename) \n",
    "            filename = folder + 'errh2_OGA_2D_{}_neuron_{}_N_{}_rand_trial_{}.pt'.format(function_name,num_epochs,N,trial)\n",
    "            torch.save(errh2,filename)\n",
    "            folder = 'data-biharmonic-2d/'\n",
    "            filename = folder + 'model_OGA_2D_{}_neuron_{}_N_{}_rand_trial_{}.pt'.format(function_name,num_epochs,N,trial)\n",
    "            torch.save(my_model.state_dict(),filename)\n",
    "\n",
    "    show_convergence_order(errh1,errh2,exponent,N, filename_write,write2file = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
