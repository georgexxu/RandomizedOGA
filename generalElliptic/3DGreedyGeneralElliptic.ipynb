{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1D General elliptic PDE of the following form: \n",
    "## -div( a(x) grad u(x)) + b(x) grad u(x) + c(x) u(x) = f(x) in [0,1] \n",
    "## a(x), b(x), c(x) are set to be constant functions \n",
    "## du_dn = g on the boundary \n",
    "## this version also contains using the tanh-activated shallow neural network to solve the PDE \n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import sys\n",
    "import os \n",
    "from scipy.sparse import linalg\n",
    "from pathlib import Path\n",
    "if torch.cuda.is_available():  \n",
    "    device = \"cuda\" \n",
    "else:  \n",
    "    device = \"cpu\" \n",
    "\n",
    "torch.set_default_dtype(torch.float64)\n",
    "pi = torch.tensor(np.pi)\n",
    "ZERO = torch.tensor([0.]).to(device)\n",
    "\n",
    "LAMBDA = 2 # c(x) = -LAMBDA**2, Helmholtz equation parameters\n",
    "BETA = 10 ## convection term parameters \n",
    "DIMENSION = 3  ## dimension of the problem \n",
    "\n",
    "## Define the neural network model\n",
    "## already general in any dimension\n",
    "class model_tanh(nn.Module):\n",
    "    \"\"\" cosine shallow neural network\n",
    "    Parameters: \n",
    "    input size: input dimension\n",
    "    hidden_size1 : number of hidden layers \n",
    "    num_classes: output classes \n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, hidden_size1, num_classes):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size1)\n",
    "        self.fc2 = nn.Linear(hidden_size1, num_classes,bias = False)\n",
    "    def forward(self, x):\n",
    "        u1 = self.fc2( F.tanh(self.fc1(x)) )\n",
    "        return u1\n",
    "    \n",
    "    def tanh_activation_dx(self,x): \n",
    "        return 1/torch.cosh(x)**2  \n",
    "      \n",
    "    def evaluate_derivative(self, x, i):\n",
    "        u1 = self.fc2( self.tanh_activation_dx(self.fc1(x)) *self.fc1.weight.t()[i-1:i,:] )  \n",
    "        return u1\n",
    "class model(nn.Module):\n",
    "    \"\"\" ReLU k shallow neural network\n",
    "    Parameters: \n",
    "    input size: input dimension\n",
    "    hidden_size1 : number of hidden layers \n",
    "    num_classes: output classes \n",
    "    k: degree of relu functions\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, hidden_size1, num_classes,k = 1):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size1)\n",
    "        self.fc2 = nn.Linear(hidden_size1, num_classes,bias = False)\n",
    "        self.k = k \n",
    "    def forward(self, x):\n",
    "        u1 = self.fc2(F.relu(self.fc1(x))**self.k)\n",
    "        return u1\n",
    "    def evaluate_derivative(self, x, i):\n",
    "        if self.k == 1:\n",
    "            ## ZERO = torch.tensor([0.]).to(device)\n",
    "            u1 = self.fc2(torch.heaviside(self.fc1(x),ZERO) * self.fc1.weight.t()[i-1:i,:] )\n",
    "        else:\n",
    "            u1 = self.fc2(self.k*F.relu(self.fc1(x))**(self.k-1) *self.fc1.weight.t()[i-1:i,:] )  \n",
    "        return u1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_solution_modified(r1,r2,model,x_test,u_true,name=None): \n",
    "    # Plot function: test results \n",
    "    u_model_cpu = model(x_test).cpu().detach()\n",
    "    \n",
    "    w = model.fc1.weight.data.squeeze()\n",
    "    b = model.fc1.bias.data.squeeze()\n",
    "    x_model_pt = (-b/w).view(-1,1)\n",
    "    x_model_pt = x_model_pt[x_model_pt>=r1].reshape(-1,1)\n",
    "    u_model_pt = model(x_model_pt).cpu().detach()\n",
    "    plt.figure(dpi = 100)\n",
    "    plt.plot(x_test.cpu(),u_model_cpu,'-.',label = \"nn function\")\n",
    "    plt.plot(x_test.cpu(),u_true.cpu(),label = \"true\")\n",
    "    # plt.plot(x_model_pt.cpu(),u_model_pt.cpu(),'.r')\n",
    "    if name!=None: \n",
    "        plt.title(name)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PiecewiseGQ1D_weights_points(x_l,x_r,Nx, order):\n",
    "    \"\"\" Output the coeffients and weights for piecewise Gauss Quadrature \n",
    "    Parameters\n",
    "    ----------\n",
    "    x_l : float \n",
    "    left endpoint of an interval \n",
    "    x_r: float\n",
    "    right endpoint of an interval \n",
    "    Nx: int \n",
    "    number of subintervals for integration\n",
    "    order: int\n",
    "    order of Gauss Quadrature \n",
    "    Returns\n",
    "    -------\n",
    "    vectorized quadrature weights and integration points\n",
    "    \"\"\"\n",
    "    x,w = np.polynomial.legendre.leggauss(order)\n",
    "    gx = torch.tensor(x).to(device)\n",
    "    gx = gx.view(1,-1) # row vector \n",
    "    gw = torch.tensor(w).to(device)    \n",
    "    gw = gw.view(-1,1) # Column vector \n",
    "    nodes = torch.linspace(x_l,x_r,Nx+1).view(-1,1).to(device) \n",
    "    coef1 = ((nodes[1:,:] - nodes[:-1,:])/2) # n by 1  \n",
    "    coef2 = ((nodes[1:,:] + nodes[:-1,:])/2) # n by 1  \n",
    "    coef2_expand = coef2.expand(-1,gx.size(1)) # Expand to n by p shape, -1: keep the first dimension n , expand the 2nd dim (columns)\n",
    "    integration_points = coef1@gx + coef2_expand\n",
    "    integration_points = integration_points.flatten().view(-1,1) # Make it a column vector\n",
    "    gw_expand = torch.tile(gw,(Nx,1)) # rows: n copies of current tensor, columns: 1 copy, no change\n",
    "    # Modify coef1 to be compatible with func_values\n",
    "    coef1_expand = coef1.expand(coef1.size(0),gx.size(1))    \n",
    "    coef1_expand = coef1_expand.flatten().view(-1,1)\n",
    "    return coef1_expand.to(device) * gw_expand.to(device), integration_points.to(device)\n",
    "\n",
    "def PiecewiseGQ2D_weights_points(Nx, order): \n",
    "    \"\"\" A slight modification of PiecewiseGQ2D function that only needs the weights and integration points.\n",
    "    Parameters\n",
    "    ----------\n",
    "\n",
    "    Nx: int \n",
    "        number of intervals along the dimension. No Ny, assume Nx = Ny\n",
    "    order: int \n",
    "        order of the Gauss Quadrature\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    long_weights: torch.tensor\n",
    "    integration_points: torch.tensor\n",
    "    \"\"\"\n",
    "\n",
    "#     print(\"order: \",order )\n",
    "    x, w = np.polynomial.legendre.leggauss(order)\n",
    "    gauss_pts = np.array(np.meshgrid(x,x,indexing='ij')).reshape(2,-1).T\n",
    "    weights =  (w*w[:,None]).ravel()\n",
    "\n",
    "    gauss_pts =torch.tensor(gauss_pts)\n",
    "    weights = torch.tensor(weights)\n",
    "\n",
    "    h = 1/Nx # 100 intervals \n",
    "    long_weights =  torch.tile(weights,(Nx**2,1))\n",
    "    long_weights = long_weights.reshape(-1,1)\n",
    "    long_weights = long_weights * h**2 /4 \n",
    "\n",
    "    integration_points = torch.tile(gauss_pts,(Nx**2,1))\n",
    "    scale_factor = h/2 \n",
    "    integration_points = scale_factor * integration_points\n",
    "\n",
    "    index = np.arange(1,Nx+1)-0.5\n",
    "    ordered_pairs = np.array(np.meshgrid(index,index,indexing='ij'))\n",
    "    ordered_pairs = ordered_pairs.reshape(2,-1).T\n",
    "\n",
    "    # print(ordered_pairs)\n",
    "    # print()\n",
    "    ordered_pairs = torch.tensor(ordered_pairs)\n",
    "    # print(ordered_pairs.size())\n",
    "    ordered_pairs = torch.tile(ordered_pairs, (1,order**2)) # number of GQ points\n",
    "    # print(ordered_pairs)\n",
    "\n",
    "    ordered_pairs =  ordered_pairs.reshape(-1,2)\n",
    "    # print(ordered_pairs)\n",
    "    translation = ordered_pairs*h \n",
    "    # print(translation)\n",
    "\n",
    "    integration_points = integration_points + translation \n",
    "#     print(integration_points.size())\n",
    "    # func_values = integrand2_torch(integration_points)\n",
    "    return long_weights.to(device), integration_points.to(device)\n",
    "\n",
    "def PiecewiseGQ3D_weights_points(Nx, order): \n",
    "    \"\"\" A slight modification of PiecewiseGQ2D function that only needs the weights and integration points.\n",
    "    Parameters\n",
    "    ----------\n",
    "\n",
    "    Nx: int \n",
    "        number of intervals along the dimension. No Ny, assume Nx = Ny\n",
    "    order: int \n",
    "        order of the Gauss Quadrature\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    long_weights: torch.tensor\n",
    "    integration_points: torch.tensor\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    target : \n",
    "        Target function \n",
    "    Nx: int \n",
    "        number of intervals along the dimension. No Ny, assume Nx = Ny\n",
    "    order: int \n",
    "        order of the Gauss Quadrature\n",
    "    \"\"\"\n",
    "\n",
    "    # print(\"order: \",order )\n",
    "    x, w = np.polynomial.legendre.leggauss(order)\n",
    "    gauss_pts = np.array(np.meshgrid(x,x,x,indexing='ij')).reshape(3,-1).T\n",
    "    weight_list = np.array(np.meshgrid(w,w,w,indexing='ij'))\n",
    "    weights =   (weight_list[0]*weight_list[1]*weight_list[2]).ravel() \n",
    "\n",
    "    gauss_pts =torch.tensor(gauss_pts)\n",
    "    weights = torch.tensor(weights)\n",
    "\n",
    "    h = 1/Nx # 100 intervals \n",
    "    long_weights =  torch.tile(weights,(Nx**3,1))\n",
    "    long_weights = long_weights.reshape(-1,1)\n",
    "    long_weights = long_weights * h**3 /8 \n",
    "\n",
    "    integration_points = torch.tile(gauss_pts,(Nx**3,1))\n",
    "    # print(\"shape of integration_points\", integration_points.size())\n",
    "    scale_factor = h/2 \n",
    "    integration_points = scale_factor * integration_points\n",
    "\n",
    "    index = np.arange(1,Nx+1)-0.5\n",
    "    ordered_pairs = np.array(np.meshgrid(index,index,index,indexing='ij'))\n",
    "    ordered_pairs = ordered_pairs.reshape(3,-1).T\n",
    "\n",
    "    # print(ordered_pairs)\n",
    "    # print()\n",
    "    ordered_pairs = torch.tensor(ordered_pairs)\n",
    "    # print(ordered_pairs.size())\n",
    "    ordered_pairs = torch.tile(ordered_pairs, (1,order**3)) # number of GQ points\n",
    "    # print(ordered_pairs)\n",
    "\n",
    "    ordered_pairs =  ordered_pairs.reshape(-1,3)\n",
    "    # print(ordered_pairs)\n",
    "    translation = ordered_pairs*h \n",
    "    # print(translation)\n",
    "\n",
    "    integration_points = integration_points + translation \n",
    "\n",
    "    return long_weights.to(device), integration_points.to(device)\n",
    "\n",
    "def MonteCarlo_Sobol_dDim_weights_points(M ,d = 4):\n",
    "    Sob_integral = torch.quasirandom.SobolEngine(dimension =d, scramble= False, seed=None) \n",
    "    integration_points = Sob_integral.draw(M).double() \n",
    "    integration_points = integration_points.to(device)\n",
    "    weights = torch.ones(M,1).to(device)/M \n",
    "    return weights, integration_points \n",
    "\n",
    "def Neumann_boundary_quadrature_points_weights(M,d):\n",
    "    def generate_quadpts_on_boundary(gw_expand_bd, integration_points_bd,d):\n",
    "        size_pts_bd = integration_points_bd.size(0) \n",
    "        gw_expand_bd_faces = torch.tile(gw_expand_bd,(2*d,1)) # 2d boundaries, 拉成长条\n",
    "\n",
    "        integration_points_bd_faces = torch.zeros(2*d*integration_points_bd.size(0),d).to(device)\n",
    "        for ind in range(d): \n",
    "            integration_points_bd_faces[2 *ind * size_pts_bd :(2 *ind +1) * size_pts_bd,ind:ind+1] = 0 \n",
    "            integration_points_bd_faces[(2 *ind)*size_pts_bd :(2 * ind +1) * size_pts_bd,:ind] = integration_points_bd[:,:ind]\n",
    "            integration_points_bd_faces[(2 *ind)*size_pts_bd :(2 * ind +1) * size_pts_bd,ind+1:] = integration_points_bd[:,ind:]\n",
    "\n",
    "            integration_points_bd_faces[(2 *ind +1) * size_pts_bd:(2 *ind +2)*size_pts_bd,ind:ind+1] = 1\n",
    "            integration_points_bd_faces[(2 *ind +1) * size_pts_bd:(2 *ind +2)*size_pts_bd,:ind] = integration_points_bd[:,:ind]        \n",
    "            integration_points_bd_faces[(2 *ind +1) * size_pts_bd:(2 *ind +2)*size_pts_bd,ind+1:] = integration_points_bd[:,ind:]\n",
    "        return gw_expand_bd_faces, integration_points_bd_faces\n",
    "    \n",
    "    if d == 1: \n",
    "        print('dim',d)\n",
    "        gw_expand_bd_faces = torch.tensor([1.,1.]).view(-1,1).to(device)\n",
    "        integration_points_bd_faces = torch.tensor([0.,1.]).view(-1,1).to(device) \n",
    "    elif d == 2: \n",
    "        print('dim',d)\n",
    "        gw_expand_bd, integration_points_bd = PiecewiseGQ1D_weights_points(0,1,8192, order = 3) \n",
    "    elif d == 3: \n",
    "        gw_expand_bd, integration_points_bd = PiecewiseGQ2D_weights_points(200, order = 3) \n",
    "    elif d == 4: \n",
    "        gw_expand_bd, integration_points_bd = PiecewiseGQ3D_weights_points(25, order = 3) \n",
    "        print('dim',d)\n",
    "    else: \n",
    "        gw_expand_bd, integration_points_bd = MonteCarlo_Sobol_dDim_weights_points(M ,d = d)\n",
    "        print('dim >=5 ')\n",
    "    gw_expand_bd_faces, integration_points_bd_faces = generate_quadpts_on_boundary(gw_expand_bd, integration_points_bd,d)\n",
    "    return gw_expand_bd_faces.to(device), integration_points_bd_faces.to(device) \n",
    "\n",
    "def generate_relu_dict3D(N_list):\n",
    "    N1 = N_list[0]\n",
    "    N2 = N_list[1]\n",
    "    N3 = N_list[2]\n",
    "    \n",
    "    N = N1*N2*N3 \n",
    "    theta1 = np.linspace(0, pi, N1, endpoint= True).reshape(N1,1)\n",
    "    theta2 = np.linspace(0, 2*pi, N2, endpoint= False).reshape(N2,1)\n",
    "    b = np.linspace(-3**0.5, 3**0.5, N3,endpoint=False).reshape(N3,1) # threshold: 3**0.5  \n",
    "    coord3 = np.array(np.meshgrid(theta1,theta2,b,indexing='ij'))\n",
    "    coord3 = coord3.reshape(3,-1).T # N1*N2*N3 x 3. coordinates for the grid points \n",
    "    coord3 = torch.tensor(coord3) \n",
    "\n",
    "    f1 = torch.zeros(N,1) \n",
    "    f2 = torch.zeros(N,1)\n",
    "    f3 = torch.zeros(N,1)\n",
    "    f4 = torch.zeros(N,1)\n",
    "\n",
    "    f1[:,0] = torch.cos(coord3[:,0]) \n",
    "    f2[:,0] = torch.sin(coord3[:,0]) * torch.cos(coord3[:,1])\n",
    "    f3[:,0] = torch.sin(coord3[:,0]) * torch.sin(coord3[:,1])\n",
    "    f4[:,0] = coord3[:,2] \n",
    "\n",
    "    Wb_tensor = torch.cat([f1,f2,f3,f4],1) # N x 4 \n",
    "    return Wb_tensor\n",
    "\n",
    "\n",
    "def generate_relu_dict3D_QMC(s,N0):\n",
    "#     Sob = torch.quasirandom.SobolEngine(dimension =3, scramble= True, seed=None) \n",
    "#     samples = Sob.draw(N0).double() \n",
    "\n",
    "#     for i in range(s-1):\n",
    "#         samples = torch.cat([samples,Sob.draw(N0).double()],0)\n",
    "\n",
    "    # Monte Carlo \n",
    "    samples = torch.rand(s*N0,3) \n",
    "    T =torch.tensor([[pi,0,0],[0,2*pi,0],[0,0,3**0.5 *2]])\n",
    "    shift = torch.tensor([0,0,-3**0.5])\n",
    "    samples = samples@T + shift \n",
    "\n",
    "    f1 = torch.zeros(s*N0,1) \n",
    "    f2 = torch.zeros(s*N0,1)\n",
    "    f3 = torch.zeros(s*N0,1)\n",
    "    f4 = torch.zeros(s*N0,1)\n",
    "\n",
    "    f1[:,0] = torch.cos(samples[:,0]) \n",
    "    f2[:,0] = torch.sin(samples[:,0]) * torch.cos(samples[:,1])\n",
    "    f3[:,0] = torch.sin(samples[:,0]) * torch.sin(samples[:,1])\n",
    "    f4[:,0] = samples[:,2] \n",
    "\n",
    "    Wb_tensor = torch.cat([f1,f2,f3,f4],1) # N x 4 \n",
    "    return Wb_tensor\n",
    "\n",
    "def generate_tanh_dict3D_QMC(s,N0,Rm):\n",
    "    # Monte Carlo \n",
    "    samples = torch.rand(s*N0,4)\n",
    "\n",
    "    T =torch.tensor([[Rm,0,0,0],[0,Rm,0,0],[0,0,Rm,0],[0,0,0,Rm]])\n",
    "\n",
    "    samples = samples@T \n",
    "\n",
    "    return samples \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minimize_linear_layer_helmholtz(model,target,weights, integration_points,weights_bd, integration_points_bd, g_N, activation = 'relu', solver = 'direct',,memory=2**29):  \n",
    "    \"\"\"\n",
    "    calls the following functions (dependency): \n",
    "    1. GQ_piecewise_2D\n",
    "    input: the nn model containing parameter \n",
    "    1. define the loss function  \n",
    "    2. take derivative to extract the linear system A\n",
    "    3. call the cg solver in scipy to solve the linear system \n",
    "    output: sol. solution of Ax = b\n",
    "    \"\"\"\n",
    "    start_time = time.time() \n",
    "    w = model.fc1.weight.data \n",
    "    b = model.fc1.bias.data \n",
    "    neuron_num = b.size(0) \n",
    "    M = integration_points.size(0)\n",
    "    if activation == 'relu':\n",
    "        basis_value_col = F.relu(integration_points @ w.t()+ b)**(model.k) \n",
    "    elif activation == 'tanh':\n",
    "        basis_value_col = F.tanh(integration_points @ w.t ()+ b) \n",
    "    weighted_basis_value_col = basis_value_col * weights \n",
    "\n",
    "    total_size = neuron_num * M # memory, number of floating numbers \n",
    "    print('total size: {} {} = {}'.format(neuron_num,M,total_size))\n",
    "    num_batch = total_size//memory + 1 # divide according to memory\n",
    "    print(\"num batches: \",num_batch)\n",
    "    batch_size = M//num_batch\n",
    "\n",
    "    coef_func = - LAMBDA**2 # 3 * model(integration_points).detach()**2 #changing after each newton iteration \n",
    "    # jac_mass = weighted_basis_value_col.t() @ (coef_func*basis_value_col) \n",
    "\n",
    "    jac = torch.zeros(neuron_num,neuron_num).to(device)\n",
    "    rhs = torch.zeros(neuron_num,1).to(device)\n",
    "    # rhs_gN = torch.zeros(neuron_num,1).to(device)\n",
    "\n",
    "    for j in range(0,M,batch_size): \n",
    "        end_index = j + batch_size\n",
    "        if activation == 'relu':\n",
    "            basis_value_col = F.relu(integration_points[j:end_index] @ w.t()+ b)**(model.k) \n",
    "        if activation == 'tanh':\n",
    "            basis_value_col = F.tanh(integration_points[j:end_index] @ w.t()+ b)\n",
    "        weighted_basis_value_col = basis_value_col * weights[j:end_index] \n",
    "        if activation == 'relu' and model.k == 1:  \n",
    "            derivative_comm_part = torch.heaviside(integration_points[j:end_index] @ w.t()+ b, ZERO) \n",
    "        elif activation == 'relu' and model.k > 1: \n",
    "            derivative_comm_part = model.k * F.relu(integration_points[j:end_index] @ w.t()+ b)**(model.k-1)\n",
    "        elif activation == 'tanh':\n",
    "            derivative_comm_part = torch.cosh(integration_points[j:end_index] @ w.t()+ b)**(-2)    \n",
    "        for d in range(DIMENSION): \n",
    "            basis_value_dxi_col = derivative_comm_part * w.t()[d:d+1,:]\n",
    "            weighted_basis_value_dxi_col = basis_value_dxi_col * weights[j:end_index] \n",
    "            jac += weighted_basis_value_col.t() @ (coef_func * basis_value_col) # mass matrix \n",
    "            jac += weighted_basis_value_dxi_col.t() @ basis_value_dxi_col # stifness matrix \n",
    "            jac += BETA* weighted_basis_value_col[j:end_index,:].t()@basis_value_dxi_col # convection term (grad u, v)\n",
    "            rhs += weighted_basis_value_col.t() @ (target(integration_points[j,end_index]) ) #rhs \n",
    "\n",
    "    # Neumman boundary condition\n",
    "    if DIMENSION == 1: \n",
    "        if activation == 'relu':\n",
    "            basis_value_col_bd = F.relu(integration_points_bd @ w.t()+ b)**(model.k) \n",
    "        elif activation == 'tanh':\n",
    "            basis_value_col_bd = F.tanh(integration_points_bd @ w.t()+ b) \n",
    "        weighted_basis_value_col_bd = basis_value_col_bd *weights_bd \n",
    "        dudn = g_N(integration_points_bd)* (torch.tensor([-1,1]).view(-1,1)).to(device) \n",
    "        rhs_gN =  weighted_basis_value_col_bd.t() @ dudn\n",
    "    # neumann boundary condition \n",
    "    if DIMENSION > 1 and g_N != None:\n",
    "        size_pts_bd = int(integration_points_bd.size(0)/(2*DIMENSION))\n",
    "        bcs_N = g_N(DIMENSION)\n",
    "        for ii, g_ii in bcs_N:\n",
    "            #Another for loop needed if we need to divide the integration points into batches \n",
    "            weighted_g_N = -g_ii(integration_points_bd[2*ii*size_pts_bd:(2*ii+1)*size_pts_bd,:])* weights_bd[2*ii*size_pts_bd:(2*ii+1)*size_pts_bd,:]\n",
    "            if activation == 'relu':\n",
    "                basis_value_bd_col = F.relu(integration_points_bd[2*ii*size_pts_bd:(2*ii+1)*size_pts_bd,:] @ w.t()+ b)**(model.k)\n",
    "            elif activation == 'tanh':\n",
    "                basis_value_bd_col = F.tanh(integration_points_bd[2*ii*size_pts_bd:(2*ii+1)*size_pts_bd,:] @ w.t()+ b) \n",
    "            rhs_gN += basis_value_bd_col.t() @ weighted_g_N\n",
    "\n",
    "            weighted_g_N = g_ii(integration_points_bd[(2*ii+1)*size_pts_bd:(2*ii+2)*size_pts_bd,:])* weights_bd[(2*ii+1)*size_pts_bd:(2*ii+2)*size_pts_bd,:]\n",
    "            if activation == 'relu':\n",
    "                basis_value_bd_col = F.relu(integration_points_bd[(2*ii+1)*size_pts_bd:(2*ii+2)*size_pts_bd,:] @ w.t()+ b)**(model.k)\n",
    "            elif activation == 'tanh':\n",
    "                basis_value_bd_col = F.tanh(integration_points_bd[(2*ii+1)*size_pts_bd:(2*ii+2)*size_pts_bd,:] @ w.t()+ b)\n",
    "            rhs_gN += basis_value_bd_col.t() @ weighted_g_N\n",
    "\n",
    "    rhs = rhs + rhs_gN\n",
    "\n",
    "    print(\"assembling the matrix time taken: \", time.time()-start_time) \n",
    "    start_time = time.time()    \n",
    "    if solver == \"cg\": \n",
    "        sol, exit_code = linalg.cg(np.array(jac.detach().cpu()),np.array(rhs.detach().cpu()),tol=1e-12)\n",
    "        sol = torch.tensor(sol).view(1,-1)\n",
    "    elif solver == \"direct\": \n",
    "#         sol = np.linalg.inv( np.array(jac.detach().cpu()) )@np.array(rhs.detach().cpu())\n",
    "        sol = (torch.linalg.solve( jac.detach(), rhs.detach())).view(1,-1)\n",
    "    elif solver == \"ls\":\n",
    "        sol = (torch.linalg.lstsq(jac.detach().cpu(),rhs.detach().cpu(),driver='gelsd').solution).view(1,-1)\n",
    "        # sol = (torch.linalg.lstsq(jac.detach(),rhs.detach()).solution).view(1,-1) # gpu/cpu, driver = 'gels', cannot solve singular\n",
    "    print(\"solving Ax = b time taken: \", time.time()-start_time)\n",
    "    ## update the solution \n",
    "    return sol "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_convergence_order(err_l2,err_h10,exponent,dict_size, filename,write2file = False):\n",
    "    \n",
    "    if write2file:\n",
    "        file_mode = \"a\" if os.path.exists(filename) else \"w\"\n",
    "        f_write = open(filename, file_mode)\n",
    "    \n",
    "    neuron_nums = [2**j for j in range(2,exponent+1)]\n",
    "    err_list = [err_l2[i] for i in neuron_nums ]\n",
    "    err_list2 = [err_h10[i] for i in neuron_nums ] \n",
    "    # f_write.write('M:{}, relu {} \\n'.format(M,k))\n",
    "    if write2file:\n",
    "        f_write.write('dictionary size: {}\\n'.format(dict_size))\n",
    "        f_write.write(\"neuron num \\t\\t error \\t\\t order \\t\\t h10 error \\\\ order \\n\")\n",
    "    print(\"neuron num \\t\\t error \\t\\t order\")\n",
    "    for i, item in enumerate(err_list):\n",
    "        if i == 0: \n",
    "            # print(neuron_nums[i], end = \"\\t\\t\")\n",
    "            # print(item, end = \"\\t\\t\")\n",
    "            \n",
    "            # print(\"*\")\n",
    "            print(\"{} \\t\\t {:.6f} \\t\\t * \\t\\t {:.6f} \\t\\t * \\n\".format(neuron_nums[i],item, err_list2[i] ) )\n",
    "            if write2file: \n",
    "                f_write.write(\"{} \\t\\t {} \\t\\t * \\t\\t {} \\t\\t * \\n\".format(neuron_nums[i],item, err_list2[i] ))\n",
    "        else: \n",
    "            # print(neuron_nums[i], end = \"\\t\\t\")\n",
    "            # print(item, end = \"\\t\\t\") \n",
    "            # print(np.log(err_list[i-1]/err_list[i])/np.log(2))\n",
    "            print(\"{} \\t\\t {:.6f} \\t\\t {:.6f} \\t\\t {:.6f} \\t\\t {:.6f} \\n\".format(neuron_nums[i],item,np.log(err_list[i-1]/err_list[i])/np.log(2),err_list2[i] , np.log(err_list2[i-1]/err_list2[i])/np.log(2) ) )\n",
    "            if write2file: \n",
    "                f_write.write(\"{} \\t\\t {} \\t\\t {} \\t\\t {} \\t\\t {} \\n\".format(neuron_nums[i],item,np.log(err_list[i-1]/err_list[i])/np.log(2),err_list2[i] , np.log(err_list2[i-1]/err_list2[i])/np.log(2) ))\n",
    "    if write2file:     \n",
    "        f_write.write(\"\\n\")\n",
    "        f_write.close()\n",
    "\n",
    "def show_convergence_order_latex(err_l2,err_h10,exponent): \n",
    "    neuron_nums = [2**j for j in range(2,exponent+1)]\n",
    "    err_list = [err_l2[i] for i in neuron_nums ]\n",
    "    err_list2 = [err_h10[i] for i in neuron_nums ] \n",
    "    print(\"neuron num  & \\t $\\|u-u_n \\|_{L^2}$ & \\t order & \\t $ | u -u_n |_{H^1}$ & \\t order \\\\\\ \\hline \\hline \")\n",
    "    for i, item in enumerate(err_list):\n",
    "        if i == 0: \n",
    "            print(\"{} \\t\\t & {:.6f} &\\t\\t * & \\t\\t {:.6f} & \\t\\t *  \\\\\\ \\hline  \\n\".format(neuron_nums[i],item, err_list2[i] ) )   \n",
    "        else: \n",
    "            print(\"{} \\t\\t &  {:.3e} &  \\t\\t {:.2f} &  \\t\\t {:.3e} &  \\t\\t {:.2f} \\\\\\ \\hline  \\n\".format(neuron_nums[i],item,np.log(err_list[i-1]/err_list[i])/np.log(2),err_list2[i] , np.log(err_list2[i-1]/err_list2[i])/np.log(2) ) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def relu_dict(x_l,x_r,N):\n",
    "    \"\"\"generate relu dictionary parameters \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x_l: float \n",
    "    x_r: float\n",
    "    N: int \n",
    "        number of dictionary elements \n",
    "        \n",
    "    Returns\n",
    "    torch tensor\n",
    "        containing relu dictionary parameters, corresponds to nodal points\n",
    "        \n",
    "    \"\"\"\n",
    "    # w = 1 \n",
    "    relu_dict_parameters = torch.zeros((2*N,2)).to(device)\n",
    "    relu_dict_parameters[:N,0] = torch.ones(N)[:]\n",
    "    relu_dict_parameters[:N,1] = torch.linspace(x_l,x_r,N+1)[:-1] # relu(x-bi)  \n",
    "    relu_dict_parameters[N:2*N,0] = -torch.ones(N)[:]\n",
    "    relu_dict_parameters[N:2*N,1] = -torch.linspace(x_l,x_r,N+1)[1:] + 1/(2*N) # relu(-x - -bi) \n",
    "    \n",
    "    return relu_dict_parameters\n",
    "\n",
    "# relu dictionary\n",
    "def relu_dict_MC(x_l,x_r,N):\n",
    "    \"\"\"generate relu dictionary parameters \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x_l: float \n",
    "    x_r: float\n",
    "    N: int \n",
    "       number of dictionary elements  \n",
    "        \n",
    "    Returns\n",
    "    torch tensor\n",
    "        containing relu dictionary parameters, corresponds to nodal points\n",
    "    \"\"\"\n",
    "    # w = 1 \n",
    "    random_value = torch.randint(0, 2, (N,)) * 2 - 1 # +1 or -1  \n",
    "    relu_dict_parameters = torch.zeros((N,2)).to(device)\n",
    "    relu_dict_parameters[:N,0] = random_value[:]\n",
    "    relu_dict_parameters[:N,1] = (torch.rand(N)*(x_r-x_l) + x_l)*random_value # relu(x-bi) \n",
    "\n",
    "    return relu_dict_parameters\n",
    "\n",
    "# relu dictionary\n",
    "def tanh_dict_MC(x_l,x_r,N):\n",
    "    \"\"\"generate relu dictionary parameters \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x_l: float \n",
    "    x_r: float\n",
    "    N: int \n",
    "       number of dictionary elements  \n",
    "        \n",
    "    Returns\n",
    "    torch tensor\n",
    "        containing relu dictionary parameters, corresponds to nodal points\n",
    "    \"\"\"\n",
    "    # w = 1 \n",
    "    # random_value = torch.randint(0, 2, (N,)) * 2 - 1 # +1 or -1  \n",
    "\n",
    "    tanh_dict_parameters = torch.zeros((N,2)).to(device)\n",
    "    tanh_dict_parameters[:N,0] = (torch.rand(N)*(x_r-x_l) + x_l)\n",
    "    tanh_dict_parameters[:N,1] = (torch.rand(N)*(x_r-x_l) + x_l) # relu(x-bi) \n",
    "\n",
    "    return tanh_dict_parameters\n",
    "\n",
    "\n",
    "def select_greedy_neuron_ind(relu_dict_parameters,my_model,target,weights, integration_points,g_N,weights_bd, integration_points_bd,k,activation = 'relu',memory = 2**29): \n",
    "    dim = integration_points.size(1) \n",
    "    M = integration_points.size(0)\n",
    "    N0 = relu_dict_parameters.size(0)   \n",
    "    neuron_num = my_model.fc2.weight.size(1) if my_model != None else 0\n",
    "\n",
    "    output = torch.zeros(N0,1).to(device) \n",
    "    s_time = time.time()\n",
    "    total_size2 = M*(neuron_num+1)\n",
    "    num_batch2 = total_size2//memory + 1 \n",
    "    batch_size_2 = M//num_batch2 # integration points \n",
    "    residual_values = torch.zeros(M,1).to(device) \n",
    "\n",
    "    if my_model!= None:\n",
    "        for jj in range(0,M,batch_size_2): \n",
    "            end_index = jj + batch_size_2\n",
    "            residual_values[jj:end_index] = - target(integration_points[jj:end_index]) \n",
    "            residual_values[jj:end_index] += - LAMBDA**2 * my_model(integration_points[jj:end_index])\n",
    "    else:  \n",
    "        for jj in range(0,M,batch_size_2): \n",
    "            end_index = jj + batch_size_2\n",
    "            residual_values[jj:end_index] = - target(integration_points[jj:end_index])\n",
    "    weight_func_values = residual_values*weights\n",
    "\n",
    "###=======working zone below==========\n",
    "    total_size = M * N0 \n",
    "    num_batch = total_size//memory + 1 \n",
    "    batch_size_1 = N0//num_batch # dictionary elements\n",
    "    print(\"======argmax subproblem:f and N(u) terms, num batches: \",num_batch)\n",
    "    for j in range(0,N0,batch_size_1):\n",
    "        end_index = j + batch_size_1 \n",
    "        if activation == 'relu':\n",
    "            basis_values = (F.relu( torch.matmul(integration_points,relu_dict_parameters[j:end_index,0:dim].T ) - relu_dict_parameters[j:end_index,dim])**k) # uses broadcasting\n",
    "        elif activation == 'tanh':\n",
    "            basis_values = (F.tanh( torch.matmul(integration_points,relu_dict_parameters[j:end_index,0:dim].T ) - relu_dict_parameters[j:end_index,dim]))\n",
    "        output[j:end_index] += basis_values.t()@weight_func_values #\n",
    "    print('======TIME=======f and N(u) terms time :',time.time()-s_time)\n",
    "###=======working zone above=========\n",
    "    s_time =time.time() \n",
    "    if my_model!= None:\n",
    "        #compute the derivative of the model \n",
    "        model_derivative_values = torch.zeros(M,dim).to(device) \n",
    "        for d in range(dim): ## there is a more efficient way \n",
    "            for jj in range(0,M,batch_size_2):\n",
    "                end_index = jj + batch_size_2 \n",
    "                model_derivative_values[jj:end_index,d:d+1] = my_model.evaluate_derivative(integration_points[jj:end_index,:],d+1).detach()\n",
    "            #compute the derivative of the dictionary elements \n",
    "        for j in range(0,N0,batch_size_1): \n",
    "            end_index = j + batch_size_1 \n",
    "            if activation == 'relu' and my_model.k == 1: \n",
    "                weighted_derivative_part = weights * torch.heaviside(integration_points@ (relu_dict_parameters[j:end_index,0:dim].T) - relu_dict_parameters[j:end_index,dim], ZERO)\n",
    "                weighted_basis_value_col = weights *  (F.relu( torch.matmul(integration_points,relu_dict_parameters[j:end_index,0:dim].T ) - relu_dict_parameters[j:end_index,dim])**k) # uses broadcasting\n",
    "            elif activation == 'relu' and my_model.k > 1:\n",
    "                weighted_derivative_part = weights * my_model.k * F.relu(integration_points@ (relu_dict_parameters[j:end_index,0:dim].T) - relu_dict_parameters[j:end_index,dim])**(my_model.k-1)\n",
    "                weighted_basis_value_col = weights *  (F.relu( torch.matmul(integration_points,relu_dict_parameters[j:end_index,0:dim].T ) - relu_dict_parameters[j:end_index,dim])**k) # uses broadcasting\n",
    "            elif activation == 'tanh':\n",
    "                weighted_derivative_part = weights * (1/torch.cosh(integration_points@ (relu_dict_parameters[j:end_index,0:dim].T) - relu_dict_parameters[j:end_index,dim])**2)\n",
    "                weighted_basis_value_col = weights *  (F.tanh( torch.matmul(integration_points,relu_dict_parameters[j:end_index,0:dim].T ) - relu_dict_parameters[j:end_index,dim])) # uses broadcasting\n",
    "            for d in range(dim):\n",
    "                weighted_basis_value_dx_col = weighted_derivative_part * relu_dict_parameters.t()[d:d+1,j:end_index] \n",
    "                output[j:end_index] += weighted_basis_value_dx_col.t() @ model_derivative_values[:,d:d+1]  # diffusion term\n",
    "                output[j:end_index] += - BETA * weighted_basis_value_col.t() @ model_derivative_values[:,d:d+1] # convection term \n",
    "    \n",
    "    # Neumann boundary condition\n",
    "    if g_N != None:  \n",
    "        if DIMENSION == 1:\n",
    "            if activation == 'relu':\n",
    "                basis_values_bd_col = (F.relu(relu_dict_parameters[:,0] *integration_points_bd - relu_dict_parameters[:,1])**k) \n",
    "            elif activation == 'tanh':\n",
    "                basis_values_bd_col = (F.tanh(relu_dict_parameters[:,0] *integration_points_bd - relu_dict_parameters[:,1])) \n",
    "            weighted_basis_value_col_bd = basis_values_bd_col * weights_bd\n",
    "            dudn = g_N(integration_points_bd)* (torch.tensor([-1,1]).view(-1,1)).to(device)\n",
    "            output -=  weighted_basis_value_col_bd.t() @ dudn\n",
    "        else: \n",
    "            size_pts_bd = int(integration_points_bd.size(0)/(2*DIMENSION)) # pre-defined rules for integration points on bdries\n",
    "            bcs_N = g_N(dim)\n",
    "            for ii, g_ii in bcs_N:\n",
    "                # pts_bd_ii = pts_bd[2*ii*size_pts_bd:(2*ii+1)*size_pts_bd,:]\n",
    "                weighted_g_N = -g_ii(integration_points_bd[2*ii*size_pts_bd:(2*ii+1)*size_pts_bd,:])* weights_bd[2*ii*size_pts_bd:(2*ii+1)*size_pts_bd,:]\n",
    "                basis_value_bd_col = F.relu(integration_points_bd[2*ii*size_pts_bd:(2*ii+1)*size_pts_bd,:] @ (relu_dict_parameters[:,0:dim].T) - relu_dict_parameters[:,dim] )**(k)\n",
    "                output -= basis_value_bd_col.t() @ weighted_g_N\n",
    "\n",
    "                weighted_g_N = g_ii(integration_points_bd[(2*ii+1)*size_pts_bd:(2*ii+2)*size_pts_bd,:])* weights_bd[(2*ii+1)*size_pts_bd:(2*ii+2)*size_pts_bd,:]\n",
    "                basis_value_bd_col = F.relu(integration_points_bd[(2*ii+1)*size_pts_bd:(2*ii+2)*size_pts_bd,:] @ (relu_dict_parameters[:,0:dim].T) - relu_dict_parameters[:,dim])**(k)\n",
    "                output -= basis_value_bd_col.t() @ weighted_g_N\n",
    "        \n",
    "    output = torch.abs(output) \n",
    "    neuron_index = torch.argmax(output.flatten())\n",
    "    \n",
    "    return neuron_index \n",
    "\n",
    "def OGAHelmholtzReLUNDim(my_model,target,u_exact,u_exact_grad,g_N, N_list,num_epochs,plot_freq = 10,Nx = 1024,order =5, activation = 'relu',k = 1,rand_deter = 'deter', solver = 'direct',memory = 2**29): \n",
    "    \"\"\" Orthogonal greedy algorithm to solve a 1D Neumann problem 1D ReLU dictionary over [0,1]\n",
    "    \"\"\"\n",
    "\n",
    "    if DIMENSION == 1:\n",
    "        weights, integration_points = PiecewiseGQ1D_weights_points(x_l= 0,x_r=1, Nx = Nx,order =order)\n",
    "    elif DIMENSION == 2:\n",
    "        weights, integration_points = PiecewiseGQ2D_weights_points(Nx = Nx, order = order)\n",
    "    elif DIMENSION == 3:\n",
    "        weights, integration_points = PiecewiseGQ3D_weights_points(Nx = Nx, order = order) \n",
    "    else:\n",
    "        weights, integration_points = MonteCarlo_Sobol_dDim_weights_points(M = 2**14 ,d = 4)\n",
    "    weights_bd, integration_points_bd = Neumann_boundary_quadrature_points_weights(M = 2**14,dim = DIMENSION)\n",
    "\n",
    "    M = integration_points.size(0) \n",
    "\n",
    "    # Compute initial L2 error and the gradient error \n",
    "    err = torch.zeros(num_epochs+1)\n",
    "    err_h10 = torch.zeros(num_epochs+1) \n",
    "    num_neuron = 0 if my_model == None else int(my_model.fc1.bias.detach().data.size(0))\n",
    "    total_size2 = M*(num_neuron+1)\n",
    "    num_batch2 = total_size2//memory + 1 \n",
    "    batch_size_2 = M//num_batch2 # in\n",
    "    \n",
    "    if my_model == None: \n",
    "        for jj in range(0,M,batch_size_2): \n",
    "            end_index = jj + batch_size_2 \n",
    "            func_values = target(integration_points[jj:end_index,:])\n",
    "            err[0] += torch.sum(func_values**2 * weights[jj:end_index,:])**0.5\n",
    "        list_b = []\n",
    "        list_w = []\n",
    "    else: \n",
    "        bias = my_model.fc1.bias.detach().data\n",
    "        weights = my_model.fc1.weight.detach().data\n",
    "        for jj in range(0,M,batch_size_2): \n",
    "            end_index = jj + batch_size_2 \n",
    "            func_values = u_exact(integration_points[jj:end_index,:]) - my_model(integration_points[jj:end_index,:]).detach()\n",
    "            err[0] += torch.sum(func_values**2 * weights[jj:end_index,:])**0.5\n",
    "        list_b = list(bias)\n",
    "        list_w = list(weights)\n",
    "\n",
    "    # initial gradient error \n",
    "    if u_exact_grad != None and my_model!=None:\n",
    "        u_grad = u_exact_grad() \n",
    "        for ii, grad_i in enumerate(u_grad): \n",
    "            for jj in range(0,M,batch_size_2): \n",
    "                end_index = jj + batch_size_2 \n",
    "                my_model_dxi = my_model.evaluate_derivative(integration_points[jj:end_index,:],ii+1).detach() \n",
    "                err_h10[0] += torch.sum((grad_i(integration_points[jj:end_index,:]) - my_model_dxi)**2 * weights[jj:end_index,:])**0.5\n",
    "    elif u_exact_grad != None and my_model==None:\n",
    "        u_grad = u_exact_grad() \n",
    "        for grad_i in u_grad: \n",
    "            for jj in range(0,M,batch_size_2): \n",
    "                end_index = jj + batch_size_2 \n",
    "                err_h10[0] += torch.sum((grad_i(integration_points[jj:end_index,:]))**2 * weights[jj:end_index,:])**0.5\n",
    "\n",
    "    start_time = time.time()\n",
    "    solver = \"direct\"\n",
    "    print(\"using linear solver: \",solver)\n",
    "    N0 = np.prod(N_list) \n",
    "    if rand_deter == 'deter': \n",
    "        if activation == 'relu':\n",
    "            dict_parameters = generate_relu_dict3D(N_list).to(device) \n",
    "        elif activation == 'tanh':\n",
    "            dict_parameters = generate_tanh_dict3D_QMC(1,N0,48).to(device) \n",
    "\n",
    "    for i in range(num_epochs): \n",
    "        print('epoch: ',i+1)\n",
    "        if rand_deter == 'rand': \n",
    "            if activation == 'relu':\n",
    "                dict_parameters = generate_relu_dict3D_QMC(1,N0,48).to(device)   \n",
    "            elif activation == 'tanh':\n",
    "                dict_parameters = generate_tanh_dict3D_QMC(1,N0,48).to(device) \n",
    "        # start_argmax = time.time() \n",
    "        neuron_index = select_greedy_neuron_ind(dict_parameters,my_model,target,weights, integration_points,g_N,weights_bd, integration_points_bd, k,activation = activation, memory=memory) \n",
    "        # print(\"argmax time: \", time.time() - start_argmax) \n",
    "        # print(\"selected neuron index: \",neuron_index,relu_dict_parameters[neuron_index,0],-relu_dict_parameters[neuron_index,1] ) \n",
    "        list_w.append(dict_parameters[neuron_index,0:DIMENSION])\n",
    "        list_b.append(-dict_parameters[neuron_index,DIMENSION]) # different sign convention \n",
    "        num_neuron += 1\n",
    "        if activation == 'relu':\n",
    "            my_model = model(DIMENSION,num_neuron,1,k).to(device)\n",
    "        elif activation == 'tanh':\n",
    "            my_model = model_tanh(DIMENSION,num_neuron,1).to(device) \n",
    "        my_model.fc1.weight.data[:,0] = torch.tensor(list_w)[:]\n",
    "        my_model.fc1.bias.data[:] = torch.tensor(list_b)[:]\n",
    "\n",
    "        sol = minimize_linear_layer_helmholtz(my_model,target,weights, integration_points,weights_bd, integration_points_bd,g_N,activation =activation, solver = solver,memory = memory)\n",
    "        sol = sol.flatten() \n",
    "        my_model.fc2.weight.data[0,:] = sol[:]\n",
    "\n",
    "        if (i+1)%plot_freq == 0 and DIMENSION == 1:  \n",
    "            x_test = torch.linspace(0,1,200).view(-1,1).to(device)\n",
    "            u_true = u_exact(x_test)\n",
    "            plot_solution_modified(0,1,my_model,x_test,u_true)\n",
    "\n",
    "        # Get L2 error and gradient error \n",
    "        total_size2 = M*(num_neuron+1)\n",
    "        num_batch2 = total_size2//memory + 1 \n",
    "        batch_size_2 = M//num_batch2 # integration points \n",
    "\n",
    "        for jj in range(0,M,batch_size_2):\n",
    "            end_index = jj + batch_size_2 \n",
    "            func_values = u_exact(integration_points[jj:end_index,:]) - my_model(integration_points[jj:end_index,:]).detach()\n",
    "            func_values = func_values**2 \n",
    "            err[i+1]+= torch.sum(func_values*weights[jj:end_index,:])**0.5\n",
    "\n",
    "        if u_exact_grad != None:\n",
    "            for ii, grad_i in enumerate(u_grad): \n",
    "                for jj in range(0,M,batch_size_2): \n",
    "                    end_index = jj + batch_size_2 \n",
    "                    my_model_dxi = my_model.evaluate_derivative(integration_points[jj:end_index,:],ii+1).detach() \n",
    "                    err_h10[i+1] += torch.sum((grad_i(integration_points[jj:end_index,:]) - my_model_dxi)**2 * weights[jj:end_index,:])**0.5\n",
    "\n",
    "    print(\"time taken: \",time.time() - start_time)\n",
    "    return err, err_h10, my_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## relu example "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = 2 \n",
    "def u_exact(x):\n",
    "    return torch.cos(freq*pi*x[:,0:1])*torch.cos( freq*pi*x[:,1:2]) * torch.cos(freq*pi*x[:,2:3])  \n",
    "\n",
    "def u_exact_grad():\n",
    "\n",
    "    def grad_1(x):\n",
    "        return - freq*pi* torch.sin(freq*pi*x[:,0:1])*torch.cos( freq*pi*x[:,1:2]) * torch.cos(freq*pi*x[:,2:3])   \n",
    "    def grad_2(x):\n",
    "        return - freq*pi* torch.cos(freq*pi*x[:,0:1])*torch.sin( freq*pi*x[:,1:2]) * torch.cos(freq*pi*x[:,2:3])  \n",
    "    def grad_3(x):\n",
    "        return - freq*pi* torch.cos(freq*pi*x[:,0:1])*torch.cos( freq*pi*x[:,1:2]) * torch.sin(freq*pi*x[:,2:3])   \n",
    "    \n",
    "    u_grad=[grad_1, grad_2,grad_3] \n",
    "\n",
    "    return u_grad\n",
    "\n",
    "def laplace_u_exact(x):\n",
    "    return 3*(freq*pi)**2 * torch.cos(freq*pi*x[:,0:1])*torch.cos( freq*pi*x[:,1:2]) * torch.cos(freq*pi*x[:,2:3])\n",
    "\n",
    "def convection_term(x):\n",
    "    grad1 = - freq*pi* torch.sin(freq*pi*x[:,0:1])*torch.cos( freq*pi*x[:,1:2]) * torch.cos(freq*pi*x[:,2:3])   \n",
    "    grad2 = - freq*pi* torch.cos(freq*pi*x[:,0:1])*torch.sin( freq*pi*x[:,1:2]) * torch.cos(freq*pi*x[:,2:3])  \n",
    "    grad3 = - freq*pi* torch.cos(freq*pi*x[:,0:1])*torch.cos( freq*pi*x[:,1:2]) * torch.sin(freq*pi*x[:,2:3])    \n",
    "    return BETA * grad1 + BETA * grad2 + BETA * grad3  \n",
    "\n",
    "def rhs(x):\n",
    "    return  -laplace_u_exact(x) + convection_term(x) - LAMBDA**2 * u_exact(x)  \n",
    "\n",
    "g_N = None \n",
    "\n",
    "\n",
    "function_name = \"cos4pix\" \n",
    "filename_write = \"3DOGA-{}-order.txt\".format(function_name)\n",
    "f_write = open(filename_write, \"a\")\n",
    "f_write.write(\"\\n\")\n",
    "f_write.close() \n",
    "save = False \n",
    "relu_k = 3\n",
    "for N_list in [[2*3,2**3,2**3]]: # ,[2**6,2**6],[2**7,2**7] \n",
    "    # save = True \n",
    "    f_write = open(filename_write, \"a\")\n",
    "    my_model = None \n",
    "    Nx = 50\n",
    "    order = 3\n",
    "    exponent = 7\n",
    "    num_epochs = 2**exponent  \n",
    "    plot_freq = num_epochs \n",
    "    N = np.prod(N_list)\n",
    "    activation = 'relu'\n",
    "    err_QMC2, err_h10, my_model = OGAHelmholtzReLUNDim(my_model,rhs, u_exact, u_exact_grad,g_N, N_list,num_epochs,plot_freq, Nx, order, activation= activation, k = relu_k, rand_deter = 'rand', solver = \"direct\")\n",
    "    if save: \n",
    "        folder = 'data-neumann/'\n",
    "        filename = folder + 'err_OGA_2D_{}_neuron_{}_N_{}_deterministic.pt'.format(function_name,num_epochs,N)\n",
    "        torch.save(err_QMC2,filename) \n",
    "        folder = 'data-neumann/'\n",
    "        filename = folder + 'model_OGA_2D_{}_neuron_{}_N_{}_deterministic.pt'.format(function_name,num_epochs,N)\n",
    "        torch.save(my_model,filename)\n",
    "\n",
    "    show_convergence_order(err_QMC2,err_h10,exponent,N,filename_write,False)\n",
    "    show_convergence_order_latex(err_QMC2,err_h10,exponent,k =relu_k,d = 3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
