{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "93cd8b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "## 1D General elliptic PDE of the following form: \n",
    "## -div( a(x) grad u(x)) + b(x) grad u(x) + c(x) u(x) = f(x) in [0,1] \n",
    "## a(x), b(x), c(x) are set to be constant functions \n",
    "## du_dn = g on the boundary \n",
    "## this version also contains using the tanh-activated shallow neural network to solve the PDE \n",
    "\"\"\"\n",
    "log\n",
    "Nov 17th 2024 Modified by Xiaofeng: \n",
    "added three functions   \n",
    "1. select_discrete_dictionary\n",
    "2. compute_l2_error \n",
    "3. compute_gradient_error\n",
    "\n",
    "Nov 20th 2024 Modified by Xiaofeng \n",
    "1. use an efficient way to assemble the matrix that reuses previous matrices \n",
    "    - minimize_linear_layer_efficient\n",
    "\n",
    "Todo: \n",
    "1. remove some redundant variable to save memory \n",
    "2. test a huge dictionary and huge quadrature points  \n",
    "\"\"\"\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import sys\n",
    "import os \n",
    "from scipy.sparse import linalg\n",
    "from pathlib import Path\n",
    "if torch.cuda.is_available():  \n",
    "    device = \"cuda\" \n",
    "else:  \n",
    "    device = \"cpu\" \n",
    "\n",
    "torch.set_default_dtype(torch.float64)\n",
    "pi = torch.tensor(np.pi)\n",
    "ZERO = torch.tensor([0.]).to(device)\n",
    "\n",
    "###===============model parameters below================================\n",
    "LAMBDA = -4 # c(x) = LAMBDA, if negative Helmholtz equation parameters\n",
    "BETA = 5 ## convection term parameters \n",
    "DIMENSION = 3  ## dimension of the problem \n",
    "###===============model parameters above================================\n",
    "\n",
    "## Define the neural network model\n",
    "## already general in any dimension\n",
    "class model_tanh(nn.Module):\n",
    "    \"\"\" cosine shallow neural network\n",
    "    Parameters: \n",
    "    input size: input dimension\n",
    "    hidden_size1 : number of hidden layers \n",
    "    num_classes: output classes \n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, hidden_size1, num_classes):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size1)\n",
    "        self.fc2 = nn.Linear(hidden_size1, num_classes,bias = False)\n",
    "    def forward(self, x):\n",
    "        u1 = self.fc2( torch.tanh(self.fc1(x)) )\n",
    "        return u1\n",
    "    \n",
    "    def tanh_activation_dx(self,x): \n",
    "        return 1/torch.cosh(x)**2  \n",
    "      \n",
    "    def evaluate_derivative(self, x, i):\n",
    "        u1 = self.fc2( self.tanh_activation_dx(self.fc1(x)) *self.fc1.weight.t()[i-1:i,:] )  \n",
    "        return u1\n",
    "\n",
    "class model(nn.Module):\n",
    "    \"\"\" ReLU k shallow neural network\n",
    "    Parameters: \n",
    "    input size: input dimension\n",
    "    hidden_size1 : number of hidden layers \n",
    "    num_classes: output classes \n",
    "    k: degree of relu functions\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, hidden_size1, num_classes,k = 1):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size1)\n",
    "        self.fc2 = nn.Linear(hidden_size1, num_classes,bias = False)\n",
    "        self.k = k \n",
    "    def forward(self, x):\n",
    "        u1 = self.fc2(F.relu(self.fc1(x))**self.k)\n",
    "        return u1\n",
    "    def evaluate_derivative(self, x, i):\n",
    "        if self.k == 1:\n",
    "            ## ZERO = torch.tensor([0.]).to(device)\n",
    "            u1 = self.fc2(torch.heaviside(self.fc1(x),ZERO) * self.fc1.weight.t()[i-1:i,:] )\n",
    "        else:\n",
    "            u1 = self.fc2(self.k*F.relu(self.fc1(x))**(self.k-1) *self.fc1.weight.t()[i-1:i,:] )  \n",
    "        return u1\n",
    "    \n",
    "\n",
    "def show_convergence_order_latex2(err_l2,err_h10,exponent,k=1,d=1): \n",
    "    neuron_nums = [2**j for j in range(2,exponent+1)]\n",
    "    err_list = [err_l2[i] for i in neuron_nums ]\n",
    "    err_list2 = [err_h10[i] for i in neuron_nums ] \n",
    "    l2_order = -1/2-(2*k + 1)/(2*d)\n",
    "    h1_order =  -1/2-(2*(k-1)+ 1)/(2*d)\n",
    "    print(\"neuron num  & \\t $\\|u-u_n \\|_{{L^2}}$ & \\t order $O(n^{{{:.2f}}})$  & \\t $ | u -u_n |_{{H^1}}$ & \\t order $O(n^{{{:.2f}}})$  \\\\\\ \\hline \\hline \".format(l2_order,h1_order))\n",
    "    for i, item in enumerate(err_list):\n",
    "        if i == 0: \n",
    "            print(\"{} \\t\\t & {:.6f} &\\t\\t * & \\t\\t {:.6f} & \\t\\t *  \\\\\\ \\hline  \\n\".format(neuron_nums[i],item, err_list2[i] ) )   \n",
    "        else: \n",
    "            print(\"{} \\t\\t &  {:.2e} &  \\t\\t {:.2f} &  \\t\\t {:.2e} &  \\t\\t {:.2f} \\\\\\ \\hline  \\n\".format(neuron_nums[i],item,np.log(err_list[i-1]/err_list[i])/np.log(2),err_list2[i] , np.log(err_list2[i-1]/err_list2[i])/np.log(2) ) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0167b640",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c3731c7a",
   "metadata": {},
   "source": [
    "## 3D ReLU$^3$ data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dde9c56a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trial:  0\n",
      "trial:  1\n",
      "trial:  2\n",
      "trial:  3\n",
      "trial:  4\n",
      "neuron num  & \t $\\|u-u_n \\|_{L^2}$ & \t order $O(n^{-1.67})$  & \t $ | u -u_n |_{H^1}$ & \t order $O(n^{-1.33})$  \\\\ \\hline \\hline \n",
      "4 \t\t & 0.684214 &\t\t * & \t\t 3.872194 & \t\t *  \\\\ \\hline  \n",
      "\n",
      "8 \t\t &  4.36e-01 &  \t\t 0.65 &  \t\t 3.85e+00 &  \t\t 0.01 \\\\ \\hline  \n",
      "\n",
      "16 \t\t &  9.21e-01 &  \t\t -1.08 &  \t\t 3.89e+00 &  \t\t -0.01 \\\\ \\hline  \n",
      "\n",
      "32 \t\t &  4.91e+00 &  \t\t -2.42 &  \t\t 4.56e+00 &  \t\t -0.23 \\\\ \\hline  \n",
      "\n",
      "64 \t\t &  1.17e+00 &  \t\t 2.07 &  \t\t 1.80e+00 &  \t\t 1.34 \\\\ \\hline  \n",
      "\n",
      "128 \t\t &  8.35e-02 &  \t\t 3.81 &  \t\t 5.42e-01 &  \t\t 1.73 \\\\ \\hline  \n",
      "\n",
      "256 \t\t &  1.15e-02 &  \t\t 2.86 &  \t\t 1.66e-01 &  \t\t 1.71 \\\\ \\hline  \n",
      "\n",
      "512 \t\t &  1.75e-03 &  \t\t 2.71 &  \t\t 5.05e-02 &  \t\t 1.71 \\\\ \\hline  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "dim = 3 \n",
    "function_name = \"cosine\"\n",
    "Nx = 50 \n",
    "order = 2\n",
    "exponent = 9\n",
    "num_epochs = 2**exponent  \n",
    "plot_freq = num_epochs \n",
    "rand_deter = 'rand'\n",
    "activation = 'relu' \n",
    "relu_k = 3 \n",
    "\n",
    "\n",
    "memory = 2**29\n",
    "trial_num = 5 \n",
    "for N_list in [[2**3,2**3,2**3]]:   \n",
    "    errl2_trials = []\n",
    "    err_h10_trials = [] \n",
    "    for trial in range(trial_num):\n",
    "        print(\"trial: \", trial)\n",
    "        N = np.prod(N_list)\n",
    "        folder = './'\n",
    "        filename = folder + 'errl2_OGA_3D_{}_{}_{}_neuron_{}_N_{}_rand_trial_{}.pt'.format(function_name,activation, relu_k, num_epochs,N,trial)\n",
    "        errl2 = torch.load(filename) \n",
    "        filename = folder + 'err_h10_OGA_3D_{}_{}_{}_neuron_{}_N_{}_rand_trial_{}.pt'.format(function_name,activation, relu_k, num_epochs,N,trial)\n",
    "        err_h10 = torch.load(filename)\n",
    "\n",
    "        errl2_trials.append(errl2)\n",
    "        err_h10_trials.append(err_h10)\n",
    "    errl2_mean = torch.mean(torch.stack(errl2_trials), dim=0)\n",
    "    err_h10_mean = torch.mean(torch.stack(err_h10_trials), dim=0)\n",
    "    errl2_std = torch.std(torch.stack(errl2_trials), dim=0)\n",
    "    err_h10_std = torch.std(torch.stack(err_h10_trials), dim=0)\n",
    "\n",
    "    show_convergence_order_latex2(errl2_mean,err_h10_mean,exponent,relu_k,dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e819fafa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6573120",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7e062072",
   "metadata": {},
   "source": [
    "## 3D tanh data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "11671c40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trial:  0\n",
      "trial:  1\n",
      "trial:  2\n",
      "trial:  3\n",
      "trial:  4\n",
      "neuron num  & \t $\\|u-u_n \\|_{L^2}$ & \t order $O(n^{-1.67})$  & \t $ | u -u_n |_{H^1}$ & \t order $O(n^{-1.33})$  \\\\ \\hline \\hline \n",
      "4 \t\t & 0.954152 &\t\t * & \t\t 3.891471 & \t\t *  \\\\ \\hline  \n",
      "\n",
      "8 \t\t &  2.82e+00 &  \t\t -1.56 &  \t\t 4.57e+00 &  \t\t -0.23 \\\\ \\hline  \n",
      "\n",
      "16 \t\t &  8.93e+00 &  \t\t -1.66 &  \t\t 7.14e+00 &  \t\t -0.64 \\\\ \\hline  \n",
      "\n",
      "32 \t\t &  4.81e+00 &  \t\t 0.89 &  \t\t 4.53e+00 &  \t\t 0.66 \\\\ \\hline  \n",
      "\n",
      "64 \t\t &  1.01e+00 &  \t\t 2.26 &  \t\t 1.75e+00 &  \t\t 1.37 \\\\ \\hline  \n",
      "\n",
      "128 \t\t &  1.10e-01 &  \t\t 3.20 &  \t\t 5.36e-01 &  \t\t 1.70 \\\\ \\hline  \n",
      "\n",
      "256 \t\t &  6.23e-03 &  \t\t 4.14 &  \t\t 1.09e-01 &  \t\t 2.29 \\\\ \\hline  \n",
      "\n",
      "512 \t\t &  4.16e-04 &  \t\t 3.90 &  \t\t 1.28e-02 &  \t\t 3.10 \\\\ \\hline  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "dim = 3 \n",
    "function_name = \"cosine\"\n",
    "Nx = 50 \n",
    "order = 2\n",
    "exponent = 9\n",
    "num_epochs = 2**exponent  \n",
    "plot_freq = num_epochs \n",
    "rand_deter = 'rand'\n",
    "memory = 2**29\n",
    "activation = 'tanh' \n",
    "relu_k = 3 # not used if activation != relu \n",
    "\n",
    "\n",
    "trial_num = 5 \n",
    "for N_list in [[2**3,2**3,2**3]]: # ,[2**6,2**6],[2**7,2**7] \n",
    "    errl2_trials = []\n",
    "    err_h10_trials = []\n",
    "    for trial in range(trial_num): \n",
    "        print(\"trial: \", trial)\n",
    "        N = np.prod(N_list)\n",
    "        folder = './'\n",
    "        filename = folder + 'errl2_OGA_3D_{}_{}_neuron_{}_N_{}_rand_trial_{}.pt'.format(function_name,activation, num_epochs,N,trial)\n",
    "        errl2 = torch.load(filename)\n",
    "        filename = folder + 'err_h10_OGA_3D_{}_{}_neuron_{}_N_{}_rand_trial_{}.pt'.format(function_name,activation, num_epochs,N,trial)\n",
    "        err_h10 = torch.load(filename)\n",
    "        errl2_trials.append(errl2)\n",
    "        err_h10_trials.append(err_h10)\n",
    "    errl2_mean = torch.mean(torch.stack(errl2_trials), dim=0)\n",
    "    err_h10_mean = torch.mean(torch.stack(err_h10_trials), dim=0)\n",
    "\n",
    "    show_convergence_order_latex2(errl2_mean,err_h10_mean,exponent,relu_k,dim)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "444ff4eb",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
